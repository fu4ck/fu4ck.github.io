[{"title":"redis 源码思维导图","url":"/2020/01/20/redis源码分析思维导图/","content":"\n## redis 源码思维导图\n\n本人画的redis源码思维导图，有点乱，自己凑合着看看吧~~ orz..\n\n![redis-server](/images/redis-server.png)","tags":["redis"],"categories":["redis"]},{"title":"open-falcon 架构","url":"/2020/01/20/open-falcon框架学习/","content":"\n## open-falcon 架构\n\nopen-falcon 主要架构图：\n\n![open-falcon architecture](/images/func_intro_1.png)\n\n各个模块说明：\n\nagent 组件：\n\n目前 agent 服务已经覆盖公司大部分机器，一个自动采集机器指标的自动化服务。\n\n数据上报支持三种方式: \n\n1. agent 自采集基础监控上报；\n\n2. 用户自定义推送数据 (数据按照指定格式推送到本地 agent 端口)；\n\n3. 插件采集上报。\n\nhbs 组件：\n\n心跳服务器，定时从 DB 获取节点与主机对应关系、插件与节点绑定列表、模板、策略、全局策略等信息；将插件与节点绑定关系解析为插件与主机一一对应关系，并提供 rpc 接口方便所有 agent 查询；将 agent 上报的版本信息、插件信息写入 falcon 数据库；将模板、策略解析为策略与主机的关系对应表，与全局策略一起，以 rpc 方式提供给 judge 服务，方便其定时获取。\n\ntransfer 组件：\n\n启动时维护两个一致性哈希列表，分别对应 graph 服务与 judge 服务，用于通过 endpoint 和 counter 计算得到的 MD5，定位每条监控数据应该存储到哪个 graph 实例和 judge 实例；提供数据转发功能，将 agent 通过 rpc 上报的监控数据，通过一致性哈希定位后，上报给相应的 graph 实例和 judge 实例；使用 rpc 接口提供 history 监控数据查询功能，用于绘图展示等。\n\ngraph 组件：\n\n接入 rrdtool，用于监控数据持久化，通过 endpoint 和 counter 计算的 MD5 确定文件名；提供 rpc 接口，接收 transfer 上报的监控数据，并支持缓存，每个监控数据缓存半小时后再做数据持久化以减轻磁盘 IO 压力，提高整体吞吐量；提供索引缓存，每一个监控数据上报后，通过 endpoint、counter、step、timestamp 构建缓存，如果已存在则更新 timestamp，否则新建并上报至 graph 数据库；提供历史监控数据查询的 rpc 接口，便于 transfer 调用查询，查询时先通过索引缓存确认相应的 endpoint、counter 是否存在，如果存在则查询合并 rrd 文件中持久化数据与缓存数据并返回，否则直接返回。\n\njudge 组件：\n\n定时从 hbs 服务获取主机与策略的一一对应关系、以及全局策略，统称告警策略，用于告警判别；提供 rpc 接口，用于接收 transfer 上报的监控数据，收到每条数据时，遍历所有告警策略，如果符合告警条件，则将告警策略和监控数据存储到 redis 队列。\n\nalarm 组件：\n\n不停遍历 redis 队列，从中取出 judge 存储的告警策略和监控数据，写入报警数据库，然后依照告警策略中配置的告警组和获取告警成员的联系方式，和告警形成一一对应的关系，上报给 redis，方便 alarm 的下游服务进行告警发送。\n\naggregator 组件：\n\n集群监控的本质是一个聚合功能。单台机器的监控指标难以反应整个集群的情况，我们需要把整个集群的机器（体现为 xbox 某个节点下的机器）综合起来看。比如所有机器的 qps 加和才是整个集群的 qps，所有机器的 request_fail 数量 ÷ 所有机器的 request_total 数量 = 整个集群的请求失败率。我们计算出集群的某个整体指标之后，也会有 “查看该指标的历史趋势图” “为该指标配置报警” 这种需求，故而，我们会把这个指标重新 push 回监控 server 端，于是，你就可以把她当成一个普通 counter 来对待了。\n\nnodata 组件：\n\nnodata 能够和 judge 一起，监测采集项的上报异常，过程为: 配置了 nodata 的采集项超时未上报数据，nodata 生成一条非法的 mock 数据；用户在 judge 上配置相应的报警策略，收到 mock 数据就产生报警。采集项上报异常检测，作为 judge 的一个必要补充，能够使 judge 的实时报警功能更加完善、可靠。nodata 只为少数重要的采集项服务，其处理的采集项的数量，应该不多于 judge 的十分之一。滥用 nodata，将会给 falcon 的运维管理带来很多问题。通常 nodata 按照 step 从绘图中取不到打点数据时候，当然是有一定的容错 step，一般我们控制在 2 到 3 个 step。\n","tags":["monitor"],"categories":["monitor"]},{"title":"CNN学习笔记","url":"/2020/01/19/CNN学习/","content":"\n## CNN学习笔记\n\n**从神经网络到卷积神经网络（CNN）**\n\n![img](/images/1093303-20170430194200912-687300437.jpg)\n\n**卷积神经网络的层级结构**\n   • 数据输入层/ Input layer\n　• 卷积计算层/ CONV layer\n　• ReLU激励层 / ReLU layer\n　• 池化层 / Pooling layer\n　• 全连接层 / FC layer\n\n### **数据输入层**\n\n该层要做的处理主要是对原始图像数据进行预处理，其中包括：\n\n去均值：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。\n\n 归一化：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。\n\nPCA/白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化\n\n去均值与归一化效果图：\n\n![img](/images/1093303-20170430194338194-1949897491.jpg)\n\n去相关与白化效果图：\n\n![img](/images/1093303-20170430194357553-1200745791.jpg)\n\n### **卷积计算层**\n\n局部关联。每个神经元看做一个滤波器(filter)\n\n窗口(receptive field)滑动， filter对局部数据计算\n\n深度/depth\n\n步长/stride （窗口一次滑动的长度）\n\n填充值/zero-padding\n\n![img](/images/1093303-20170430194425147-845167791.png)\n\n![img](/images/1093303-20190120113539659-455066516.gif)\n\n**参数共享机制**\n\n在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。神经元就是图像处理中的滤波器，比如边缘检测专用的Sobel滤波器，即卷积层的每个滤波器都会有自己所关注一个图像特征，比如垂直边缘，水平边缘，颜色，纹理等等，这些所有神经元加起来就好比就是整张图像的特征提取器集合。\n\n一组固定的权重和不同窗口内数据做内积: 卷积\n\n### **激励层**\n\n把卷积层输出结果做非线性映射。\n\n![img](/images/1093303-20170430194934006-705271151.jpg)\n\nCNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)                 \n\n激励层的实践经验：\n不要用sigmoid！不要用sigmoid！不要用sigmoid！\n首先试RELU，因为快，但要小心点\n如果2失效，请用Leaky ReLU或者Maxout\n某些情况下tanh倒是有不错的结果，但是很少\n\n### **池化层**\n\n池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。\n简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像。\n\n1. 特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。\n2. 特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。\n3. 在一定程度上防止过拟合，更方便优化。\n\n![img](/images/1093303-20170430195028600-318072954.jpg)\n\n\n\n池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。\n\nMax pooling：\n\n对于每个2*2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2*2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。\n\n### **全连接层**\n\n![img](/images/1093303-20170430195130772-454262568.jpg)\n\n**一般CNN结构依次为**\n　　1. INPUT\n　　2. [[CONV -> RELU]*N -> POOL?]*M \n　　3. [FC -> RELU]*K\n　　4. FC\n\n**卷积神经网络之优缺点**：\n\n优点：\n共享卷积核，对高维数据处理无压力\n无需手动选取特征，训练好权重，即得特征分类效果好\n缺点：\n需要调参，需要大样本量，训练最好要GPU\n物理含义不明确\n\n**总结**\n卷积网络在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["深度学习"],"categories":["深度学习"]},{"title":"monit代码分析","url":"/2020/01/17/monit学习/","content":"\n## monit代码分析\n\n主要流程main函数:\n\n```c\n/**\n * The Prime mover\n */\nint main(int argc, char **argv) {\n        Bootstrap(); // Bootstrap libmonit  //初始化代码\n        Bootstrap_setAbortHandler(vLogAbortHandler);  // Abort Monit on exceptions thrown by libmonit\n        Bootstrap_setErrorHandler(vLogError);\n        setlocale(LC_ALL, \"C\");\n        prog = File_basename(argv[0]);\n#ifdef HAVE_OPENSSL\n        Ssl_start();\n#endif\n        init_env();\n        handle_options(argc, argv);\n        do_init();\n        do_action(argc, argv);\n        do_exit(false);\n        return 0;\n}\n```\n\nBootstrap函数：\n\n ```c\nBootstrap:\nvoid Bootstrap(void) {\n        Exception_init();\n        Thread_init();\n}\n\n ```\n\nSsl_start函数，加载ssl协议\n\n```c\nvoid Ssl_start() {\n#if (OPENSSL_VERSION_NUMBER < 0x10100000L) || defined(LIBRESSL_VERSION_NUMBER)\n        SSL_library_init();\n        SSL_load_error_strings();\n        int locks = CRYPTO_num_locks();\n        instanceMutexTable = CALLOC(locks, sizeof(Mutex_T));\n        for (int i = 0; i < locks; i++)\n                Mutex_init(instanceMutexTable[i]);\n        CRYPTO_THREADID_set_callback(_threadID);\n        CRYPTO_set_locking_callback(_mutexLock);\n#endif\n        if (File_exist(URANDOM_DEVICE))\n                RAND_load_file(URANDOM_DEVICE, RANDOM_BYTES);\n        else if (File_exist(RANDOM_DEVICE))\n                RAND_load_file(RANDOM_DEVICE, RANDOM_BYTES);\n        else\n                THROW(AssertException, \"SSL: cannot find %s nor %s on the system\", URANDOM_DEVICE, RANDOM_DEVICE);\n}\n```\n\n初始化环境：\n\n```c\n/**\n * Initialize the program environment\n *\n * @see https://bitbucket.org/tildeslash/monit/commits/cd545838378517f84bdb0989cadf461a19d8ba11\n */\nvoid init_env() {\n        Util_closeFds();\n        // Ensure that std descriptors (0, 1 and 2) are open\n        int devnull = open(\"/dev/null\", O_RDWR);\n        if (devnull == -1) {\n                THROW(AssertException, \"Cannot open /dev/null -- %s\", STRERROR);\n        }\n        for (int i = 0; i < 3; i++) {\n                struct stat st;\n                if (fstat(i, &st) == -1) {\n                        if (dup2(devnull, i) < 0) {\n                                close(devnull);\n                                THROW(AssertException, \"dup2 failed -- %s\", STRERROR);\n                        }\n                }\n        }\n        close(devnull);\n        // Get password struct with user info\n        char buf[4096];\n        struct passwd pw, *result = NULL;\n        if (getpwuid_r(geteuid(), &pw, buf, sizeof(buf), &result) != 0 || ! result)\n                THROW(AssertException, \"getpwuid_r failed -- %s\", STRERROR);\n        Run.Env.home = Str_dup(pw.pw_dir);\n        Run.Env.user = Str_dup(pw.pw_name);\n        // Get CWD\n        char t[PATH_MAX];\n        if (! Dir_cwd(t, PATH_MAX))\n                THROW(AssertException, \"Monit: Cannot read current directory -- %s\", STRERROR);\n        Run.Env.cwd = Str_dup(t);\n}\n\n```\n\nhandle_options函数处理传参情况：\n\ndo_init函数初始化文件和服务\n\n```c\n/**\n * Initialize this application - Register signal handlers,\n * Parse the control file and initialize the program's\n * datastructures and the log system.\n */\nstatic void do_init() {\n        /*\n         * Register interest for the SIGTERM signal,\n         * in case we run in daemon mode this signal\n         * will terminate a running daemon.\n         */\n        signal(SIGTERM, do_destroy);\n\n        /*\n         * Register interest for the SIGUSER1 signal,\n         * in case we run in daemon mode this signal\n         * will wakeup a sleeping daemon.\n         */\n        signal(SIGUSR1, do_wakeup);\n\n        /*\n         * Register interest for the SIGINT signal,\n         * in case we run as a server but not as a daemon\n         * we need to catch this signal if the user pressed\n         * CTRL^C in the terminal\n         */\n        signal(SIGINT, do_destroy);\n\n        /*\n         * Register interest for the SIGHUP signal,\n         * in case we run in daemon mode this signal\n         * will reload the configuration.\n         */\n        signal(SIGHUP, do_reload);\n\n        /*\n         * Register no interest for the SIGPIPE signal,\n         */\n        signal(SIGPIPE, SIG_IGN);\n\n        /*\n         * Initialize the random number generator\n         */\n        srandom((unsigned)(Time_now() + getpid()));\n\n        /*\n         * Initialize the Runtime mutex. This mutex\n         * is used to synchronize handling of global\n         * service data\n         */\n        Mutex_init(Run.mutex);\n\n        /*\n         * Initialize heartbeat mutex and condition\n         */\n        Mutex_init(heartbeatMutex);\n        Sem_init(heartbeatCond);\n\n        /*\n         * Get the position of the control file\n         */\n        if (! Run.files.control)\n                Run.files.control = file_findControlFile();\n\n        /*\n         * Initialize the system information data collecting interface\n         */\n        if (init_system_info())\n                Run.flags |= Run_ProcessEngineEnabled;\n\n        /*\n         * Start the Parser and create the service list. This will also set\n         * any Runtime constants defined in the controlfile.\n         */\n        if (! parse(Run.files.control))\n                exit(1);\n\n        /*\n         * Initialize the log system\n         */\n        if (! log_init())\n                exit(1);\n\n        /*\n         * Did we find any service ?\n         */\n        if (! servicelist) {\n                LogError(\"No service has been specified\\n\");\n                exit(0);\n        }\n\n        /*\n         * Initialize Runtime file variables\n         */\n        file_init();\n\n        /*\n         * Should we print debug information ?\n         */\n        if (Run.debug) {\n                Util_printRunList();\n                Util_printServiceList();\n        }\n\n        /*\n         * Reap any stray child processes we may have created\n         */\n        atexit(waitforchildren);\n}\n```\n\nfile_findControlFile()函数，读取配置文件，corefoundation\n\n```c\nchar *file_findControlFile() {\n        char *rcfile = CALLOC(sizeof(char), STRLEN + 1);\n        snprintf(rcfile, STRLEN, \"%s/.%s\", Run.Env.home, MONITRC);\n        if (File_exist(rcfile)) {\n                return rcfile;\n        }\n        snprintf(rcfile, STRLEN, \"/etc/%s\", MONITRC);\n        if (File_exist(rcfile)) {\n                return rcfile;\n        }\n        snprintf(rcfile, STRLEN, \"%s/%s\", SYSCONFDIR, MONITRC);\n        if (File_exist(rcfile)) {\n                return rcfile;\n        }\n        snprintf(rcfile, STRLEN, \"/usr/local/etc/%s\", MONITRC);\n        if (File_exist(rcfile)) {\n                return rcfile;\n        }\n        if (File_exist(MONITRC)) {\n                snprintf(rcfile, STRLEN, \"%s/%s\", Run.Env.cwd, MONITRC);\n                return rcfile;\n        }\n        LogError(\"Cannot find the Monit control file at ~/.%s, /etc/%s, %s/%s, /usr/local/etc/%s or at ./%s \\n\", MONITRC, MONITRC, SYSCONFDIR, MONITRC, MONITRC, MONITRC);\n        exit(1);\n}\n```\n\ndo_action主流程:\n\n```c\n/**\n * Dispatch to the submitted action - actions are program arguments\n */\nstatic void do_action(int argc, char **args) {\n        char *action = args[optind];\n\n        Run.flags |= Run_Once;\n\n        if (! action) {\n                do_default();\n        } else if (IS(action, \"start\")     ||\n                   IS(action, \"stop\")      ||\n                   IS(action, \"monitor\")   ||\n                   IS(action, \"unmonitor\") ||\n                   IS(action, \"restart\")) {\n                char *service = args[++optind];\n                if (Run.mygroup || service) {\n                        int errors = 0;\n                        List_T services = List_new();\n                        if (Run.mygroup) {\n                                for (ServiceGroup_T sg = servicegrouplist; sg; sg = sg->next) {\n                                        if (IS(Run.mygroup, sg->name)) {\n                                                for (list_t m = sg->members->head; m; m = m->next) {\n                                                        Service_T s = m->e;\n                                                        List_append(services, s->name);\n                                                }\n                                                break;\n                                        }\n                                }\n                                if (List_length(services) == 0) {\n                                        List_free(&services);\n                                        LogError(\"Group '%s' not found\\n\", Run.mygroup);\n                                        exit(1);\n                                }\n                        } else if (IS(service, \"all\")) {\n                                for (Service_T s = servicelist; s; s = s->next)\n                                        List_append(services, s->name);\n                        } else {\n                                List_append(services, service);\n                        }\n                        errors = exist_daemon() ? (HttpClient_action(action, services) ? 0 : 1) : control_service_string(services, action);\n                        List_free(&services);\n                        if (errors)\n                                exit(1);\n                } else {\n                        LogError(\"Please specify a service name or 'all' after %s\\n\", action);\n                        exit(1);\n                }\n        } else if (IS(action, \"reload\")) {\n                LogInfo(\"Reinitializing %s daemon\\n\", prog);\n                kill_daemon(SIGHUP);\n        } else if (IS(action, \"status\")) {\n                char *service = args[++optind];\n                if (! HttpClient_status(Run.mygroup, service))\n                        exit(1);\n        } else if (IS(action, \"summary\")) {\n                char *service = args[++optind];\n                if (! HttpClient_summary(Run.mygroup, service))\n                        exit(1);\n        } else if (IS(action, \"report\")) {\n                char *type = args[++optind];\n                if (! HttpClient_report(type))\n                        exit(1);\n        } else if (IS(action, \"procmatch\")) {\n                char *pattern = args[++optind];\n                if (! pattern) {\n                        printf(\"Invalid syntax - usage: procmatch \\\"<pattern>\\\"\\n\");\n                        exit(1);\n                }\n                ProcessTree_testMatch(pattern);\n        } else if (IS(action, \"quit\")) {\n                kill_daemon(SIGTERM);\n        } else if (IS(action, \"validate\")) {\n                if (do_wakeupcall()) {\n                        char *service = args[++optind];\n                        HttpClient_status(Run.mygroup, service);\n                } else {\n                        _validateOnce();\n                }\n                exit(1);\n        } else {\n                LogError(\"Invalid argument -- %s  (-h will show valid arguments)\\n\", action);\n                exit(1);\n        }\n}\n```\n\naction= start stop monitor unmonitor restart 通过维护一个服务列表发送post请求给服务端来启动服务。\n\ndo_default主要启动服务的函数\n\n```c\n/**\n * Default action - become a daemon if defined in the Run object and\n * run validate() between sleeps. If not, just run validate() once.\n * Also, if specified, start the monit http server if in deamon mode.\n */\nstatic void do_default() {\n        if (Run.flags & Run_Daemon) {\n                if (do_wakeupcall())\n                        exit(0);\n\n                Run.flags &= ~Run_Once;\n                if (can_http()) {\n                        if (Run.httpd.flags & Httpd_Net)\n                                LogInfo(\"Starting Monit %s daemon with http interface at [%s]:%d\\n\", VERSION, Run.httpd.socket.net.address ? Run.httpd.socket.net.address : \"*\", Run.httpd.socket.net.port);\n                        else if (Run.httpd.flags & Httpd_Unix)\n                                LogInfo(\"Starting Monit %s daemon with http interface at %s\\n\", VERSION, Run.httpd.socket.unix.path);\n                } else {\n                        LogInfo(\"Starting Monit %s daemon\\n\", VERSION);\n                }\n\n                if (! (Run.flags & Run_Foreground))\n                        daemonize();\n\n                if (! file_createPidFile(Run.files.pid)) {\n                        LogError(\"Monit daemon died\\n\");\n                        exit(1);\n                }\n\n                if (! State_open())\n                        exit(1);\n                State_restore();\n\n                atexit(file_finalize);\n\n                if (Run.startdelay && State_reboot()) {\n                        time_t now = Time_now();\n                        time_t delay = now + Run.startdelay;\n\n                        LogInfo(\"Monit will delay for %ds on first start after reboot ...\\n\", Run.startdelay);\n\n                        /* sleep can be interrupted by signal => make sure we paused long enough */\n                        while (now < delay) {\n                                sleep((unsigned int)(delay - now));\n                                if (Run.flags & Run_Stopped)\n                                        do_exit(false);\n                                now = Time_now();\n                        }\n                }\n\n                if (can_http())\n                        monit_http(Httpd_Start);\n\n                /* send the monit startup notification */\n                Event_post(Run.system, Event_Instance, State_Changed, Run.system->action_MONIT_START, \"Monit %s started\", VERSION);\n\n                if (Run.mmonits) {\n                        Thread_create(heartbeatThread, heartbeat, NULL);\n                        heartbeatRunning = true;\n                }\n\n                while (true) {\n                        validate();\n\n                        /* In the case that there is no pending action then sleep */\n                        if (! (Run.flags & Run_ActionPending) && ! interrupt())\n                                sleep(Run.polltime);\n\n                        if (Run.flags & Run_DoWakeup) {\n                                Run.flags &= ~Run_DoWakeup;\n                                LogInfo(\"Awakened by User defined signal 1\\n\");\n                        }\n\n                        if (Run.flags & Run_Stopped) {\n                                do_exit(true);\n                        } else if (Run.flags & Run_DoReload) {\n                                do_reinit();\n                        } else {\n                                State_saveIfDirty();\n                        }\n                }\n        } else {\n                _validateOnce();\n        }\n}\n```\n\ndo_wakeupcall调用函数是否需要唤醒进程。\n\ncan_http()判断是否可以启动http.\n\ndaemonize()函数：\n\n```c\n/**\n * Transform a program into a daemon. Inspired by code from Stephen\n * A. Rago's book, Unix System V Network Programming.\n */\nvoid daemonize() {\n        pid_t pid;\n        /*\n         * Become a session leader to lose our controlling terminal\n         */\n        if ((pid = fork ()) < 0) {\n                LogError(\"Cannot fork a new process\\n\");\n                exit (1);\n        } else if (pid != 0) {\n                _exit(0);\n        }\n        setsid();\n        if ((pid = fork ()) < 0) {\n                LogError(\"Cannot fork a new process\\n\");\n                exit (1);\n        } else if (pid != 0) {\n                _exit(0);\n        }\n        /*\n         * Change current directory to the root so that other file systems can be unmounted while we're running\n         */\n        if (chdir(\"/\") < 0) {\n                LogError(\"Cannot chdir to / -- %s\\n\", STRERROR);\n                exit(1);\n        }\n        /*\n         * Attach standard descriptors to /dev/null. Other descriptors should be closed in env.c\n         */\n        Util_redirectStdFds();\n}\n```\n\nfile_createPidFile场景pid文件。\n\n服务数据结构，所有的服务数据结构都在monit.h文件中\n\nyacc flex解析\n\n使用flex词法解析器，yacc语法解析器。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"influxdb","url":"/2020/01/17/influxdb1/","content":"\n### influxdb 启动流程学习笔记\n\n#### 流程分析\n\n本文基于influxdb 1.4来进行分析代码\n\ninfluxdb入口文件在 /cmd/influxd/main.go文件中\n\n```go\n// 主函数\nfunc main() {\n\trand.Seed(time.Now().UnixNano())\n\t//初始化\n\tm := NewMain()\n    // Run \n\tif err := m.Run(os.Args[1:]...); err != nil {\n\t\tfmt.Fprintln(os.Stderr, err)\n\t\tos.Exit(1)\n\t}\n}\n```\n\nNewMain函数初始化一个实例\n\n```go\n// NewMain return a new instance of Main.\nfunc NewMain() *Main {\n\treturn &Main{\n\t\tStdin:  os.Stdin,\n\t\tStdout: os.Stdout,\n\t\tStderr: os.Stderr,\n\t}\n}\n```\n\n主要流程在Run函数中，\n\n```go\n// Run determines and runs the command specified by the CLI args.\nfunc (m *Main) Run(args ...string) error {\n\tname, args := cmd.ParseCommandName(args)\n\n\t// Extract name from args.\n\tswitch name {\n\tcase \"\", \"run\":\n        // 默认执行流程\n\t\tcmd := run.NewCommand()\n\n\t\t// Tell the server the build details.\n\t\tcmd.Version = version\n\t\tcmd.Commit = commit\n\t\tcmd.Branch = branch\n\n        // 执行主要的函数\n\t\tif err := cmd.Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"run: %s\", err)\n\t\t}\n\t\t//中断信号量\n\t\tsignalCh := make(chan os.Signal, 1)\n\t\tsignal.Notify(signalCh, os.Interrupt, syscall.SIGTERM)\n\t\tcmd.Logger.Info(\"Listening for signals\")\n\n\t\t// Block until one of the signals above is received\n\t\t<-signalCh\n\t\tcmd.Logger.Info(\"Signal received, initializing clean shutdown...\")\n\t\tgo cmd.Close()\n\n\t\t// Block again until another signal is received, a shutdown timeout elapses,\n\t\t// or the Command is gracefully closed\n\t\tcmd.Logger.Info(\"Waiting for clean shutdown...\")\n\t\tselect {\n\t\tcase <-signalCh:\n\t\t\tcmd.Logger.Info(\"Second signal received, initializing hard shutdown\")\n\t\tcase <-time.After(time.Second * 30):\n\t\t\tcmd.Logger.Info(\"Time limit reached, initializing hard shutdown\")\n\t\tcase <-cmd.Closed:\n\t\t\tcmd.Logger.Info(\"Server shutdown completed\")\n\t\t}\n\n\t\t// goodbye.\n\n\tcase \"backup\":\n        //备份\n\t\tname := backup.NewCommand()\n\t\tif err := name.Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"backup: %s\", err)\n\t\t}\n\tcase \"restore\":\n        //恢复\n\t\tname := restore.NewCommand()\n\t\tif err := name.Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"restore: %s\", err)\n\t\t}\n\tcase \"config\":\n        //打印当前配置\n\t\tif err := run.NewPrintConfigCommand().Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"config: %s\", err)\n\t\t}\n\tcase \"version\":\n\t\tif err := NewVersionCommand().Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"version: %s\", err)\n\t\t}\n\tcase \"help\":\n\t\tif err := help.NewCommand().Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"help: %s\", err)\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(`unknown command \"%s\"`+\"\\n\"+`Run 'influxd help' for usage`+\"\\n\\n\", name)\n\t}\n\n\treturn nil\n}\n```\n\n先分析run部分：\n\n```go\n// Run parses the config from args and runs the server.\nfunc (cmd *Command) Run(args ...string) error {\n\t// Parse the command line flags.\n\toptions, err := cmd.ParseFlags(args...)\n\tif err != nil {\n\t\treturn err\n\t}\n\n    //解析配置文件\n\tconfig, err := cmd.ParseConfig(options.GetConfigPath())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parse config: %s\", err)\n\t}\n\n\t// Apply any environment variables on top of the parsed config\n\tif err := config.ApplyEnvOverrides(cmd.Getenv); err != nil {\n\t\treturn fmt.Errorf(\"apply env config: %v\", err)\n\t}\n\n\t// Propogate the top-level join options down to the meta config\n    //解析join的集群环境下的iplist\n\tif config.Join != \"\" {\n\t\tconfig.Meta.JoinPeers = strings.Split(config.Join, \",\")\n\t}\n\n\t// Command-line flags for -join and -hostname override the config\n\t// and env variable\n\tif options.Join != \"\" {\n\t\tconfig.Meta.JoinPeers = strings.Split(options.Join, \",\")\n\t}\n\n    // 解析本地hostname\n\tif options.Hostname != \"\" {\n\t\tconfig.Hostname = options.Hostname\n\t}\n\n\t// Propogate the top-level hostname down to dependendent configs\n\tconfig.Meta.RemoteHostname = config.Hostname\n\n\t// Validate the configuration.\n    // 检查各个配置是否为空\n\tif err := config.Validate(); err != nil {\n\t\treturn fmt.Errorf(\"%s. To generate a valid configuration file run `influxd config > influxdb.generated.conf`\", err)\n\t}\n\n\tvar logErr error\n\tif cmd.Logger, logErr = config.Logging.New(cmd.Stderr); logErr != nil {\n\t\t// assign the default logger\n\t\tcmd.Logger = logger.New(cmd.Stderr)\n\t}\n\n\t// Attempt to run pprof on :6060 before startup if debug pprof enabled.\n    //是否开启pprof\n\tif config.HTTPD.DebugPprofEnabled {\n\t\truntime.SetBlockProfileRate(int(1 * time.Second))\n\t\truntime.SetMutexProfileFraction(1)\n\t\tgo func() { http.ListenAndServe(\"localhost:6060\", nil) }()\n\t}\n\n\t// Print sweet InfluxDB logo.\n    // 打印logo\n\tif !config.Logging.SuppressLogo && logger.IsTerminal(cmd.Stdout) {\n\t\tfmt.Fprint(cmd.Stdout, logo)\n\t}\n\n\t// Mark start-up in log.\n\tcmd.Logger.Info(\"InfluxDB starting\",\n\t\tzap.String(\"version\", cmd.Version),\n\t\tzap.String(\"branch\", cmd.Branch),\n\t\tzap.String(\"commit\", cmd.Commit))\n\tcmd.Logger.Info(\"Go runtime\",\n\t\tzap.String(\"version\", runtime.Version()),\n\t\tzap.Int(\"maxprocs\", runtime.GOMAXPROCS(0)))\n\n\t// If there was an error on startup when creating the logger, output it now.\n\tif logErr != nil {\n\t\tcmd.Logger.Error(\"Unable to configure logger\", zap.Error(logErr))\n\t}\n\n\t// Write the PID file.\n    // 写入pid文件\n\tif err := cmd.writePIDFile(options.PIDFile); err != nil {\n\t\treturn fmt.Errorf(\"write pid file: %s\", err)\n\t}\n\tcmd.pidfile = options.PIDFile\n\n\tif config.HTTPD.PprofEnabled {\n\t\t// Turn on block and mutex profiling.\n\t\truntime.SetBlockProfileRate(int(1 * time.Second))\n\t\truntime.SetMutexProfileFraction(1) // Collect every sample\n\t}\n\n\t// Create server from config and start it.\n    // 初始化服务器\n\tbuildInfo := &BuildInfo{\n\t\tVersion: cmd.Version,\n\t\tCommit:  cmd.Commit,\n\t\tBranch:  cmd.Branch,\n\t\tTime:    cmd.BuildTime,\n\t}\n\ts, err := NewServer(config, buildInfo)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"create server: %s\", err)\n\t}\n\ts.Logger = cmd.Logger\n\ts.CPUProfile = options.CPUProfile\n\ts.MemProfile = options.MemProfile\n     // 启动\n\tif err := s.Open(); err != nil {\n\t\treturn fmt.Errorf(\"open server: %s\", err)\n\t}\n\tcmd.Server = s\n\n\t// Begin monitoring the server's error channel.\n\tgo cmd.monitorServerErrors()\n\n\treturn nil\n}\n```\n\n初始化函数NewServer\n\n```go\n// NewServer returns a new instance of Server built from a config.\nfunc NewServer(c *Config, buildInfo *BuildInfo) (*Server, error) {\n\t// We need to ensure that a meta directory always exists even if\n\t// we don't start the meta store.  node.json is always stored under\n\t// the meta directory.\n    // 建立元数据目录，并加权\n\tif err := os.MkdirAll(c.Meta.Dir, 0777); err != nil {\n\t\treturn nil, fmt.Errorf(\"mkdir all: %s\", err)\n\t}\n\n\t// 0.10-rc1 and prior would sometimes put the node.json at the root\n\t// dir which breaks backup/restore and restarting nodes.  This moves\n\t// the file from the root so it's always under the meta dir.\n    //移动和恢复节点信息\n\toldPath := filepath.Join(filepath.Dir(c.Meta.Dir), \"node.json\")\n\tnewPath := filepath.Join(c.Meta.Dir, \"node.json\")\n\t//修改\n\tif _, err := os.Stat(oldPath); err == nil {\n\t\tif err := os.Rename(oldPath, newPath); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n// 从磁盘中加载节点信息\n\tnode, err := influxdb.LoadNode(c.Meta.Dir)\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn nil, err\n\t\t}\n\t\t//不存在则新建\n\t\tnode = influxdb.NewNode(c.Meta.Dir)\n\t}\n\n\t//if err := raftDBExists(c.Meta.Dir); err != nil {\n\t//\treturn nil, err\n\t//}\n\n\t// In 0.10.0 bind-address got moved to the top level. Check\n\t// The old location to keep things backwards compatible\n\tbind := c.BindAddress\n\tif c.Meta.BindAddress != \"\" {\n\t\tbind = c.Meta.BindAddress\n\t}\n\t//判断元数据是否打开\n\tif !c.Data.Enabled && !c.Meta.Enabled {\n\t\treturn nil, fmt.Errorf(\"must run as either meta node or data node or both\")\n\t}\n\t//初始化\n\ts := &Server{\n\t\tbuildInfo: *buildInfo,\n\t\terr:       make(chan error),\n\t\tclosing:   make(chan struct{}),\n\n\t\tNode:        node,\n\t\tBindAddress: bind,\n\n\t\tLogger: logger.New(os.Stderr),\n\n\t\t//MetaClient: meta.NewClient(c.Meta),\n\t\tMetaClient: meta.NewClient(),  \n\n\t\treportingDisabled: c.ReportingDisabled,\n\t\tjoinPeers:         c.Meta.JoinPeers,\n\t\tmetaUseTLS:        c.Meta.HTTPSEnabled,\n\n\t\thttpAPIAddr: c.HTTPD.BindAddress,   // http服务bind地址\n\t\thttpUseTLS:  c.HTTPD.HTTPSEnabled,   //https打开\n\t\ttcpAddr:     bind,\n\n\t\tconfig: c,\n\t}\n\t//初始化元数据服务\n\tif c.Meta.Enabled {\n\t\ts.MetaService = meta.NewService(c.Meta)\n\t\ts.MetaService.Version = s.buildInfo.Version\n\t\ts.MetaService.Node = s.Node\n\t}\n\n\tif c.AdminCluster.Enabled {\n\t\ts.AdminClusterService = admin_cluster.NewService(c.AdminCluster)\n\t\ts.AdminClusterService.Version = s.buildInfo.Version\n\t\ts.AdminClusterService.Handler.MetaClient = s.MetaClient\n\t\ts.AdminClusterService.TCPHandler.MetaClient = s.MetaClient\n\t\ts.AdminClusterService.TCPHandler.Server = s\n\t}\n\t//初始化监控信息\n\ts.Monitor = monitor.New(s, c.Monitor)\n\ts.config.registerDiagnostics(s.Monitor)\n\n\tif c.Data.Enabled {\n        //初始化tsdb\n\t\ts.TSDBStore = tsdb.NewStore(c.Data.Dir)\n\t\ts.TSDBStore.EngineOptions.Config = c.Data\n\n\t\ts.AdminClusterService.TCPHandler.TSDBStore = s.TSDBStore\n\n\t\t// Copy TSDB configuration.\n\t\ts.TSDBStore.EngineOptions.EngineVersion = c.Data.Engine\n\t\ts.TSDBStore.EngineOptions.IndexVersion = c.Data.Index\n\n\t\t// Create the Subscriber service\n\t\ts.Subscriber = subscriber.NewService(c.Subscriber)\n\n\t\t// Set the shard writer\n\t\ts.ShardWriter = cluster.NewShardWriter(time.Duration(c.Cluster.ShardWriterTimeout), c.Cluster.MaxRemoteWriteConnections)\n\n\t\t// Create the hinted handoff service\n\t\ts.HintedHandoff = hh.NewService(c.HintedHandoff, s.ShardWriter, s.MetaClient)\n\t\ts.HintedHandoff.Monitor = s.Monitor\n\n\t\t// Initialize points writer.\n\t\ts.PointsWriter = cluster.NewPointsWriter()\n\t\ts.PointsWriter.WriteTimeout = time.Duration(c.Coordinator.WriteTimeout)\n\t\ts.PointsWriter.TSDBStore = s.TSDBStore\n\t\ts.PointsWriter.ShardWriter = s.ShardWriter\n\t\ts.PointsWriter.HintedHandoff = s.HintedHandoff\n\t\ts.PointsWriter.Node = s.Node\n\n\t\t// Initialize meta executor.\n\t\tmetaExecutor := cluster.NewMetaExecutor()\n\t\tmetaExecutor.MetaClient = s.MetaClient\n\t\tmetaExecutor.Node = s.Node\n\n\t\t// Initialize query executor.\n        // 初始化查询\n\t\ts.QueryExecutor = query.NewExecutor()\n        //初始化集群存储分片\n\t\tclusterShardMapper := &cluster.ClusterShardMapper{\n\t\t\tMetaClient: s.MetaClient,\n\t\t\tTSDBStore:  coordinator.LocalTSDBStore{Store: s.TSDBStore},\n\t\t\tLocalShardMapper: &coordinator.LocalShardMapper{\n\t\t\t\tMetaClient: s.MetaClient,\n\t\t\t\tTSDBStore:  coordinator.LocalTSDBStore{Store: s.TSDBStore},\n\t\t\t},\n\t\t\tNode:               s.Node,\n\t\t\tShardMapperTimeout: time.Duration(s.config.Cluster.ShardMapperTimeout),\n\t\t}\n\t\tclusterShardMapper.WithLogger(s.Logger)\n\t\t//初始化执行\n        //设置最大的查询范围和bucket数目等\n\t\ts.QueryExecutor.StatementExecutor = &cluster.StatementExecutor{\n\t\t\tMetaClient:        s.MetaClient,\n\t\t\tTaskManager:       s.QueryExecutor.TaskManager,\n\t\t\tTSDBStore:         s.TSDBStore,\n\t\t\tShardMapper:       clusterShardMapper,\n\t\t\tMonitor:           s.Monitor,\n\t\t\tPointsWriter:      s.PointsWriter,\n\t\t\tMaxSelectPointN:   c.Coordinator.MaxSelectPointN,\n\t\t\tMaxSelectSeriesN:  c.Coordinator.MaxSelectSeriesN,\n\t\t\tMaxSelectBucketsN: c.Coordinator.MaxSelectBucketsN,\n\t\t\tMetaExecutor:      metaExecutor,\n\t\t}\n\t\ts.QueryExecutor.TaskManager.QueryTimeout = time.Duration(c.Coordinator.QueryTimeout)\n\t\ts.QueryExecutor.TaskManager.LogQueriesAfter = time.Duration(c.Coordinator.LogQueriesAfter)\n\t\ts.QueryExecutor.TaskManager.MaxConcurrentQueries = c.Coordinator.MaxConcurrentQueries\n\n\t\t// Initialize the monitor\n\t\ts.Monitor.Version = s.buildInfo.Version\n\t\ts.Monitor.Commit = s.buildInfo.Commit\n\t\ts.Monitor.Branch = s.buildInfo.Branch\n\t\ts.Monitor.BuildTime = s.buildInfo.Time\n\t\ts.Monitor.PointsWriter = (*monitorPointsWriter)(s.PointsWriter)\n\t}\n\n\treturn s, nil\n}\n```\n\nopen启动服务：\n\n```go\n// Open opens the meta and data store and all services.\nfunc (s *Server) Open() error {\n\t// Start profiling, if set.\n    // linux profile\n\tstartProfile(s.CPUProfile, s.MemProfile)\n\n\t// Open shared TCP connection.\n    // 启动tcp连接\n\tln, err := net.Listen(\"tcp\", s.BindAddress)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"listen: %s\", err)\n\t}\n\ts.Listener = ln\n\n\t// Multiplex listener.\n    // 启动多路复用器\n\tmux := tcp.NewMux()\n\ts.Mux = mux\n\tgo mux.Serve(ln)\n\n\tif s.MetaService != nil {\n        //元数据服务raftlistener初始化\n\t\ts.MetaService.RaftListener = mux.Listen(meta.MuxHeader)\n\n\t\t// Configure logging for all services and clients.\n\t\tif s.config.Meta.LoggingEnabled {\n\t\t\ts.MetaService.WithLogger(s.Logger)\n\t\t}\n\n\t\t// Open meta service.\n        //元数据服务启动\n\t\tif err := s.MetaService.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open meta service: %s\", err)\n\t\t}\n\t\tgo s.monitorErrorChan(s.MetaService.Err())\n\t}\n\n\tif s.AdminClusterService != nil {\n\t\t// Configure logging for all services and clients.\n\t\tif s.config.AdminCluster.ClusterTracing {\n\t\t\ts.AdminClusterService.WithLogger(s.Logger)\n\t\t}\n\t\t// TCP listen\n\t\ts.AdminClusterService.TCPHandler.Listener = s.Mux.Listen(admin_cluster.MuxHeader)\n\n\t\t// Open admin cluster service.\n        //启动集群admin_cluster服务\n\t\tif err := s.AdminClusterService.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open admin cluster service: %s\", err)\n\t\t}\n\t}\n\n\t// initialize MetaClient.\n    //初始化元数据客户端，用于设置集群功能，加入集群等功能。\n\tif err = s.initializeMetaClient(); err != nil {\n\t\treturn err\n\t}\n\n\t// Start the reporting service, if not disabled.\n\t//if !s.reportingDisabled {\n\t//\tgo s.startServerReporting()\n\t//}\n\n\treturn nil\n}\n```\n\ninitializeMetaClient函数中：\n\n```go\n// initializeMetaClient will set the MetaClient and join the node to the cluster if needed\nfunc (s *Server) initializeMetaClient() error {\n\t// It's the first time starting up and we need to either join\n\t// the cluster or initialize this node as the first member\n    //如果每天joinpeers，则返回\n\tif len(s.joinPeers) == 0 {\n\t\t// start up a new single node cluster\n\t\tif s.MetaService == nil {\n\t\t\treturn fmt.Errorf(\"server not set to join existing cluster must run also as a meta node\")\n\t\t}\n\t\ts.MetaClient.SetMetaServers([]string{s.MetaService.HTTPAddr()})\n\t\ts.MetaClient.SetTLS(s.metaUseTLS)\n\t} else {\n\t\tvar err error\n\t\tvar joinPeers []string\n\t\tif s.MetaService != nil {\n\t\t\traddr := s.remoteAddr(s.MetaService.HTTPAddr())\n\t\t\tjoinPeers, err = s.filterAddr(s.joinPeers, raddr)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tjoinPeers = s.joinPeers\n\t\t}\n\t\ts.MetaClient.SetMetaServers(joinPeers)\n\t\ts.MetaClient.SetTLS(s.metaUseTLS)\n\t}\n    //打开client\n\tif err := s.MetaClient.Open(); err != nil {\n\t\treturn err\n\t}\n\n\t// if the node ID is > 0 then we need to initialize the metaclient\n\tif s.Node.GetMetaID() > 0 || s.Node.GetDataID() > 0 {\n\t\ts.MetaClient.WaitForDataChanged()\n\t}\n\tif len(s.joinPeers) > 0 {\n\t\ts.MetaClient.SetMetaServers(s.joinPeers)\n\t}\n\tif s.config.Data.Enabled {\n\n\t\tgo func() {\n\t\t\tt := time.NewTicker(time.Second)\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-t.C:\n                    //定时器服务，检查是否打开数据服务\n\t\t\t\t\tif _, err := s.MetaClient.DataNode(s.Node.GetDataID()); err == nil {\n\t\t\t\t\t\toerr := s.OpenDataServer()\n\t\t\t\t\t\tif oerr != nil {\n\t\t\t\t\t\t\ts.Logger.Error(\"failed to open data server.\", zap.Error(oerr))\n\t\t\t\t\t\t\tpanic(\"open data server failed\")\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.Logger.Info(\"data server started\", zap.Uint64(\"node id\", s.Node.GetDataID()))\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\tcase <-s.closing:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t}\n\treturn nil\n}\n```\n\n如果找到数据节点，则启动opendataServer函数，启动数据服务：\n\n```go\nfunc (s *Server) OpenDataServer() error {\n\tif s.TSDBStore != nil && !s.DataServicesOpened {\n\t\ts.DataServicesOpened = true\n\t\t// Append services.\n         // 启动集群服务，初始化所有的服务\n\t\ts.appendClusterService(s.config.Cluster)\n\t\ts.appendMonitorService()\n\t\ts.appendPrecreatorService(s.config.Precreator)\n\t\ts.appendSnapshotterService()\n\t\ts.appendContinuousQueryService(s.config.ContinuousQuery)\n\t\ts.appendAntiEntropyService(s.config.AntiEntropy)\n        // http服务\n\t\ts.appendHTTPDService(s.config.HTTPD)\n\t\ts.appendStorageService(s.config.Storage)\n        //RetentionPolicy\n\t\ts.appendRetentionPolicyService(s.config.Retention)\n\t\tfor _, i := range s.config.GraphiteInputs {\n\t\t\tif err := s.appendGraphiteService(i); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tfor _, i := range s.config.CollectdInputs {\n\t\t\ts.appendCollectdService(i)\n\t\t}\n\t\tfor _, i := range s.config.OpenTSDBInputs {\n\t\t\tif err := s.appendOpenTSDBService(i); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tfor _, i := range s.config.UDPInputs {\n\t\t\ts.appendUDPService(i)\n\t\t}\n\t\t\n\t\ts.Subscriber.MetaClient = s.MetaClient\n\t\ts.PointsWriter.MetaClient = s.MetaClient\n\t\ts.Monitor.MetaClient = s.MetaClient\n\t\ts.ShardWriter.MetaClient = s.MetaClient\n\t\ts.HintedHandoff.MetaClient = s.MetaClient\n\n\t\ts.ClusterService.Listener = s.Mux.Listen(cluster.MuxHeader)\n\t\ts.SnapshotterService.Listener = s.Mux.Listen(snapshotter.MuxHeader)\n\n\t\t// Configure logging for all services and clients.\n\t\tif s.config.Meta.LoggingEnabled {\n\t\t\ts.MetaClient.WithLogger(s.Logger)\n\t\t}\n\t\ts.TSDBStore.WithLogger(s.Logger)\n\t\tif s.config.Data.QueryLogEnabled {\n\t\t\ts.QueryExecutor.WithLogger(s.Logger)\n\t\t}\n\t\ts.PointsWriter.WithLogger(s.Logger)\n\t\ts.Subscriber.WithLogger(s.Logger)\n\t\ts.HintedHandoff.WithLogger(s.Logger)\n\t\tfor _, svc := range s.Services {\n\t\t\tsvc.WithLogger(s.Logger)\n\t\t}\n\t\ts.SnapshotterService.WithLogger(s.Logger)\n\t\ts.Monitor.WithLogger(s.Logger)\n\n\t\t// Open TSDB store.\n        // tsdb启动\n\t\tif err := s.TSDBStore.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open tsdb store: %s\", err)\n\t\t}\n\n\t\t// Open the hinted handoff service\n\t\tif err := s.HintedHandoff.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open hinted handoff: %s\", err)\n\t\t}\n\n\t\t// Open the subscriber service\n\t\tif err := s.Subscriber.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open subscriber: %s\", err)\n\t\t}\n\n\t\t// Open the points writer service\n\t\tif err := s.PointsWriter.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open points writer: %s\", err)\n\t\t}\n\n\t\ts.PointsWriter.AddWriteSubscriber(s.Subscriber.Points())\n\n\t\tfor _, service := range s.Services {\n            //将注册的服务都启动起来，这边调用每个服务的open方法启动起来\n\t\t\tif err := service.Open(); err != nil {\n\t\t\t\treturn fmt.Errorf(\"open service: %s\", err)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\n\t}\n\tif s.TSDBStore == nil {\n\t\treturn fmt.Errorf(\"Data server is not enabled\")\n\t}\n\treturn nil\n}\n```\n\n服务主要有下面这些：\n\n```shell\ncluster\nmonitor\nprecreator\nsnapshotter\ncontinuousquery\nantientropy\nhttp\nstorage\nretentionpolicy\ngraphite\ncollectd\nopentsdb\nudp\nhh\nmeta\n```\n\n每个服务都有open函数，分别启动。\n\n举例来说：\n\nhttp服务初始化函数NewService:\n\n```go\n// NewService returns a new instance of Service.\nfunc NewService(c Config) *Service {\n\ts := &Service{\n\t\taddr:           c.BindAddress,\n\t\thttps:          c.HTTPSEnabled,\n\t\tcert:           c.HTTPSCertificate,\n\t\tkey:            c.HTTPSPrivateKey,\n\t\tlimit:          c.MaxConnectionLimit,\n\t\terr:            make(chan error),\n\t\tunixSocket:     c.UnixSocketEnabled,\n\t\tunixSocketPerm: uint32(c.UnixSocketPermissions),\n\t\tbindSocket:     c.BindSocket,\n\t\tHandler:        NewHandler(c),  //服务启动处理函数\n\t\tLogger:         zap.NewNop(),\n\t}\n\tif s.key == \"\" {\n\t\ts.key = s.cert\n\t}\n\tif c.UnixSocketGroup != nil {\n\t\ts.unixSocketGroup = int(*c.UnixSocketGroup)\n\t}\n\ts.Handler.Logger = s.Logger\n\treturn s\n}\n```\n\nhandler函数：\n\n```go\nfunc NewHandler(c Config) *Handler {\n\th := &Handler{\n\t\tmux:            pat.New(),\n\t\tConfig:         &c,\n\t\tLogger:         zap.NewNop(),\n\t\tCLFLogger:      log.New(os.Stderr, \"[httpd] \", 0),\n\t\tStore:          storage.NewStore(),\n\t\tstats:          &Statistics{},\n\t\trequestTracker: NewRequestTracker(),\n\t\tsema:           make(chan struct{}, 100),\n\t}\n\n\t// Limit the number of concurrent & enqueued write requests.\n\th.writeThrottler = NewThrottler(c.MaxConcurrentWriteLimit, c.MaxEnqueuedWriteLimit)\n\th.writeThrottler.EnqueueTimeout = c.EnqueuedWriteTimeout\n\n\t// Disable the write log if they have been suppressed.\n\twriteLogEnabled := c.LogEnabled\n\tif c.SuppressWriteLog {\n\t\twriteLogEnabled = false\n\t}\n    //所有服务查询的入口函数在这边处理\n    h.AddRoutes([]Route{\n\t\tRoute{\n\t\t\t\"query-options\", // Satisfy CORS checks.\n\t\t\t\"OPTIONS\", \"/query\", false, true, h.serveOptions,\n\t\t},\n\t\tRoute{\n\t\t\t\"query\", // Query serving route.\n\t\t\t\"GET\", \"/query\", true, true, h.serveQuery,\n\t\t},\n\t\tRoute{\n\t\t\t\"query\", // Query serving route.\n\t\t\t\"POST\", \"/query\", true, true, h.serveQuery,\n\t\t},\n\t\tRoute{\n            ....\n\t\"GET\", \"/metrics\", false, true, promhttp.Handler().ServeHTTP,\n\t\t},\n\t}...)\n\n\treturn h\n}\n            \n```\n\n查询函数serveQuery；\n\n```go\n// serveQuery parses an incoming query and, if valid, executes the query.\nfunc (h *Handler) serveQuery(w http.ResponseWriter, r *http.Request, user meta.User) {\n\tatomic.AddInt64(&h.stats.QueryRequests, 1)\n\tdefer func(start time.Time) {\n\t\tatomic.AddInt64(&h.stats.QueryRequestDuration, time.Since(start).Nanoseconds())\n\t}(time.Now())\n\th.requestTracker.Add(r, user)\n\n\t// Retrieve the underlying ResponseWriter or initialize our own.\n\trw, ok := w.(ResponseWriter)\n\tif !ok {\n\t\trw = NewResponseWriter(w, r)\n\t}\n\n\t// Retrieve the node id the query should be executed on.\n\tnodeID, _ := strconv.ParseUint(r.FormValue(\"node_id\"), 10, 64)\n\n\tvar qr io.Reader\n\t// Attempt to read the form value from the \"q\" form value.\n\tif qp := strings.TrimSpace(r.FormValue(\"q\")); qp != \"\" {\n\t\tqr = strings.NewReader(qp)\n\t} else if r.MultipartForm != nil && r.MultipartForm.File != nil {\n\t\t// If we have a multipart/form-data, try to retrieve a file from 'q'.\n\t\tif fhs := r.MultipartForm.File[\"q\"]; len(fhs) > 0 {\n\t\t\tf, err := fhs[0].Open()\n\t\t\tif err != nil {\n\t\t\t\th.httpError(rw, err.Error(), http.StatusBadRequest)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer f.Close()\n\t\t\tqr = f\n\t\t}\n\t}\n\n\tif qr == nil {\n\t\th.httpError(rw, `missing required parameter \"q\"`, http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tepoch := strings.TrimSpace(r.FormValue(\"epoch\"))\n\t// 初始化查询解析器\n\tp := influxql.NewParser(qr)\n\tdb := r.FormValue(\"db\")\n\n\t// Sanitize the request query params so it doesn't show up in the response logger.\n\t// Do this before anything else so a parsing error doesn't leak passwords.\n\tsanitize(r)\n\n\t// Parse the parameters\n\trawParams := r.FormValue(\"params\")\n\tif rawParams != \"\" {\n\t\tvar params map[string]interface{}\n\t\tdecoder := json.NewDecoder(strings.NewReader(rawParams))\n\t\tdecoder.UseNumber()\n\t\tif err := decoder.Decode(&params); err != nil {\n\t\t\th.httpError(rw, \"error parsing query parameters: \"+err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\t// Convert json.Number into int64 and float64 values\n\t\tfor k, v := range params {\n\t\t\tif v, ok := v.(json.Number); ok {\n\t\t\t\tvar err error\n\t\t\t\tif strings.Contains(string(v), \".\") {\n\t\t\t\t\tparams[k], err = v.Float64()\n\t\t\t\t} else {\n\t\t\t\t\tparams[k], err = v.Int64()\n\t\t\t\t}\n\n\t\t\t\tif err != nil {\n\t\t\t\t\th.httpError(rw, \"error parsing json value: \"+err.Error(), http.StatusBadRequest)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tp.SetParams(params)\n\t}\n\n\t// Parse query from query string.\n    //开始解析query查询语句\n\tq, err := p.ParseQuery()\n\tif err != nil {\n\t\th.httpError(rw, \"error parsing query: \"+err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\t// Check authorization.\n    //检查认证信息\n\tif h.Config.AuthEnabled {\n\t\tif err := h.QueryAuthorizer.AuthorizeQuery(user, q, db); err != nil {\n\t\t\tif err, ok := err.(meta.ErrAuthorize); ok {\n\t\t\t\th.Logger.Info(\"Unauthorized request\",\n\t\t\t\t\tzap.String(\"user\", err.User),\n\t\t\t\t\tzap.Stringer(\"query\", err.Query),\n\t\t\t\t\tlogger.Database(err.Database))\n\t\t\t}\n\t\t\th.httpError(rw, \"error authorizing query: \"+err.Error(), http.StatusForbidden)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Parse chunk size. Use default if not provided or unparsable.\n\tchunked := r.FormValue(\"chunked\") == \"true\"\n\tchunkSize := DefaultChunkSize\n\tif chunked {\n\t\tif n, err := strconv.ParseInt(r.FormValue(\"chunk_size\"), 10, 64); err == nil && int(n) > 0 {\n\t\t\tchunkSize = int(n)\n\t\t}\n\t}\n\n\t// Parse whether this is an async command.\n\tasync := r.FormValue(\"async\") == \"true\"\n//参数实例化\n\topts := query.ExecutionOptions{\n\t\tDatabase:        db,\n\t\tRetentionPolicy: r.FormValue(\"rp\"),\n\t\tChunkSize:       chunkSize,\n\t\tReadOnly:        r.Method == \"GET\",\n\t\tNodeID:          nodeID,\n\t}\n\n\tif h.Config.AuthEnabled {\n\t\t// The current user determines the authorized actions.\n\t\topts.Authorizer = user\n\t} else {\n\t\t// Auth is disabled, so allow everything.\n\t\topts.Authorizer = query.OpenAuthorizer\n\t}\n\n\t// Make sure if the client disconnects we signal the query to abort\n\tvar closing chan struct{}\n\tif !async {\n\t\tclosing = make(chan struct{})\n\t\tif notifier, ok := w.(http.CloseNotifier); ok {\n\t\t\t// CloseNotify() is not guaranteed to send a notification when the query\n\t\t\t// is closed. Use this channel to signal that the query is finished to\n\t\t\t// prevent lingering goroutines that may be stuck.\n\t\t\tdone := make(chan struct{})\n\t\t\tdefer close(done)\n\n\t\t\tnotify := notifier.CloseNotify()\n\t\t\tgo func() {\n\t\t\t\t// Wait for either the request to finish\n\t\t\t\t// or for the client to disconnect\n\t\t\t\tselect {\n\t\t\t\tcase <-done:\n\t\t\t\tcase <-notify:\n\t\t\t\t\tclose(closing)\n\t\t\t\t}\n\t\t\t}()\n\t\t\topts.AbortCh = done\n\t\t} else {\n\t\t\tdefer close(closing)\n\t\t}\n\t}\n\n\t// Execute query.\n    //执行查询语句\n\tresults := h.QueryExecutor.ExecuteQuery(q, opts, closing)\n\n\t// If we are running in async mode, open a goroutine to drain the results\n\t// and return with a StatusNoContent.\n\tif async {\n\t\tgo h.async(q, results)\n\t\th.writeHeader(w, http.StatusNoContent)\n\t\treturn\n\t}\n\n\t// if we're not chunking, this will be the in memory buffer for all results before sending to client\n\tresp := Response{Results: make([]*query.Result, 0)}\n\n\t// Status header is OK once this point is reached.\n\t// Attempt to flush the header immediately so the client gets the header information\n\t// and knows the query was accepted.\n\th.writeHeader(rw, http.StatusOK)\n\tif w, ok := w.(http.Flusher); ok {\n\t\tw.Flush()\n\t}\n\n\t// pull all results from the channel\n\trows := 0\n\tfor r := range results {\n\t\t// Ignore nil results.\n\t\tif r == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// if requested, convert result timestamps to epoch\n\t\tif epoch != \"\" {\n\t\t\tconvertToEpoch(r, epoch)\n\t\t}\n\n\t\t// Write out result immediately if chunked.\n\t\tif chunked {\n\t\t\tn, _ := rw.WriteResponse(Response{\n\t\t\t\tResults: []*query.Result{r},\n\t\t\t})\n\t\t\tatomic.AddInt64(&h.stats.QueryRequestBytesTransmitted, int64(n))\n\t\t\tw.(http.Flusher).Flush()\n\t\t\tcontinue\n\t\t}\n\n\t\t// Limit the number of rows that can be returned in a non-chunked\n\t\t// response.  This is to prevent the server from going OOM when\n\t\t// returning a large response.  If you want to return more than the\n\t\t// default chunk size, then use chunking to process multiple blobs.\n\t\t// Iterate through the series in this result to count the rows and\n\t\t// truncate any rows we shouldn't return.\n        //最大限制数目\n\t\tif h.Config.MaxRowLimit > 0 {\n\t\t\tfor i, series := range r.Series {\n\t\t\t\tn := h.Config.MaxRowLimit - rows\n\t\t\t\tif n < len(series.Values) {\n\t\t\t\t\t// We have reached the maximum number of values. Truncate\n\t\t\t\t\t// the values within this row.\n\t\t\t\t\tseries.Values = series.Values[:n]\n\t\t\t\t\t// Since this was truncated, it will always be a partial return.\n\t\t\t\t\t// Add this so the client knows we truncated the response.\n\t\t\t\t\tseries.Partial = true\n\t\t\t\t}\n\t\t\t\trows += len(series.Values)\n\n\t\t\t\tif rows >= h.Config.MaxRowLimit {\n\t\t\t\t\t// Drop any remaining series since we have already reached the row limit.\n\t\t\t\t\tif i < len(r.Series) {\n\t\t\t\t\t\tr.Series = r.Series[:i+1]\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// It's not chunked so buffer results in memory.\n\t\t// Results for statements need to be combined together.\n\t\t// We need to check if this new result is for the same statement as\n\t\t// the last result, or for the next statement\n\t\tl := len(resp.Results)\n\t\tif l == 0 {\n\t\t\tresp.Results = append(resp.Results, r)\n\t\t} else if resp.Results[l-1].StatementID == r.StatementID {\n\t\t\tif r.Err != nil {\n\t\t\t\tresp.Results[l-1] = r\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tcr := resp.Results[l-1]\n\t\t\trowsMerged := 0\n\t\t\tif len(cr.Series) > 0 {\n\t\t\t\tlastSeries := cr.Series[len(cr.Series)-1]\n\n\t\t\t\tfor _, row := range r.Series {\n\t\t\t\t\tif !lastSeries.SameSeries(row) {\n\t\t\t\t\t\t// Next row is for a different series than last.\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\t// Values are for the same series, so append them.\n\t\t\t\t\tlastSeries.Values = append(lastSeries.Values, row.Values...)\n\t\t\t\t\trowsMerged++\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Append remaining rows as new rows.\n\t\t\tr.Series = r.Series[rowsMerged:]\n\t\t\tcr.Series = append(cr.Series, r.Series...)\n\t\t\tcr.Messages = append(cr.Messages, r.Messages...)\n\t\t\tcr.Partial = r.Partial\n\t\t} else {\n\t\t\tresp.Results = append(resp.Results, r)\n\t\t}\n\n\t\t// Drop out of this loop and do not process further results when we hit the row limit.\n\t\tif h.Config.MaxRowLimit > 0 && rows >= h.Config.MaxRowLimit {\n\t\t\t// If the result is marked as partial, remove that partial marking\n\t\t\t// here. While the series is partial and we would normally have\n\t\t\t// tried to return the rest in the next chunk, we are not using\n\t\t\t// chunking and are truncating the series so we don't want to\n\t\t\t// signal to the client that we plan on sending another JSON blob\n\t\t\t// with another result.  The series, on the other hand, still\n\t\t\t// returns partial true if it was truncated or had more data to\n\t\t\t// send in a future chunk.\n\t\t\tr.Partial = false\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// If it's not chunked we buffered everything in memory, so write it out\n\tif !chunked {\n\t\tn, _ := rw.WriteResponse(resp)\n\t\tatomic.AddInt64(&h.stats.QueryRequestBytesTransmitted, int64(n))\n\t}\n}\n```\n\n函数ParseQuery函数解析query：\n\n```go\n// ParseQuery parses an InfluxQL string and returns a Query AST object.\nfunc (p *Parser) ParseQuery() (*Query, error) {\n\tvar statements Statements\n\tsemi := true\n\n\tfor {\n\t\tif tok, pos, lit := p.ScanIgnoreWhitespace(); tok == EOF {//如果tok==EOF的时候，正常解析完成返回;\n\t\t\treturn &Query{Statements: statements}, nil\n\t\t} else if tok == SEMICOLON {\n\t\t\tsemi = true\n\t\t} else {\n\t\t\tif !semi {\n\t\t\t\treturn nil, newParseError(tokstr(tok, lit), []string{\";\"}, pos)\n\t\t\t}\n\t\t\tp.Unscan()\n\t\t\ts, err := p.ParseStatement() //解析词\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tstatements = append(statements, s) //返回解析的statments\n\t\t\tsemi = false\n\t\t}\n\t}\n}\n```\n\n执行解析的executeQuery函数：\n\n```go\n// ExecuteQuery executes each statement within a query.\nfunc (e *Executor) ExecuteQuery(query *influxql.Query, opt ExecutionOptions, closing chan struct{}) <-chan *Result {\n\tresults := make(chan *Result)\n\tgo e.executeQuery(query, opt, closing, results)  //执行查询语句\n\treturn results\n}\n```\n\n调用executeQuery函数：\n\n```go\nfunc (e *Executor) executeQuery(query *influxql.Query, opt ExecutionOptions, closing <-chan struct{}, results chan *Result) {\n\tdefer close(results)\n\tdefer e.recover(query, results)\n\n\tatomic.AddInt64(&e.stats.ActiveQueries, 1)\n\tatomic.AddInt64(&e.stats.ExecutedQueries, 1)\n\tdefer func(start time.Time) {\n\t\tatomic.AddInt64(&e.stats.ActiveQueries, -1)\n\t\tatomic.AddInt64(&e.stats.FinishedQueries, 1)\n\t\tatomic.AddInt64(&e.stats.QueryExecutionDuration, time.Since(start).Nanoseconds())\n\t}(time.Now())\n// 使用taskManager来管理查询query,返回一个channel，当query完成running的时候。\n\tctx, detach, err := e.TaskManager.AttachQuery(query, opt, closing)\n\tif err != nil {\n\t\tselect {\n\t\tcase results <- &Result{Err: err}:\n\t\tcase <-opt.AbortCh:\n\t\t}\n\t\treturn\n\t}\n\tdefer detach()\n\n\t// Setup the execution context that will be used when executing statements.\n\tctx.Results = results\n\n\tvar i int\nLOOP:\n\tfor ; i < len(query.Statements); i++ {\n\t\tctx.statementID = i\n\t\tstmt := query.Statements[i]\n\n\t\t// If a default database wasn't passed in by the caller, check the statement.\n\t\tdefaultDB := opt.Database\n\t\tif defaultDB == \"\" {\n\t\t\tif s, ok := stmt.(influxql.HasDefaultDatabase); ok {\n\t\t\t\tdefaultDB = s.DefaultDatabase()\n\t\t\t}\n\t\t}\n\n\t\t// Do not let queries manually use the system measurements. If we find\n\t\t// one, return an error. This prevents a person from using the\n\t\t// measurement incorrectly and causing a panic.\n\t\tif stmt, ok := stmt.(*influxql.SelectStatement); ok {\n\t\t\tfor _, s := range stmt.Sources {\n\t\t\t\tswitch s := s.(type) {\n\t\t\t\tcase *influxql.Measurement:\n\t\t\t\t\tif influxql.IsSystemName(s.Name) {\n\t\t\t\t\t\tcommand := \"the appropriate meta command\"\n\t\t\t\t\t\tswitch s.Name {\n\t\t\t\t\t\tcase \"_fieldKeys\":\n\t\t\t\t\t\t\tcommand = \"SHOW FIELD KEYS\"\n\t\t\t\t\t\tcase \"_measurements\":\n\t\t\t\t\t\t\tcommand = \"SHOW MEASUREMENTS\"\n\t\t\t\t\t\tcase \"_series\":\n\t\t\t\t\t\t\tcommand = \"SHOW SERIES\"\n\t\t\t\t\t\tcase \"_tagKeys\":\n\t\t\t\t\t\t\tcommand = \"SHOW TAG KEYS\"\n\t\t\t\t\t\tcase \"_tags\":\n\t\t\t\t\t\t\tcommand = \"SHOW TAG VALUES\"\n\t\t\t\t\t\t}\n\t\t\t\t\t\tresults <- &Result{\n\t\t\t\t\t\t\tErr: fmt.Errorf(\"unable to use system source '%s': use %s instead\", s.Name, command),\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak LOOP\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Rewrite statements, if necessary.\n\t\t// This can occur on meta read statements which convert to SELECT statements.\n\t\tnewStmt, err := RewriteStatement(stmt)\n\t\tif err != nil {\n\t\t\tresults <- &Result{Err: err}\n\t\t\tbreak\n\t\t}\n\t\tstmt = newStmt\n\n\t\t// Normalize each statement if possible.\n\t\tif normalizer, ok := e.StatementExecutor.(StatementNormalizer); ok {\n\t\t\tif err := normalizer.NormalizeStatement(stmt, defaultDB, opt.RetentionPolicy); err != nil {\n\t\t\t\tif err := ctx.send(&Result{Err: err}); err == ErrQueryAborted {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t// Log each normalized statement.\n\t\tif !ctx.Quiet {\n\t\t\te.Logger.Info(\"Executing query\", zap.Stringer(\"query\", stmt))\n\t\t}\n\n\t\t// Send any other statements to the underlying statement executor.\n\t\terr = e.StatementExecutor.ExecuteStatement(stmt, ctx)\n\t\tif err == ErrQueryInterrupted {\n\t\t\t// Query was interrupted so retrieve the real interrupt error from\n\t\t\t// the query task if there is one.\n\t\t\tif qerr := ctx.Err(); qerr != nil {\n\t\t\t\terr = qerr\n\t\t\t}\n\t\t}\n\n\t\t// Send an error for this result if it failed for some reason.\n\t\tif err != nil {\n\t\t\tif err := ctx.send(&Result{\n\t\t\t\tStatementID: i,\n\t\t\t\tErr:         err,\n\t\t\t}); err == ErrQueryAborted {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Stop after the first error.\n\t\t\tbreak\n\t\t}\n\n\t\t// Check if the query was interrupted during an uninterruptible statement.\n\t\tinterrupted := false\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tinterrupted = true\n\t\tdefault:\n\t\t\t// Query has not been interrupted.\n\t\t}\n\n\t\tif interrupted {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Send error results for any statements which were not executed.\n\tfor ; i < len(query.Statements)-1; i++ {\n\t\tif err := ctx.send(&Result{\n\t\t\tStatementID: i,\n\t\t\tErr:         ErrNotExecuted,\n\t\t}); err == ErrQueryAborted {\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n\n函数AttachQuery用于管理当前查询的query的状态\n\n```go\n// AttachQuery attaches a running query to be managed by the TaskManager.\n// Returns the query id of the newly attached query or an error if it was\n// unable to assign a query id or attach the query to the TaskManager.\n// This function also returns a channel that will be closed when this\n// query finishes running.\n//\n// After a query finishes running, the system is free to reuse a query id.\nfunc (t *TaskManager) AttachQuery(q *influxql.Query, opt ExecutionOptions, interrupt <-chan struct{}) (*ExecutionContext, func(), error) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\n\tif t.shutdown {\n\t\treturn nil, nil, ErrQueryEngineShutdown\n\t}\n\n\tif t.MaxConcurrentQueries > 0 && len(t.queries) >= t.MaxConcurrentQueries {\n\t\treturn nil, nil, ErrMaxConcurrentQueriesLimitExceeded(len(t.queries), t.MaxConcurrentQueries)\n\t}\n\n\tqid := t.nextID\n    //初始化task\n\tquery := &Task{\n\t\tquery:     q.String(),\n\t\tdatabase:  opt.Database,\n\t\tstatus:    RunningTask,\n\t\tstartTime: time.Now(),\n\t\tclosing:   make(chan struct{}),\n\t\tmonitorCh: make(chan error),\n\t}\n\tt.queries[qid] = query\n\n\tgo t.waitForQuery(qid, query.closing, interrupt, query.monitorCh)//开启协程来监听query是否结束。\n\tif t.LogQueriesAfter != 0 {\n\t\tgo query.monitor(func(closing <-chan struct{}) error {\n\t\t\ttimer := time.NewTimer(t.LogQueriesAfter)//检测到慢查询的时候，报警。\n\t\t\tdefer timer.Stop()\n\n\t\t\tselect {\n\t\t\tcase <-timer.C:\n\t\t\t\tt.Logger.Warn(fmt.Sprintf(\"Detected slow query: %s (qid: %d, database: %s, threshold: %s)\",\n\t\t\t\t\tquery.query, qid, query.database, t.LogQueriesAfter))\n\t\t\tcase <-closing:\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\tt.nextID++\n    //初始化一个ctx上下文\n\tctx := &ExecutionContext{\n\t\tContext:          context.Background(),\n\t\tQueryID:          qid,\n\t\ttask:             query,\n\t\tExecutionOptions: opt,\n\t}\n\tctx.watch()\n   \t// detach query，从查询table中去除。\n\treturn ctx, func() { t.DetachQuery(qid) }, nil\n    \n}\n```\n\n将解析出来的statement执行函数ExecuteStatement\n\n```go\n// ExecuteStatement executes the given statement with the given execution context.\nfunc (e *StatementExecutor) ExecuteStatement(stmt influxql.Statement, ctx *query.ExecutionContext) error {\n\t// Select statements are handled separately so that they can be streamed.\n    //特殊处理select查询\n\tif stmt, ok := stmt.(*influxql.SelectStatement); ok {\n\t\treturn e.executeSelectStatement(stmt, ctx)\n\t}\n\n\tvar rows models.Rows\n\tvar messages []*query.Message\n\tvar err error\n\tswitch stmt := stmt.(type) {\n     //根据每个类别分别处理不同type的查询语句，有点多，自己看下吧~~~\n\tcase *influxql.AlterRetentionPolicyStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeAlterRetentionPolicyStatement(stmt)\n\tcase *influxql.CreateContinuousQueryStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateContinuousQueryStatement(stmt)\n\tcase *influxql.CreateDatabaseStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateDatabaseStatement(stmt)\n\tcase *influxql.CreateRetentionPolicyStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateRetentionPolicyStatement(stmt)\n\tcase *influxql.CreateSubscriptionStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateSubscriptionStatement(stmt)\n\tcase *influxql.CreateUserStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateUserStatement(stmt)\n\tcase *influxql.DeleteSeriesStatement:\n\t\terr = e.executeDeleteSeriesStatement(stmt, ctx.Database)\n\tcase *influxql.DropContinuousQueryStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropContinuousQueryStatement(stmt)\n\tcase *influxql.DropDatabaseStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropDatabaseStatement(stmt)\n\tcase *influxql.DropMeasurementStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropMeasurementStatement(stmt, ctx.Database)\n\tcase *influxql.DropSeriesStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropSeriesStatement(stmt, ctx.Database)\n\tcase *influxql.DropRetentionPolicyStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropRetentionPolicyStatement(stmt)\n\tcase *influxql.DropShardStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropShardStatement(stmt)\n\tcase *influxql.DropSubscriptionStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropSubscriptionStatement(stmt)\n\tcase *influxql.DropUserStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropUserStatement(stmt)\n\tcase *influxql.ExplainStatement:\n\t\tif stmt.Analyze {\n\t\t\trows, err = e.executeExplainAnalyzeStatement(stmt, ctx)\n\t\t} else {\n\t\t\trows, err = e.executeExplainStatement(stmt, ctx)\n\t\t}\n\tcase *influxql.GrantStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeGrantStatement(stmt)\n\tcase *influxql.GrantAdminStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeGrantAdminStatement(stmt)\n\tcase *influxql.RevokeStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeRevokeStatement(stmt)\n\tcase *influxql.RevokeAdminStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeRevokeAdminStatement(stmt)\n\tcase *influxql.ShowContinuousQueriesStatement:\n\t\trows, err = e.executeShowContinuousQueriesStatement(stmt)\n\tcase *influxql.ShowDatabasesStatement:\n\t\trows, err = e.executeShowDatabasesStatement(stmt, ctx)\n\tcase *influxql.ShowDiagnosticsStatement:\n\t\trows, err = e.executeShowDiagnosticsStatement(stmt)\n\tcase *influxql.ShowGrantsForUserStatement:\n\t\trows, err = e.executeShowGrantsForUserStatement(stmt)\n\tcase *influxql.ShowMeasurementsStatement:\n\t\treturn e.executeShowMeasurementsStatement(stmt, ctx)\n\tcase *influxql.ShowMeasurementCardinalityStatement:\n\t\trows, err = e.executeShowMeasurementCardinalityStatement(stmt)\n\tcase *influxql.ShowRetentionPoliciesStatement:\n\t\trows, err = e.executeShowRetentionPoliciesStatement(stmt)\n\tcase *influxql.ShowSeriesCardinalityStatement:\n\t\trows, err = e.executeShowSeriesCardinalityStatement(stmt)\n\tcase *influxql.ShowShardsStatement:\n\t\trows, err = e.executeShowShardsStatement(stmt)\n\tcase *influxql.ShowShardGroupsStatement:\n\t\trows, err = e.executeShowShardGroupsStatement(stmt)\n\tcase *influxql.ShowStatsStatement:\n\t\trows, err = e.executeShowStatsStatement(stmt)\n\tcase *influxql.ShowSubscriptionsStatement:\n\t\trows, err = e.executeShowSubscriptionsStatement(stmt)\n\tcase *influxql.ShowTagKeysStatement:\n\t\treturn e.executeShowTagKeys(stmt, ctx)\n\tcase *influxql.ShowTagValuesStatement:\n\t\treturn e.executeShowTagValues(stmt, ctx)\n\tcase *influxql.ShowUsersStatement:\n\t\trows, err = e.executeShowUsersStatement(stmt)\n\tcase *influxql.SetPasswordUserStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeSetPasswordUserStatement(stmt)\n\tcase *influxql.ShowQueriesStatement, *influxql.KillQueryStatement:\n\t\t// Send query related statements to the task manager.\n\t\treturn e.TaskManager.ExecuteStatement(stmt, ctx)\n\tdefault:\n\t\treturn query.ErrInvalidQuery\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn ctx.Send(&query.Result{\n\t\tSeries:   rows,\n\t\tMessages: messages,\n\t})\n}\n```\n\n针对不同类型的statment执行不同的查询tsdb过程。以select查询为例。，executeSelectStatement单独处理，为了能够streamed。\n\n```go\nfunc (e *StatementExecutor) executeSelectStatement(stmt *influxql.SelectStatement, ctx *query.ExecutionContext) error {\n\t//创建迭代器\n    cur, err := e.createIterators(ctx, stmt, ctx.ExecutionOptions)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Generate a row emitter from the iterator set.\n    // 从迭代器中生成一个row emitter，chunkSize大小。\n\tem := query.NewEmitter(cur, ctx.ChunkSize)\n\tdefer em.Close()\n\n\t// Emit rows to the results channel.\n\tvar writeN int64\n\tvar emitted bool\n\n\tvar pointsWriter *BufferedPointsWriter\n\tif stmt.Target != nil {\n        //初始化\n\t\tpointsWriter = NewBufferedPointsWriter(e.PointsWriter, stmt.Target.Measurement.Database, stmt.Target.Measurement.RetentionPolicy, 10000)\n\t}\n\n\tfor {\n        // 查询数据\n\t\trow, partial, err := em.Emit()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t} else if row == nil {\n\t\t\t// Check if the query was interrupted while emitting.\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tdefault:\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\t// Write points back into system for INTO statements.\n        // INTO不为空，则写入这个pointswriter\n\t\tif stmt.Target != nil {\n\t\t\tif err := e.writeInto(pointsWriter, stmt, row); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\twriteN += int64(len(row.Values))\n\t\t\tcontinue\n\t\t}\n\n\t\tresult := &query.Result{\n\t\t\tSeries:  []*models.Row{row},\n\t\t\tPartial: partial,\n\t\t}\n\n\t\t// Send results or exit if closing.\n        //发送结果\n\t\tif err := ctx.Send(result); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\temitted = true\n\t}\n\n\t// Flush remaining points and emit write count if an INTO statement.\n\tif stmt.Target != nil {\n\t\tif err := pointsWriter.Flush(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar messages []*query.Message\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\n\t\treturn ctx.Send(&query.Result{\n\t\t\tMessages: messages,\n\t\t\tSeries: []*models.Row{{\n\t\t\t\tName:    \"result\",\n\t\t\t\tColumns: []string{\"time\", \"written\"},\n\t\t\t\tValues:  [][]interface{}{{time.Unix(0, 0).UTC(), writeN}},\n\t\t\t}},\n\t\t})\n\t}\n\n\t// Always emit at least one result.\n\tif !emitted {\n\t\treturn ctx.Send(&query.Result{\n\t\t\tSeries: make([]*models.Row, 0),\n\t\t})\n\t}\n\n\treturn nil\n}\n```\n\nemit函数查询获取数据并返回:\n\n```go\n// Emit returns the next row from the iterators.\nfunc (e *Emitter) Emit() (*models.Row, bool, error) {\n\t// Continually read from the cursor until it is exhausted.\n\tfor {\n\t\t// Scan the next row. If there are no rows left, return the current row.\n\t\tvar row Row\n\t\tif !e.cur.Scan(&row) {\n\t\t\tif err := e.cur.Err(); err != nil {\n\t\t\t\treturn nil, false, err\n\t\t\t}\n\t\t\tr := e.row\n\t\t\te.row = nil\n\t\t\treturn r, false, nil\n\t\t}\n\n\t\t// If there's no row yet then create one.\n\t\t// If the name and tags match the existing row, append to that row if\n\t\t// the number of values doesn't exceed the chunk size.\n\t\t// Otherwise return existing row and add values to next emitted row.\n\t\tif e.row == nil {\n\t\t\te.createRow(row.Series, row.Values)\n\t\t} else if e.series.SameSeries(row.Series) {\n\t\t\tif e.chunkSize > 0 && len(e.row.Values) >= e.chunkSize {//如果查询数据量大于chunkSize，则返回，同时 partial=true标识。\n\t\t\t\tr := e.row\n\t\t\t\tr.Partial = true\n\t\t\t\te.createRow(row.Series, row.Values)\n\t\t\t\treturn r, true, nil\n\t\t\t}\n\t\t\te.row.Values = append(e.row.Values, row.Values)\n\t\t} else {\n\t\t\tr := e.row\n\t\t\te.createRow(row.Series, row.Values)\n\t\t\treturn r, true, nil\n\t\t}\n\t}\n}\n```\n\n\n\n #### 总结\n\n大概看了下influxdb从启动到服务查询接口的整体流程。以select为例，看了不同的query查询和解析方式类似，都需要走解析查询的。词法解析器是 influxdb自己写的。 底层如何构建的以后再讨论吧。还有很多细节需要自己去看下了。orz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["influxdb"],"categories":["influxdb"]},{"title":"golang","url":"/2017/05/26/gotips/","content":"\n## golang超时问题\n\ngolang中http请求经常遇到的问题，本人也遇到过超时的情况。写个笔记记录下。\n\n当在编写一个Go语言的HTTP服务端或者是客户端时，超时是最容易同时也是最敏感的错误，有很多选择，一个错误可以导致很长时间没有结果，知道网络出现故障，或者进程宕掉。\n\n在分析过程中，发现服务之间调用有EOF的问题，一般情况下是两个服务之间的readtimeout和writetimeout设置超时导致的。当然也有一个keepalive超时的问题。需要保证服务A调用服务B的时候，服务A的keepalive大于服务B的keepalive。\n\npython服务器gunicon在设置keepalive的时候，之前遇到过默认情况的keepalive时间给了5s钟，遇到服务A的keepalive时间大于60s的时候，可能服务B的连接已经断开了，但是服务A还维持的会话，当获取数据的时候发现读取数据失败返回EOF问题了。\n\n建议保证：服务B的Keepalive时间 > 服务A的keepalive时间\n\n![HTTP server phases](http://img.kuqin.com/upimg/allimg/160720/2036151E2-0.png)\n\n![HTTP Client phases](http://img.kuqin.com/upimg/allimg/160720/2036154434-1.png)\n\n### 参考资料\n\nhttps://studygolang.com/articles/7692\n","tags":["golang"],"categories":["golang"]},{"title":"kubectl 学习笔记","url":"/2017/05/26/kubectl/","content":"\n\n## kubectl 学习笔记\n\n思考：kubectl 和docker命令源码的设计思想类似。\n\ndocker中启动了服务器接受请求注册*api*, 而kubectl发送命令给apiserver请求数据或者创建资源。\n\nCmds是kubectl中的命令集合，所有命令都会整理在里面。\n\nCmd 是命令的实体，其中主要是具体执行用户命令。每个cmd负责一个命令执行类型(describe,get...)。\n\nBuilder 是cmd执行操作时的辅助工具，主要是负责封装与Apiserver交互的底层操作，和将Apiserver的返回数据转化为统一数据结构。\n\nKubectl 依赖于[cobra](https://github.com/spf13/cobra)包构建命令行支持，该包是支持通用的命令行构建库。\n\n```\nCmds(命令集合)<---Cmd(命令obj)\n       |          |\n       |          |\n       |          | \n       |        Builder\n       |          |\n       |          |  \n       |----------Cmd(命令obj)\n```\n\nKubectl 的执行流程分析以describe命令分析。\n\n1. 用户发起请求\n2. 根据用户执行动作分发给处理对应动作的Cmd (Cmd是执行用户命令的实体)\n3. 解析用户命令\n4. 向Apiserver获取数据\n5. 整理返回为通用的数据集合\n6. 找到解释查询类型数据的句柄\n7. 使用具柄对整理出的数据集合进行打印输出\n\n```\nkubectl describe node node1\n```\n\n如下, NewKubectlCommand 方法中cobra会根据命令动作将请求分配给describe注册的cmd。\n\n```\ngroups := templates.CommandGroups{\n        //...\n        {\n            Message: \"Troubleshooting and Debugging Commands:\",\n            Commands: []*cobra.Command{\n                NewCmdDescribe(f, out, err),    //<------describe操作的cmd\n                NewCmdLogs(f, out),\n                NewCmdAttach(f, in, out, err),\n                NewCmdExec(f, in, out, err),\n                NewCmdPortForward(f, out, err),\n                NewCmdProxy(f, out),\n                NewCmdCp(f, out, err),\n                auth.NewCmdAuth(f, out, err),\n            },\n        },\n        {\n            Message: \"Advanced Commands:\",\n            Commands: []*cobra.Command{\n                NewCmdApply(\"kubectl\", f, out, err),\n                NewCmdPatch(f, out),\n                NewCmdReplace(f, out),\n                NewCmdConvert(f, out),\n            },\n        },\n        // ...\n    }\n    groups.Add(cmds)\n```\n\nCmd会对获取用户输入数据， 并检查正确性然后使用Run函数处理。\n\n```\nfunc NewCmdDescribe(f cmdutil.Factory, out, cmdErr io.Writer) *cobra.Command {\n    options := &resource.FilenameOptions{}\n    describerSettings := &printers.DescriberSettings{}\n\n    validArgs := printersinternal.DescribableResources()\n    argAliases := kubectl.ResourceAliases(validArgs)\n\n    cmd := &cobra.Command{\n        Use:     \"describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME)\",\n        Short:   i18n.T(\"Show details of a specific resource or group of resources\"),\n        Long:    describeLong + \"\\n\\n\" + cmdutil.ValidResourceTypeList(f),\n        Example: describeExample,\n        Run: func(cmd *cobra.Command, args []string) {   // <------处理回调函数\n            err := RunDescribe(f, out, cmdErr, cmd, args, options, describerSettings)\n            cmdutil.CheckErr(err)\n        },\n        ValidArgs:  validArgs,     //<-----------------合法性检查 \n        ArgAliases: argAliases,\n    }\n    usage := \"containing the resource to describe\"\n    cmdutil.AddFilenameOptionFlags(cmd, options, usage)\n    \n    // 下面主要是输入参数检查 \n    \n    cmd.Flags().StringP(\"selector\", \"l\", \"\", \"Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)\")\n    cmd.Flags().Bool(\"all-namespaces\", false, \"If present, list the requested object(s) across all namespaces. Namespace in current context is ignored even if specified with --namespace.\")\n    cmd.Flags().BoolVar(&describerSettings.ShowEvents, \"show-events\", true, \"If true, display events related to the described object.\")\n    cmdutil.AddInclude3rdPartyFlags(cmd)\n    cmdutil.AddIncludeUninitializedFlag(cmd)\n    return cmd\n}\n```\n\n如下, 在 RunDescribe 中时对该命令的具体处理\n\n- Builder(), Unstructured(), ContinueOnError().\n   NamespaceParam(), FilenameParam(), LabelSelectorParam() ... Flatten() 的链式调用流程主要是为执行命令做准备。\n- Do() 函数是注册具体向Apiserver请求数据，和讲返回数据转化为通用结构的方法。\n- 最后的 describer.Describe（） 函数是将提取出的返回数据 打印出来做可视化接口。\n\n```\nfunc RunDescribe(f cmdutil.Factory, out, cmdErr io.Writer, cmd *cobra.Command, args []string, options *resource.FilenameOptions, describerSettings *printers.DescriberSettings) error {\n    \n    // ...\n\n    // include the uninitialized objects by default\n    // unless user explicitly set --include-uninitialized=false\n    includeUninitialized := cmdutil.ShouldIncludeUninitialized(cmd, true)\n    r := f.NewBuilder().\n        Unstructured().\n        ContinueOnError().\n        NamespaceParam(cmdNamespace).DefaultNamespace().AllNamespaces(allNamespaces).\n        FilenameParam(enforceNamespace, options).\n        LabelSelectorParam(selector).    // 设置用户的标签选择\n        IncludeUninitialized(includeUninitialized).\n        ResourceTypeOrNameArgs(true, args...). // 提取用户选择操作的对象类型\n        Flatten().                             //决定以何种方式从K8s的返回数据中提取信息                     \n        Do()                                   //执行命令获取数据\n    \n    // ...\n    \n    infos, err := r.Infos()                     \n    if err != nil {\n        if apierrors.IsNotFound(err) && len(args) == 2 {\n            return DescribeMatchingResources(f, cmdNamespace, args[0], args[1], describerSettings, out, err)\n        }\n        allErrs = append(allErrs, err)\n    }\n\n    errs := sets.NewString()\n    first := true\n    for _, info := range infos {\n        mapping := info.ResourceMapping()\n        describer, err := f.Describer(mapping)\n        if err != nil {\n            if errs.Has(err.Error()) {\n                continue\n            }\n            allErrs = append(allErrs, err)\n            errs.Insert(err.Error())\n            continue\n        }\n        // 下面通过describe 方法将提取到的数据 打印出来\n        s, err := describer.Describe(info.Namespace, info.Name, *describerSettings)\n        if err != nil {\n            if errs.Has(err.Error()) {\n                continue\n            }\n            allErrs = append(allErrs, err)\n            errs.Insert(err.Error())\n            continue\n        }\n        if first {\n            first = false\n            fmt.Fprint(out, s)\n        } else {\n            fmt.Fprintf(out, \"\\n\\n%s\", s)\n        }\n    }\n\n    return utilerrors.NewAggregate(allErrs)\n}\n```\n\n下面具体分析获取数据的流程，获取数据包括从Apiserver请求数据以及从返回信息中提取有用数据两个操作。\n\nRetrieveLazy 中注册了从Apiserver获取数据的操作。\nNewDecoratedVisitor 中注册了从获取到的数据结构中转化出通用数据的方法。\n\n```\n// inputs are consumed by the first execution - use Infos() or Object() on the Result to capture a list\n// for further iteration.\nfunc (b *Builder) Do() *Result {\n    r := b.visitorResult()\n    //... \n    \n    helpers := []VisitorFunc{}\n    //注册获取数据前的动作\n    if b.defaultNamespace {\n        helpers = append(helpers, SetNamespace(b.namespace))\n    }\n    if b.requireNamespace {\n        helpers = append(helpers, RequireNamespace(b.namespace))\n    }\n    helpers = append(helpers, FilterNamespace)\n    if b.requireObject {\n        //注册从Apiserver获取数据的方法\n        helpers = append(helpers, RetrieveLazy) \n    }\n    //注册从返回数据中提取信息的方法\n    r.visitor = NewDecoratedVisitor(r.visitor, helpers...)\n    if b.continueOnError {\n        r.visitor = ContinueOnErrorVisitor{r.visitor}\n    }\n    return r\n}\n```\n\n```\n// RetrieveLazy updates the object if it has not been loaded yet.\nfunc RetrieveLazy(info *Info, err error) error {\n    if err != nil {\n        return err\n    }\n    if info.Object == nil {\n        return info.Get()     //从Apiserver获取数据\n    }\n    return nil\n}\n```\n\n而 NewDecoratedVisitor 方法注册了数据处理的关键函数 Visit， 这个函数可以使用户可以将来自Apiserver的数据转化为通用数据集合。\n\n```\n// NewDecoratedVisitor will create a visitor that invokes the provided visitor functions before\n// the user supplied visitor function is invoked, giving them the opportunity to mutate the Info\n// object or terminate early with an error.\nfunc NewDecoratedVisitor(v Visitor, fn ...VisitorFunc) Visitor {\n    if len(fn) == 0 {\n        return v\n    }\n    return DecoratedVisitor{v, fn}\n}\n\n// Visit implements Visitor\nfunc (v DecoratedVisitor) Visit(fn VisitorFunc) error {\n    return v.visitor.Visit(func(info *Info, err error) error {\n        if err != nil {\n            return err\n        }\n        for i := range v.decorators {\n            if err := v.decorators[i](info, nil); err != nil {\n                return err\n            }\n        }\n        return fn(info, nil)\n    })\n}\n```\n\n打印提取到的数据主要是调用注册的describe方法，会根据用户的请求如下获取对应的describe\n\n```\ndescriber, err := f.Describer(mapping)\n```\n\nDescribe 集合中注册了 对K8s各种数据的打印方法(针对visit转化后的通用数据)\n\n```\nfunc init() {\n    d := &Describers{}\n    err := d.Add(\n        describeLimitRange,\n        describeQuota,\n        describePod,\n        describeService,\n        describeReplicationController,\n        describeDaemonSet,\n        describeNode,              //打印节点\n        describeNamespace,\n    )\n    if err != nil {\n        glog.Fatalf(\"Cannot register describers: %v\", err)\n    }\n    DefaultObjectDescriber = d\n}\n```\n\n使用获取到的对应的Describe作打印\n\n```\n//遍历整理出的返回信息\nfor _, info := range infos {\n        // 执行打印操作\n        s, err := describer.Describe(info.Namespace, info.Name, *describerSettings)\n        // ...\n    }\n```\n\n\n\n","tags":["k8s"],"categories":["k8s"]},{"title":"tensorflow环境安装","url":"/2017/05/26/tensorflow安装小结/","content":"\n## 深度学习环境 tensorflow安装\n\n### 环境准备\n\n需要支持RTX2080ti显卡，最好有11g现存。\n\n本人使用CUDA10.1，cudnn 10.1适配。\n\npython3.7 pycharm安装。\n\nanaconde3 安装多次使用了 anaconde3 4.4 版本安装上了。\n\n之后安装tensorflow，安装版本1.13版本的支持10.1的版本的tensorflow.\n\n### 运行测试\n\n```python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n# 创建一个变量, 初始化为标量 0.\nstate = tf.Variable(0, name=\"counter\")\n\n# 创建一个 op, 其作用是使 state 增加 1\none = tf.constant(1)\nnew_value = tf.add(state, one)\nupdate = tf.assign(state, new_value)\n\n# 启动图后, 变量必须先经过`初始化` (init) op 初始化,\n# 首先必须增加一个`初始化` op 到图中.\n# initialize_all_variables 警告换成 global_variables_initializer\ninit_op = tf.global_variables_initializer()\n\n# 启动图, 运行 op\nwith tf.Session() as sess:\n    # 运行 'init' op\n    sess.run(init_op)\n    # 打印 'state' 的初始值\n    print(sess.run(state))\n    # 运行 op, 更新 'state', 并打印 'state'\n    for i in range(3):\n        sess.run(update)\n        print(sess.run(state))\n```\n\n运行成功则正常。\n\n开启旅程啦~~\n","tags":["深度学习"],"categories":["深度学习"]}]